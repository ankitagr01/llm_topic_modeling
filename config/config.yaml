model:
  teacher_model_name: "unslot/llama-70b"
  teacher_model_name: "meta-llama/Meta-Llama-3-70B-Instruct"
  student_model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
  max_seq_length: 2048
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: True
  bnb_4bit_compute_dtype: torch.bfloat16
  lora_params:
    r: 16
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    lora_alpha: 16
    lora_dropout: 0
    bias: "none"
    use_gradient_checkpointing: "unsloth"
    random_state: 3407
    use_rslora: False
    loftq_config: None

training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  save_steps: 10000
  save_total_limit: 2
  fp16: True
  logging_steps: 500
  report_to: "wandb"
  run_name: "llama-topic-modeling"
  output_dir: ".results//llama_2_7b_ft/"

data:
  dataset_name: "CShorten/ML-ArXiv-Papers"
  train_data_name: "ankitagr01/dynamic_topic_modeling_arxiv_abstracts"
  test_data_name: "ankitagr01/dynamic_topic_modeling_arxiv_abstract_1k"
  test_size: 0.05
  val_size: 0.1
  generated_topics_file: "generated_topics.csv"

eval:
  fewshot: False
  eval_model_name: "ankitagr01/llama_3_8b_ft_topic_new"