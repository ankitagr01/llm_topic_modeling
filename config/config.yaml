model:
  teacher_model_name: "unslot/llama-70b"
  teacher_model_name: "unsloth/llama-3-70b-bnb-4bit"
  student_model_name: "unslot/llama-3-8b-bnb-4bit"
  max_seq_length: 512
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: True
  bnb_4bit_compute_dtype: torch.bfloat16
  lora_params:
    r: 16
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    lora_alpha: 16
    lora_dropout: 0
    bias: "none"
    use_gradient_checkpointing: "unsloth"
    random_state: 3407
    use_rslora: False
    loftq_config: None

training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  save_steps: 10000
  save_total_limit: 2
  fp16: True
  logging_steps: 500
  report_to: "wandb"
  run_name: "llama-topic-modeling"
  output_dir: ".results//llama_2_7b_ft/"

data:
  dataset_name: "CShorten/ML-ArXiv-Papers"
  test_size: 0.05
  val_size: 0.1
  generated_topics_file: "generated_topics.csv"

