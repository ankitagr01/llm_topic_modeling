Abstract,Topic,Prediction
"  We propose an adversarial training procedure for learning a causal implicit
generative model for a given causal graph. We show that adversarial training
can be used to learn a generative model with true observational and
interventional distributions if the generator architecture is consistent with
the given causal graph. We consider the application of generating faces based
on given binary labels where the dependency structure between the labels is
preserved with a causal graph. This problem can be seen as learning a causal
implicit generative model for the image and labels. We devise a two-stage
procedure for this problem. First we train a causal implicit generative model
over binary labels using a neural network consistent with a causal graph as the
generator. We empirically show that WassersteinGAN can be used to output
discrete labels. Later, we propose two new conditional GAN architectures, which
we call CausalGAN and CausalBEGAN. We show that the optimal generator of the
CausalGAN, given the labels, samples from the image distributions conditioned
on these labels. The conditional GAN combined with a trained causal implicit
generative model for the labels is then a causal implicit generative model over
the labels and the generated image. We show that the proposed architectures can
be used to sample from observational and interventional image distributions,
even for interventions which do not naturally occur in the dataset.
",Causal Image Generation,causal generative models
"  Multiple automakers have in development or in production automated driving
systems (ADS) that offer freeway-pilot functions. This type of ADS is typically
limited to restricted-access freeways only, that is, the transition from manual
to automated modes takes place only after the ramp merging process is completed
manually. One major challenge to extend the automation to ramp merging is that
the automated vehicle needs to incorporate and optimize long-term objectives
(e.g. successful and smooth merge) when near-term actions must be safely
executed. Moreover, the merging process involves interactions with other
vehicles whose behaviors are sometimes hard to predict but may influence the
merging vehicle optimal actions. To tackle such a complicated control problem,
we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an
optimal driving policy by maximizing the long-term reward in an interactive
environment. Specifically, we apply a Long Short-Term Memory (LSTM)
architecture to model the interactive environment, from which an internal state
containing historical driving information is conveyed to a Deep Q-Network
(DQN). The DQN is used to approximate the Q-function, which takes the internal
state as input and generates Q-values as output for action selection. With this
DRL architecture, the historical impact of interactive environment on the
long-term reward can be captured and taken into account for deciding the
optimal control policy. The proposed architecture has the potential to be
extended and applied to other autonomous driving scenarios such as driving
through a complex intersection or changing lanes under varying traffic flow
conditions.
",Ramp Merging in Autonomous Driving Systems,autonomous vehicle control
"  We propose a probabilistic model for interpreting gene expression levels that
are observed through single-cell RNA sequencing. In the model, each cell has a
low-dimensional latent representation. Additional latent variables account for
technical effects that may erroneously set some observations of gene expression
levels to zero. Conditional distributions are specified by neural networks,
giving the proposed model enough flexibility to fit the data well. We use
variational inference and stochastic optimization to approximate the posterior
distribution. The inference procedure scales to over one million cells, whereas
competing algorithms do not. Even for smaller datasets, for several tasks, the
proposed procedure outperforms state-of-the-art methods like ZIFA and
ZINB-WaVE. We also extend our framework to account for batch effects and other
confounding factors, and propose a Bayesian hypothesis test for differential
expression that outperforms DESeq2.
",Single-Cell RNA Sequencing Analysis,single-cell rna sequencing analysis
"  We study the problem of generalized uniformity testing \cite{BC17} of a
discrete probability distribution: Given samples from a probability
distribution $p$ over an {\em unknown} discrete domain $\mathbf{\Omega}$, we
want to distinguish, with probability at least $2/3$, between the case that $p$
is uniform on some {\em subset} of $\mathbf{\Omega}$ versus $\epsilon$-far, in
total variation distance, from any such uniform distribution.
  We establish tight bounds on the sample complexity of generalized uniformity
testing. In more detail, we present a computationally efficient tester whose
sample complexity is optimal, up to constant factors, and a matching
information-theoretic lower bound. Specifically, we show that the sample
complexity of generalized uniformity testing is
$\Theta\left(1/(\epsilon^{4/3}\|p\|_3) + 1/(\epsilon^{2} \|p\|_2) \right)$.
",Uniformity Testing in Discrete Probability Distributions,discrete probability distribution testing
"  Specialized classifiers, namely those dedicated to a subset of classes, are
often adopted in real-world recognition systems. However, integrating such
classifiers is nontrivial. Existing methods, e.g. weighted average, usually
implicitly assume that all constituents of an ensemble cover the same set of
classes. Such methods can produce misleading predictions when used to combine
specialized classifiers. This work explores a novel approach. Instead of
combining predictions from individual classifiers directly, it first decomposes
the predictions into sets of pairwise preferences, treating them as transition
channels between classes, and thereon constructs a continuous-time Markov
chain, and use the equilibrium distribution of this chain as the final
prediction. This way allows us to form a coherent picture over all specialized
predictions. On large public datasets, the proposed method obtains considerable
improvement compared to mainstream ensemble methods, especially when the
classifier coverage is highly unbalanced.
",Ensemble Methods for Specialized Classifiers,ensemble learning for specialized classifiers
"  In outdoor environments, mobile robots are required to navigate through
terrain with varying characteristics, some of which might significantly affect
the integrity of the platform. Ideally, the robot should be able to identify
areas that are safe for navigation based on its own percepts about the
environment while avoiding damage to itself. Bayesian optimisation (BO) has
been successfully applied to the task of learning a model of terrain
traversability while guiding the robot through more traversable areas. An
issue, however, is that localisation uncertainty can end up guiding the robot
to unsafe areas and distort the model being learnt. In this paper, we address
this problem and present a novel method that allows BO to consider localisation
uncertainty by applying a Gaussian process model for uncertain inputs as a
prior. We evaluate the proposed method in simulation and in experiments with a
real robot navigating over rough terrain and compare it against standard BO
methods.
",Mobile Robot Navigation in Uncertain Terrain,robot navigation and terrain traversability
"  Much combinatorial optimisation problems constitute a non-polynomial (NP)
hard optimisation problem, i.e., they can not be solved in polynomial time. One
such problem is finding the shortest route between two nodes on a graph.
Meta-heuristic algorithms such as $A^{*}$ along with mixed-integer programming
(MIP) methods are often employed for these problems. Our work demonstrates that
it is possible to approximate solutions generated by a meta-heuristic algorithm
using a deep recurrent neural network. We compare different methodologies based
on reinforcement learning (RL) and recurrent neural networks (RNN) to gauge
their respective quality of approximation. We show the viability of recurrent
neural network solutions on a graph that has over 300 nodes and argue that a
sequence-to-sequence network rather than other recurrent networks has improved
approximation quality. Additionally, we argue that homotopy continuation --
that increases chances of hitting an extremum -- further improves the estimate
generated by a vanilla RNN.
",Approximating Combinatorial Optimization Solutions with Deep Neural Networks,combinatorial optimization and meta-heuristic algorithms
"  An RNN-based forecasting approach is used to early detect anomalies in
industrial multivariate time series data from a simulated Tennessee Eastman
Process (TEP) with many cyber-attacks. This work continues a previously
proposed LSTM-based approach to the fault detection in simpler data. It is
considered necessary to adapt the RNN network to deal with data containing
stochastic, stationary, transitive and a rich variety of anomalous behaviours.
There is particular focus on early detection with special NAB-metric. A
comparison with the DPCA approach is provided. The generated data set is made
publicly available.
",Anomaly Detection in Industrial Time Series Data,anomaly detection in industrial time series data using rnn
"  The instability of myoelectric signals over time complicates their use to
control highly articulated prostheses. To address this problem, studies have
tried to combine surface electromyography with modalities that are less
affected by the amputation and environment, such as accelerometry or gaze
information. In the latter case, the hypothesis is that a subject looks at the
object he or she intends to manipulate and that knowing this object's
affordances allows to constrain the set of possible grasps. In this paper, we
develop an automated way to detect stable fixations and show that gaze
information is indeed helpful in predicting hand movements. In our multimodal
approach, we automatically detect stable gazes and segment an object of
interest around the subject's fixation in the visual frame. The patch extracted
around this object is subsequently fed through an off-the-shelf deep
convolutional neural network to obtain a high level feature representation,
which is then combined with traditional surface electromyography in the
classification stage. Tests have been performed on a dataset acquired from five
intact subjects who performed ten types of grasps on various objects as well as
in a functional setting. They show that the addition of gaze information
increases the classification accuracy considerably. Further analysis
demonstrates that this improvement is consistent for all grasps and
concentrated during the movement onset and offset.
",Multimodal Prosthetic Control Using Gaze Information,multimodal prosthetic control
"  In this paper, we propose an uncertainty-aware learning from demonstration
method by presenting a novel uncertainty estimation method utilizing a mixture
density network appropriate for modeling complex and noisy human behaviors. The
proposed uncertainty acquisition can be done with a single forward path without
Monte Carlo sampling and is suitable for real-time robotics applications. The
properties of the proposed uncertainty measure are analyzed through three
different synthetic examples, absence of data, heavy measurement noise, and
composition of functions scenarios. We show that each case can be distinguished
using the proposed uncertainty measure and presented an uncertainty-aware
learn- ing from demonstration method of an autonomous driving using this
property. The proposed uncertainty-aware learning from demonstration method
outperforms other compared methods in terms of safety using a complex
real-world driving dataset.
",Uncertainty Estimation in Robot Learning,uncertainty-aware learning from demonstration
"  Continuous dimensional emotion prediction is a challenging task where the
fusion of various modalities usually achieves state-of-the-art performance such
as early fusion or late fusion. In this paper, we propose a novel multi-modal
fusion strategy named conditional attention fusion, which can dynamically pay
attention to different modalities at each time step. Long-short term memory
recurrent neural networks (LSTM-RNN) is applied as the basic uni-modality model
to capture long time dependencies. The weights assigned to different modalities
are automatically decided by the current input features and recent history
information rather than being fixed at any kinds of situation. Our experimental
results on a benchmark dataset AVEC2015 show the effectiveness of our method
which outperforms several common fusion strategies for valence prediction.
",Multimodal Emotion Prediction,multimodal fusion for emotion prediction
"  As a new machine learning approach, extreme learning machine (ELM) has
received wide attentions due to its good performances. However, when directly
applied to the hyperspectral image (HSI) classification, the recognition rate
is too low. This is because ELM does not use the spatial information which is
very important for HSI classification. In view of this, this paper proposes a
new framework for spectral-spatial classification of HSI by combining ELM with
loopy belief propagation (LBP). The original ELM is linear, and the nonlinear
ELMs (or Kernel ELMs) are the improvement of linear ELM (LELM). However, based
on lots of experiments and analysis, we found out that the LELM is a better
choice than nonlinear ELM for spectral-spatial classification of HSI.
Furthermore, we exploit the marginal probability distribution that uses the
whole information in the HSI and learn such distribution using the LBP. The
proposed method not only maintain the fast speed of ELM, but also greatly
improves the accuracy of classification. The experimental results in the
well-known HSI data sets, Indian Pines and Pavia University, demonstrate the
good performances of the proposed method.
",Hyperspectral Image Classification using Extreme Learning Machine,hyperspectral image classification using extreme learning machine
"  In this paper, we describe how a patient-specific, ultrasound-probe-induced
prostate motion model can be directly generated from a single preoperative MR
image. Our motion model allows for sampling from the conditional distribution
of dense displacement fields, is encoded by a generative neural network
conditioned on a medical image, and accepts random noise as additional input.
The generative network is trained by a minimax optimisation with a second
discriminative neural network, tasked to distinguish generated samples from
training motion data. In this work, we propose that 1) jointly optimising a
third conditioning neural network that pre-processes the input image, can
effectively extract patient-specific features for conditioning; and 2)
combining multiple generative models trained separately with heuristically
pre-disjointed training data sets can adequately mitigate the problem of mode
collapse. Trained with diagnostic T2-weighted MR images from 143 real patients
and 73,216 3D dense displacement fields from finite element simulations of
intraoperative prostate motion due to transrectal ultrasound probe pressure,
the proposed models produced physically-plausible patient-specific motion of
prostate glands. The ability to capture biomechanically simulated motion was
evaluated using two errors representing generalisability and specificity of the
model. The median values, calculated from a 10-fold cross-validation, were
2.8+/-0.3 mm and 1.7+/-0.1 mm, respectively. We conclude that the introduced
approach demonstrates the feasibility of applying state-of-the-art machine
learning algorithms to generate organ motion models from patient images, and
shows significant promise for future research.
",Ultrasound-Probe-Induced Prostate Motion Modeling Using MRI,medical imaging and machine learning
"  We study embedded Binarized Neural Networks (eBNNs) with the aim of allowing
current binarized neural networks (BNNs) in the literature to perform
feedforward inference efficiently on small embedded devices. We focus on
minimizing the required memory footprint, given that these devices often have
memory as small as tens of kilobytes (KB). Beyond minimizing the memory
required to store weights, as in a BNN, we show that it is essential to
minimize the memory used for temporaries which hold intermediate results
between layers in feedforward inference. To accomplish this, eBNN reorders the
computation of inference while preserving the original BNN structure, and uses
just a single floating-point temporary for the entire neural network. All
intermediate results from a layer are stored as binary values, as opposed to
floating-points used in current BNN implementations, leading to a 32x reduction
in required temporary space. We provide empirical evidence that our proposed
eBNN approach allows efficient inference (10s of ms) on devices with severely
limited memory (10s of KB). For example, eBNN achieves 95\% accuracy on the
MNIST dataset running on an Intel Curie with only 15 KB of usable memory with
an inference runtime of under 50 ms per sample. To ease the development of
applications in embedded contexts, we make our source code available that
allows users to train and discover eBNN models for a learning task at hand,
which fit within the memory constraint of the target device.
",Efficient Inference in Embedded Binarized Neural Networks,embedded machine learning for resource-constrained devices
"  Background: Convolutional Neural Networks can be effectively used only when
data are endowed with an intrinsic concept of neighbourhood in the input space,
as is the case of pixels in images. We introduce here Ph-CNN, a novel deep
learning architecture for the classification of metagenomics data based on the
Convolutional Neural Networks, with the patristic distance defined on the
phylogenetic tree being used as the proximity measure. The patristic distance
between variables is used together with a sparsified version of
MultiDimensional Scaling to embed the phylogenetic tree in a Euclidean space.
Results: Ph-CNN is tested with a domain adaptation approach on synthetic data
and on a metagenomics collection of gut microbiota of 38 healthy subjects and
222 Inflammatory Bowel Disease patients, divided in 6 subclasses.
Classification performance is promising when compared to classical algorithms
like Support Vector Machines and Random Forest and a baseline fully connected
neural network, e.g. the Multi-Layer Perceptron. Conclusion: Ph-CNN represents
a novel deep learning approach for the classification of metagenomics data.
Operatively, the algorithm has been implemented as a custom Keras layer taking
care of passing to the following convolutional layer not only the data but also
the ranked list of neighbourhood of each sample, thus mimicking the case of
image data, transparently to the user. Keywords: Metagenomics; Deep learning;
Convolutional Neural Networks; Phylogenetic trees
",Ph-CNN for Metagenomics Data Classification,deep learning in metagenomics
"  When convolutional neural networks are used to tackle learning problems based
on music or, more generally, time series data, raw one-dimensional data are
commonly pre-processed to obtain spectrogram or mel-spectrogram coefficients,
which are then used as input to the actual neural network. In this
contribution, we investigate, both theoretically and experimentally, the
influence of this pre-processing step on the network's performance and pose the
question, whether replacing it by applying adaptive or learned filters directly
to the raw data, can improve learning success. The theoretical results show
that approximately reproducing mel-spectrogram coefficients by applying
adaptive filters and subsequent time-averaging is in principle possible. We
also conducted extensive experimental work on the task of singing voice
detection in music. The results of these experiments show that for
classification based on Convolutional Neural Networks the features obtained
from adaptive filter banks followed by time-averaging perform better than the
canonical Fourier-transform-based mel-spectrogram coefficients. Alternative
adaptive approaches with center frequencies or time-averaging lengths learned
from training data perform equally well.
",Music Information Retrieval,pre-processing of time series data for neural networks
"  A visual-relational knowledge graph (KG) is a multi-relational graph whose
entities are associated with images. We explore novel machine learning
approaches for answering visual-relational queries in web-extracted knowledge
graphs. To this end, we have created ImageGraph, a KG with 1,330 relation
types, 14,870 entities, and 829,931 images crawled from the web. With
visual-relational KGs such as ImageGraph one can introduce novel probabilistic
query types in which images are treated as first-class citizens. Both the
prediction of relations between unseen images as well as multi-relational image
retrieval can be expressed with specific families of visual-relational queries.
We introduce novel combinations of convolutional networks and knowledge graph
embedding methods to answer such queries. We also explore a zero-shot learning
scenario where an image of an entirely new entity is linked with multiple
relations to entities of an existing KG. The resulting multi-relational
grounding of unseen entity images into a knowledge graph serves as a semantic
entity representation. We conduct experiments to demonstrate that the proposed
methods can answer these visual-relational queries efficiently and accurately.
",Visual Query Answering in Knowledge Graphs,visual relational knowledge graphs
"  This paper describes a distributed MapReduce implementation of the minimum
Redundancy Maximum Relevance algorithm, a popular feature selection method in
bioinformatics and network inference problems. The proposed approach handles
both tall/narrow and wide/short datasets. We further provide an open source
implementation based on Hadoop/Spark, and illustrate its scalability on
datasets involving millions of observations or features.
",Distributed Feature Selection,bioinformatics
"  We present MILABOT: a deep reinforcement learning chatbot developed by the
Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize
competition. MILABOT is capable of conversing with humans on popular small talk
topics through both speech and text. The system consists of an ensemble of
natural language generation and retrieval models, including template-based
models, bag-of-words models, sequence-to-sequence neural network and latent
variable neural network models. By applying reinforcement learning to
crowdsourced data and real-world user interactions, the system has been trained
to select an appropriate response from the models in its ensemble. The system
has been evaluated through A/B testing with real-world users, where it
performed significantly better than many competing systems. Due to its machine
learning architecture, the system is likely to improve with additional data.
",Chatbots for Conversational Interfaces,artificial intelligence for human-computer interaction
"  In this paper, we present an online adaptive PCA algorithm that is able to
compute the full dimensional eigenspace per new time-step of sequential data.
The algorithm is based on a one-step update rule that considers all second
order correlations between previous samples and the new time-step. Our
algorithm has O(n) complexity per new time-step in its deterministic mode and
O(1) complexity per new time-step in its stochastic mode. We test our algorithm
on a number of time-varying datasets of different physical phenomena. Explained
variance curves indicate that our technique provides an excellent approximation
to the original eigenspace computed using standard PCA in batch mode. In
addition, our experiments show that the stochastic mode, despite its much lower
computational complexity, converges to the same eigenspace computed using the
deterministic mode.
",Online Adaptive PCA Algorithm,online adaptive pca
"  Recent work on privacy-preserving machine learning has considered how
data-mining competitions such as Kaggle could potentially be ""hacked"", either
intentionally or inadvertently, by using information from an oracle that
reports a classifier's accuracy on the test set. For binary classification
tasks in particular, one of the most common accuracy metrics is the Area Under
the ROC Curve (AUC), and in this paper we explore the mathematical structure of
how the AUC is computed from an n-vector of real-valued ""guesses"" with respect
to the ground-truth labels. We show how knowledge of a classifier's AUC on the
test set can constrain the set of possible ground-truth labelings, and we
derive an algorithm both to compute the exact number of such labelings and to
enumerate efficiently over them. Finally, we provide empirical evidence that,
surprisingly, the number of compatible labelings can actually decrease as n
grows, until a test set-dependent threshold is reached.
",Privacy in Machine Learning Competitions,privacy-preserving machine learning
"  We have developed a new data-driven paradigm for the rapid inference,
modeling and simulation of the physics of transport phenomena by deep learning.
Using conditional generative adversarial networks (cGAN), we train models for
the direct generation of solutions to steady state heat conduction and
incompressible fluid flow purely on observation without knowledge of the
underlying governing equations. Rather than using iterative numerical methods
to approximate the solution of the constitutive equations, cGANs learn to
directly generate the solutions to these phenomena, given arbitrary boundary
conditions and domain, with high test accuracy (MAE$<$1\%) and state-of-the-art
computational performance. The cGAN framework can be used to learn causal
models directly from experimental observations where the underlying physical
model is complex or unknown.
",Deep Learning for Transport Phenomena Simulation,machine learning for physics
"  Machine learning (ML) plays an ever-increasing role in advanced automotive
functionality for driver assistance and autonomous operation; however, its
adequacy from the perspective of safety certification remains controversial. In
this paper, we analyze the impacts that the use of ML as an implementation
approach has on ISO 26262 safety lifecycle and ask what could be done to
address them. We then provide a set of recommendations on how to adapt the
standard to accommodate ML.
",ML in Automotive Safety Certification,machine learning in automotive safety certification
"  We propose a neural embedding algorithm called Network Vector, which learns
distributed representations of nodes and the entire networks simultaneously. By
embedding networks in a low-dimensional space, the algorithm allows us to
compare networks in terms of structural similarity and to solve outstanding
predictive problems. Unlike alternative approaches that focus on node level
features, we learn a continuous global vector that captures each node's global
context by maximizing the predictive likelihood of random walk paths in the
network. Our algorithm is scalable to real world graphs with many nodes. We
evaluate our algorithm on datasets from diverse domains, and compare it with
state-of-the-art techniques in node classification, role discovery and concept
analogy tasks. The empirical results show the effectiveness and the efficiency
of our algorithm.
",Network Embedding,network embeddings
"  The last decade has seen a surge of interest in adaptive learning algorithms
for data stream classification, with applications ranging from predicting ozone
level peaks, learning stock market indicators, to detecting computer security
violations. In addition, a number of methods have been developed to detect
concept drifts in these streams. Consider a scenario where we have a number of
classifiers with diverse learning styles and different drift detectors.
Intuitively, the current 'best' (classifier, detector) pair is application
dependent and may change as a result of the stream evolution. Our research
builds on this observation. We introduce the $\mbox{Tornado}$ framework that
implements a reservoir of diverse classifiers, together with a variety of drift
detection algorithms. In our framework, all (classifier, detector) pairs
proceed, in parallel, to construct models against the evolving data streams. At
any point in time, we select the pair which currently yields the best
performance. We further incorporate two novel stacking-based drift detection
methods, namely the $\mbox{FHDDMS}$ and $\mbox{FHDDMS}_{add}$ approaches. The
experimental evaluation confirms that the current 'best' (classifier, detector)
pair is not only heavily dependent on the characteristics of the stream, but
also that this selection evolves as the stream flows. Further, our
$\mbox{FHDDMS}$ variants detect concept drifts accurately in a timely fashion
while outperforming the state-of-the-art.
",Adaptive Learning for Data Stream Classification,adaptive learning for data stream classification
"  Obtaining enough labeled data to robustly train complex discriminative models
is a major bottleneck in the machine learning pipeline. A popular solution is
combining multiple sources of weak supervision using generative models. The
structure of these models affects training label quality, but is difficult to
learn without any ground truth labels. We instead rely on these weak
supervision sources having some structure by virtue of being encoded
programmatically. We present Coral, a paradigm that infers generative model
structure by statically analyzing the code for these heuristics, thus reducing
the data required to learn structure significantly. We prove that Coral's
sample complexity scales quasilinearly with the number of heuristics and number
of relations found, improving over the standard sample complexity, which is
exponential in $n$ for identifying $n^{\textrm{th}}$ degree relations.
Experimentally, Coral matches or outperforms traditional structure learning
approaches by up to 3.81 F1 points. Using Coral to model dependencies instead
of assuming independence results in better performance than a fully supervised
model by 3.07 accuracy points when heuristics are used to label radiology data
without ground truth labels.
",Weak Supervision for Machine Learning,machine learning
"  In recent years, attention has been focused on the relationship between
black-box optimiza- tion problem and reinforcement learning problem. In this
research, we propose the Mirror Descent Search (MDS) algorithm which is
applicable both for black box optimization prob- lems and reinforcement
learning problems. Our method is based on the mirror descent method, which is a
general optimization algorithm. The contribution of this research is roughly
twofold. We propose two essential algorithms, called MDS and Accelerated Mirror
Descent Search (AMDS), and two more approximate algorithms: Gaussian Mirror
Descent Search (G-MDS) and Gaussian Accelerated Mirror Descent Search (G-AMDS).
This re- search shows that the advanced methods developed in the context of the
mirror descent research can be applied to reinforcement learning problem. We
also clarify the relationship between an existing reinforcement learning
algorithm and our method. With two evaluation experiments, we show our proposed
algorithms converge faster than some state-of-the-art methods.
",Mirror Descent Optimization,reinforcement learning
"  Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
",Adversarial Attack Detection in Autonomous Systems,adversarial attacks in deep learning
"  The expressive power of neural networks is important for understanding deep
learning. Most existing works consider this problem from the view of the depth
of a network. In this paper, we study how width affects the expressiveness of
neural networks. Classical results state that depth-bounded (e.g. depth-$2$)
networks with suitable activation functions are universal approximators. We
show a universal approximation theorem for width-bounded ReLU networks:
width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal
approximators. Moreover, except for a measure zero set, all functions cannot be
approximated by width-$n$ ReLU networks, which exhibits a phase transition.
Several recent works demonstrate the benefits of depth by proving the
depth-efficiency of neural networks. That is, there are classes of deep
networks which cannot be realized by any shallow network whose size is no more
than an exponential bound. Here we pose the dual question on the
width-efficiency of ReLU networks: Are there wide networks that cannot be
realized by narrow networks whose size is not substantially larger? We show
that there exist classes of wide networks which cannot be realized by any
narrow network whose depth is no more than a polynomial bound. On the other
hand, we demonstrate by extensive experiments that narrow networks whose size
exceed the polynomial bound by a constant factor can approximate wide and
shallow network with high accuracy. Our results provide more comprehensive
evidence that depth is more effective than width for the expressiveness of ReLU
networks.
",Effect of Network Width on Expressiveness,neural network expressiveness
"  Falsification is drawing attention in quality assurance of heterogeneous
systems whose complexities are beyond most verification techniques'
scalability. In this paper we introduce the idea of causality aid in
falsification: by providing a falsification solver -- that relies on stochastic
optimization of a certain cost function -- with suitable causal information
expressed by a Bayesian network, search for a falsifying input value can be
efficient. Our experiment results show the idea's viability.
",Causality-Aided Falsification in Complex Systems,falsification in complex systems
"  This paper presents a deep learning method for faster magnetic resonance
imaging (MRI) by reducing k-space data with sub-Nyquist sampling strategies and
provides a rationale for why the proposed approach works well. Uniform
subsampling is used in the time-consuming phase-encoding direction to capture
high-resolution image information, while permitting the image-folding problem
dictated by the Poisson summation formula. To deal with the localization
uncertainty due to image folding, very few low-frequency k-space data are
added. Training the deep learning net involves input and output images that are
pairs of Fourier transforms of the subsampled and fully sampled k-space data.
Numerous experiments show the remarkable performance of the proposed method;
only 29% of k-space data can generate images of high quality as effectively as
standard MRI reconstruction with fully sampled data.
",Deep Learning for Fast MRI Reconstruction,fast magnetic resonance imaging (mri) techniques
"  Kernel methods have recently attracted resurgent interest, showing
performance competitive with deep neural networks in tasks such as speech
recognition. The random Fourier features map is a technique commonly used to
scale up kernel machines, but employing the randomized feature map means that
$O(\epsilon^{-2})$ samples are required to achieve an approximation error of at
most $\epsilon$. We investigate some alternative schemes for constructing
feature maps that are deterministic, rather than random, by approximating the
kernel in the frequency domain using Gaussian quadrature. We show that
deterministic feature maps can be constructed, for any $\gamma > 0$, to achieve
error $\epsilon$ with $O(e^{e^\gamma} + \epsilon^{-1/\gamma})$ samples as
$\epsilon$ goes to 0. Our method works particularly well with sparse ANOVA
kernels, which are inspired by the convolutional layer of CNNs. We validate our
methods on datasets in different domains, such as MNIST and TIMIT, showing that
deterministic features are faster to generate and achieve accuracy comparable
to the state-of-the-art kernel methods based on random Fourier features.
",Deterministic Feature Maps for Kernel Machines,kernel methods for feature map construction
"  Internet traffic classification has become more important with rapid growth
of current Internet network and online applications. There have been numerous
studies on this topic which have led to many different approaches. Most of
these approaches use predefined features extracted by an expert in order to
classify network traffic. In contrast, in this study, we propose a \emph{deep
learning} based approach which integrates both feature extraction and
classification phases into one system. Our proposed scheme, called ""Deep
Packet,"" can handle both \emph{traffic characterization} in which the network
traffic is categorized into major classes (\eg, FTP and P2P) and application
identification in which end-user applications (\eg, BitTorrent and Skype)
identification is desired. Contrary to most of the current methods, Deep Packet
can identify encrypted traffic and also distinguishes between VPN and non-VPN
network traffic. After an initial pre-processing phase on data, packets are fed
into Deep Packet framework that embeds stacked autoencoder and convolution
neural network in order to classify network traffic. Deep packet with CNN as
its classification model achieved recall of $0.98$ in application
identification task and $0.94$ in traffic categorization task. To the best of
our knowledge, Deep Packet outperforms all of the proposed classification
methods on UNB ISCX VPN-nonVPN dataset.
",Deep Learning for Network Traffic Classification,internet traffic classification
"  Selecting the right web links for a website is important because appropriate
links not only can provide high attractiveness but can also increase the
website's revenue. In this work, we first show that web links have an intrinsic
\emph{multi-level feedback structure}. For example, consider a $2$-level
feedback web link: the $1$st level feedback provides the Click-Through Rate
(CTR) and the $2$nd level feedback provides the potential revenue, which
collectively produce the compound $2$-level revenue. We consider the
context-free links selection problem of selecting links for a homepage so as to
maximize the total compound $2$-level revenue while keeping the total $1$st
level feedback above a preset threshold. We further generalize the problem to
links with $n~(n\ge2)$-level feedback structure. The key challenge is that the
links' multi-level feedback structures are unobservable unless the links are
selected on the homepage. To our best knowledge, we are the first to model the
links selection problem as a constrained multi-armed bandit problem and design
an effective links selection algorithm by learning the links' multi-level
structure with provable \emph{sub-linear} regret and violation bounds. We
uncover the multi-level feedback structures of web links in two real-world
datasets. We also conduct extensive experiments on the datasets to compare our
proposed \textbf{LExp} algorithm with two state-of-the-art context-free bandit
algorithms and show that \textbf{LExp} algorithm is the most effective in links
selection while satisfying the constraint.
",Web Link Selection Optimization,web links selection for maximizing revenue
"  Consider the following estimation problem: there are $n$ entities, each with
an unknown parameter $p_i \in [0,1]$, and we observe $n$ independent random
variables, $X_1,\ldots,X_n$, with $X_i \sim $ Binomial$(t, p_i)$. How
accurately can one recover the ""histogram"" (i.e. cumulative density function)
of the $p_i$'s? While the empirical estimates would recover the histogram to
earth mover distance $\Theta(\frac{1}{\sqrt{t}})$ (equivalently, $\ell_1$
distance between the CDFs), we show that, provided $n$ is sufficiently large,
we can achieve error $O(\frac{1}{t})$ which is information theoretically
optimal. We also extend our results to the multi-dimensional parameter case,
capturing settings where each member of the population has multiple associated
parameters. Beyond the theoretical results, we demonstrate that the recovery
algorithm performs well in practice on a variety of datasets, providing
illuminating insights into several domains, including politics, sports
analytics, and variation in the gender ratio of offspring.
",Optimal Estimation of Distribution of Parameters,estimation of histograms from noisy data
"  Recently, much work has been done on extending the scope of online learning
and incremental stochastic optimization algorithms. In this paper we contribute
to this effort in two ways: First, based on a new regret decomposition and a
generalization of Bregman divergences, we provide a self-contained, modular
analysis of the two workhorses of online learning: (general) adaptive versions
of Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms.
The analysis is done with extra care so as not to introduce assumptions not
needed in the proofs and allows to combine, in a straightforward way, different
algorithmic ideas (e.g., adaptivity, optimism, implicit updates) and learning
settings (e.g., strongly convex or composite objectives). This way we are able
to reprove, extend and refine a large body of the literature, while keeping the
proofs concise. The second contribution is a byproduct of this careful
analysis: We present algorithms with improved variational bounds for smooth,
composite objectives, including a new family of optimistic MD algorithms with
only one projection step per round. Furthermore, we provide a simple extension
of adaptive regret bounds to practically relevant non-convex problem settings
with essentially no extra effort.
",Online Learning Algorithms,online learning and stochastic optimization
"  Regularized learning is a fundamental technique in online optimization,
machine learning and many other fields of computer science. A natural question
that arises in these settings is how regularized learning algorithms behave
when faced against each other. We study a natural formulation of this problem
by coupling regularized learning dynamics in zero-sum games. We show that the
system's behavior is Poincar\'e recurrent, implying that almost every
trajectory revisits any (arbitrarily small) neighborhood of its starting point
infinitely often. This cycling behavior is robust to the agents' choice of
regularization mechanism (each agent could be using a different regularizer),
to positive-affine transformations of the agents' utilities, and it also
persists in the case of networked competition, i.e., for zero-sum polymatrix
games.
",Regularized Learning in Zero-Sum Games,dynamics of regularized learning in zero-sum games
"  In June 2016, Apple announced that it will deploy differential privacy for
some user data collection in order to ensure privacy of user data, even from
Apple. The details of Apple's approach remained sparse. Although several
patents have since appeared hinting at the algorithms that may be used to
achieve differential privacy, they did not include a precise explanation of the
approach taken to privacy parameter choice. Such choice and the overall
approach to privacy budget use and management are key questions for
understanding the privacy protections provided by any deployment of
differential privacy.
  In this work, through a combination of experiments, static and dynamic code
analysis of macOS Sierra (Version 10.12) implementation, we shed light on the
choices Apple made for privacy budget management. We discover and describe
Apple's set-up for differentially private data processing, including the
overall data pipeline, the parameters used for differentially private
perturbation of each piece of data, and the frequency with which such data is
sent to Apple's servers.
  We find that although Apple's deployment ensures that the (differential)
privacy loss per each datum submitted to its servers is $1$ or $2$, the overall
privacy loss permitted by the system is significantly higher, as high as $16$
per day for the four initially announced applications of Emojis, New words,
Deeplinks and Lookup Hints. Furthermore, Apple renews the privacy budget
available every day, which leads to a possible privacy loss of 16 times the
number of days since user opt-in to differentially private data collection for
those four applications.
  We advocate that in order to claim the full benefits of differentially
private data collection, Apple must give full transparency of its
implementation, enable user choice in areas related to privacy loss, and set
meaningful defaults on the privacy loss permitted.
",Differential Privacy Implementation Analysis,apple's implementation of differential privacy
"  A new approach to the study of Generalized Graphs as semantic data structures
using machine learning techniques is presented. We show how vector
representations maintaining semantic characteristics of the original data can
be obtained from a given graph using neural encoding architectures and
considering the topological properties of the graph. Semantic features of these
new representations are tested by using some machine learning tasks and new
directions on efficient link discovery, entitity retrieval and long distance
query methodologies on large relational datasets are investigated using real
datasets.
  ----
  En este trabajo se presenta un nuevo enfoque en el contexto del aprendizaje
autom\'atico multi-relacional para el estudio de Grafos Generalizados. Se
muestra c\'omo se pueden obtener representaciones vectoriales que mantienen
caracter\'isticas sem\'anticas del grafo original utilizando codificadores
neuronales y considerando las propiedades topol\'ogicas del grafo. Adem\'as, se
eval\'uan las caracter\'isticas sem\'anticas capturadas por estas nuevas
representaciones y se investigan nuevas metodolog\'ias eficientes relacionadas
con Link Discovery, Entity Retrieval y consultas a larga distancia en grandes
conjuntos de datos relacionales haciendo uso de bases de datos reales.
",Graph Representation Learning,machine learning for generalized graphs
"  We prove an exact relationship between the optimal denoising function and the
data distribution in the case of additive Gaussian noise, showing that
denoising implicitly models the structure of data allowing it to be exploited
in the unsupervised learning of representations. This result generalizes a
known relationship [2], which is valid only in the limit of small corruption
noise.
",Denoising and Representation Learning,relationship between denoising functions and data distributions
"  Designing adaptive classifiers for an evolving data stream is a challenging
task due to the data size and its dynamically changing nature. Combining
individual classifiers in an online setting, the ensemble approach, is a
well-known solution. It is possible that a subset of classifiers in the
ensemble outperforms others in a time-varying fashion. However, optimum weight
assignment for component classifiers is a problem which is not yet fully
addressed in online evolving environments. We propose a novel data stream
ensemble classifier, called Geometrically Optimum and Online-Weighted Ensemble
(GOOWE), which assigns optimum weights to the component classifiers using a
sliding window containing the most recent data instances. We map vote scores of
individual classifiers and true class labels into a spatial environment. Based
on the Euclidean distance between vote scores and ideal-points, and using the
linear least squares (LSQ) solution, we present a novel, dynamic, and online
weighting approach. While LSQ is used for batch mode ensemble classifiers, it
is the first time that we adapt and use it for online environments by providing
a spatial modeling of online ensembles. In order to show the robustness of the
proposed algorithm, we use real-world datasets and synthetic data generators
using the MOA libraries. First, we analyze the impact of our weighting system
on prediction accuracy through two scenarios. Second, we compare GOOWE with 8
state-of-the-art ensemble classifiers in a comprehensive experimental
environment. Our experiments show that GOOWE provides improved reactions to
different types of concept drift compared to our baselines. The statistical
tests indicate a significant improvement in accuracy, with conservative time
and memory requirements.
",Dynamic Weighting in Online Ensemble Classifiers,adaptive ensemble learning for evolving data streams
"  Autonomous vehicles are highly complex systems, required to function reliably
in a wide variety of situations. Manually crafting software controllers for
these vehicles is difficult, but there has been some success in using deep
neural networks generated using machine-learning. However, deep neural networks
are opaque to human engineers, rendering their correctness very difficult to
prove manually; and existing automated techniques, which were not designed to
operate on neural networks, fail to scale to large systems. This paper focuses
on proving the adversarial robustness of deep neural networks, i.e. proving
that small perturbations to a correctly-classified input to the network cannot
cause it to be misclassified. We describe some of our recent and ongoing work
on verifying the adversarial robustness of networks, and discuss some of the
open questions we have encountered and how they might be addressed.
","""Verifying Adversarial Robustness in Autonomous Vehicle Neural Networks""",adversarial robustness of deep neural networks
"  This monograph aims at providing an introduction to key concepts, algorithms,
and theoretical results in machine learning. The treatment concentrates on
probabilistic models for supervised and unsupervised learning problems. It
introduces fundamental concepts and algorithms by building on first principles,
while also exposing the reader to more advanced topics with extensive pointers
to the literature, within a unified notation and mathematical framework. The
material is organized according to clearly defined categories, such as
discriminative and generative models, frequentist and Bayesian approaches,
exact and approximate inference, as well as directed and undirected models.
This monograph is meant as an entry point for researchers with a background in
probability and linear algebra.
",Machine Learning Fundamentals,machine learning
"  We introduce TensorFlow Agents, an efficient infrastructure paradigm for
building parallel reinforcement learning algorithms in TensorFlow. We simulate
multiple environments in parallel, and group them to perform the neural network
computation on a batch rather than individual observations. This allows the
TensorFlow execution engine to parallelize computation, without the need for
manual synchronization. Environments are stepped in separate Python processes
to progress them in parallel without interference of the global interpreter
lock. As part of this project, we introduce BatchPPO, an efficient
implementation of the proximal policy optimization algorithm. By open sourcing
TensorFlow Agents, we hope to provide a flexible starting point for future
projects that accelerates future research in the field.
",Parallel Reinforcement Learning,reinforcement learning
"  Markov Chain Monte Carlo (MCMC) sampling methods are widely used but often
encounter either slow convergence or biased sampling when applied to multimodal
high dimensional distributions. In this paper, we present a general framework
of improving classical MCMC samplers by employing a global optimization method.
The global optimization method first reduces a high dimensional search to an
one dimensional geodesic to find a starting point close to a local mode. The
search is accelerated and completed by using a local search method such as
BFGS. We modify the target distribution by extracting a local Gaussian
distribution aound the found mode. The process is repeated to find all the
modes during sampling on the fly. We integrate the optimization algorithm into
the Wormhole Hamiltonian Monte Carlo (WHMC) method. Experimental results show
that, when applied to high dimensional, multimodal Gaussian mixture models and
the network sensor localization problem, the proposed method achieves much
faster convergence, with relative error from the mean improved by about an
order of magnitude than WHMC in some cases.
",MCMC Sampling Optimization for Multimodal Distributions,improving markov chain monte carlo (mcmc) sampling for multimodal high-dimensional distributions
"  Convolutional sparse representations are a form of sparse representation with
a dictionary that has a structure that is equivalent to convolution with a set
of linear filters. While effective algorithms have recently been developed for
the convolutional sparse coding problem, the corresponding dictionary learning
problem is substantially more challenging. Furthermore, although a number of
different approaches have been proposed, the absence of thorough comparisons
between them makes it difficult to determine which of them represents the
current state of the art. The present work both addresses this deficiency and
proposes some new approaches that outperform existing ones in certain contexts.
A thorough set of performance comparisons indicates a very wide range of
performance differences among the existing and proposed methods, and clearly
identifies those that are the most effective.
",Convolutional Sparse Dictionary Learning,convolutional sparse representation
"  Explicitly or implicitly, most of dimensionality reduction methods need to
determine which samples are neighbors and the similarity between the neighbors
in the original highdimensional space. The projection matrix is then learned on
the assumption that the neighborhood information (e.g., the similarity) is
known and fixed prior to learning. However, it is difficult to precisely
measure the intrinsic similarity of samples in high-dimensional space because
of the curse of dimensionality. Consequently, the neighbors selected according
to such similarity might and the projection matrix obtained according to such
similarity and neighbors are not optimal in the sense of classification and
generalization. To overcome the drawbacks, in this paper we propose to let the
similarity and neighbors be variables and model them in low-dimensional space.
Both the optimal similarity and projection matrix are obtained by minimizing a
unified objective function. Nonnegative and sum-to-one constraints on the
similarity are adopted. Instead of empirically setting the regularization
parameter, we treat it as a variable to be optimized. It is interesting that
the optimal regularization parameter is adaptive to the neighbors in
low-dimensional space and has intuitive meaning. Experimental results on the
YALE B, COIL-100, and MNIST datasets demonstrate the effectiveness of the
proposed method.
",Adaptive Dimensionality Reduction,dimensionality reduction
"  In this paper, we present a simple analysis of {\bf fast rates} with {\it
high probability} of {\bf empirical minimization} for {\it stochastic composite
optimization} over a finite-dimensional bounded convex set with exponential
concave loss functions and an arbitrary convex regularization. To the best of
our knowledge, this result is the first of its kind. As a byproduct, we can
directly obtain the fast rate with {\it high probability} for exponential
concave empirical risk minimization with and without any convex regularization,
which not only extends existing results of empirical risk minimization but also
provides a unified framework for analyzing exponential concave empirical risk
minimization with and without {\it any} convex regularization. Our proof is
very simple only exploiting the covering number of a finite-dimensional bounded
set and a concentration inequality of random vectors.
",Stochastic Composite Optimization,stochastic composite optimization
"  The number of component classifiers chosen for an ensemble greatly impacts
the prediction ability. In this paper, we use a geometric framework for a
priori determining the ensemble size, which is applicable to most of existing
batch and online ensemble classifiers. There are only a limited number of
studies on the ensemble size examining Majority Voting (MV) and Weighted
Majority Voting (WMV). Almost all of them are designed for batch-mode, hardly
addressing online environments. Big data dimensions and resource limitations,
in terms of time and memory, make determination of ensemble size crucial,
especially for online environments. For the MV aggregation rule, our framework
proves that the more strong components we add to the ensemble, the more
accurate predictions we can achieve. For the WMV aggregation rule, our
framework proves the existence of an ideal number of components, which is equal
to the number of class labels, with the premise that components are completely
independent of each other and strong enough. While giving the exact definition
for a strong and independent classifier in the context of an ensemble is a
challenging task, our proposed geometric framework provides a theoretical
explanation of diversity and its impact on the accuracy of predictions. We
conduct a series of experimental evaluations to show the practical value of our
theorems and existing challenges.
",Ensemble Size Determination,ensemble size determination for machine learning classifiers
"  Residual Network (ResNet) is the state-of-the-art architecture that realizes
successful training of really deep neural network. It is also known that good
weight initialization of neural network avoids problem of vanishing/exploding
gradients. In this paper, simplified models of ResNets are analyzed. We argue
that goodness of ResNet is correlated with the fact that ResNets are relatively
insensitive to choice of initial weights. We also demonstrate how batch
normalization improves backpropagation of deep ResNets without tuning initial
values of weights.
",ResNet Weight Initialization Analysis,deep learning
"  Recent advances in deep learning have led various applications to
unprecedented achievements, which could potentially bring higher intelligence
to a broad spectrum of mobile and ubiquitous applications. Although existing
studies have demonstrated the effectiveness and feasibility of running deep
neural network inference operations on mobile and embedded devices, they
overlooked the reliability of mobile computing models. Reliability measurements
such as predictive uncertainty estimations are key factors for improving the
decision accuracy and user experience. In this work, we propose RDeepSense, the
first deep learning model that provides well-calibrated uncertainty estimations
for resource-constrained mobile and embedded devices. RDeepSense enables the
predictive uncertainty by adopting a tunable proper scoring rule as the
training criterion and dropout as the implicit Bayesian approximation, which
theoretically proves its correctness.To reduce the computational complexity,
RDeepSense employs efficient dropout and predictive distribution estimation
instead of model ensemble or sampling-based method for inference operations. We
evaluate RDeepSense with four mobile sensing applications using Intel Edison
devices. Results show that RDeepSense can reduce around 90% of the energy
consumption while producing superior uncertainty estimations and preserving at
least the same model accuracy compared with other state-of-the-art methods.
",Uncertainty Estimation in Mobile Deep Learning,deep learning for mobile and embedded devices
"  Power grids are critical infrastructure assets that face non-technical losses
(NTL) such as electricity theft or faulty meters. NTL may range up to 40% of
the total electricity distributed in emerging countries. Industrial NTL
detection systems are still largely based on expert knowledge when deciding
whether to carry out costly on-site inspections of customers. Electricity
providers are reluctant to move to large-scale deployments of automated systems
that learn NTL profiles from data due to the latter's propensity to suggest a
large number of unnecessary inspections. In this paper, we propose a novel
system that combines automated statistical decision making with expert
knowledge. First, we propose a machine learning framework that classifies
customers into NTL or non-NTL using a variety of features derived from the
customers' consumption data. The methodology used is specifically tailored to
the level of noise in the data. Second, in order to allow human experts to feed
their knowledge in the decision loop, we propose a method for visualizing
prediction results at various granularity levels in a spatial hologram. Our
approach allows domain experts to put the classification results into the
context of the data and to incorporate their knowledge for making the final
decisions of which customers to inspect. This work has resulted in appreciable
results on a real-world data set of 3.6M customers. Our system is being
deployed in a commercial NTL detection software.
",Non-Technical Loss Detection in Power Grids,power grid non-technical loss detection
"  Unordered feature sets are a nonstandard data structure that traditional
neural networks are incapable of addressing in a principled manner. Providing a
concatenation of features in an arbitrary order may lead to the learning of
spurious patterns or biases that do not actually exist. Another complication is
introduced if the number of features varies between each set. We propose
convolutional deep averaging networks (CDANs) for classifying and learning
representations of datasets whose instances comprise variable-size, unordered
feature sets. CDANs are efficient, permutation-invariant, and capable of
accepting sets of arbitrary size. We emphasize the importance of nonlinear
feature embeddings for obtaining effective CDAN classifiers and illustrate
their advantages in experiments versus linear embeddings and alternative
permutation-invariant and -equivariant architectures.
",Neural Networks for Unordered Feature Sets,unordered feature sets in neural networks
"  Sparse coding (SC) is attracting more and more attention due to its
comprehensive theoretical studies and its excellent performance in many signal
processing applications. However, most existing sparse coding algorithms are
nonconvex and are thus prone to becoming stuck into bad local minima,
especially when there are outliers and noisy data. To enhance the learning
robustness, in this paper, we propose a unified framework named Self-Paced
Sparse Coding (SPSC), which gradually include matrix elements into SC learning
from easy to complex. We also generalize the self-paced learning schema into
different levels of dynamic selection on samples, features and elements
respectively. Experimental results on real-world data demonstrate the efficacy
of the proposed algorithms.
",Robust Sparse Coding Algorithms,sparse coding algorithms
"  We study question-answering over semi-structured data. We introduce a new way
to apply the technique of semantic parsing by applying machine learning only to
provide annotations that the system infers to be missing; all the other parsing
logic is in the form of manually authored rules. In effect, the machine
learning is used to provide non-syntactic matches, a step that is ill-suited to
manual rules. The advantage of this approach is in its debuggability and in its
transparency to the end-user. We demonstrate the effectiveness of the approach
by achieving state-of-the-art performance of 40.42% accuracy on a standard
benchmark dataset over tables from Wikipedia.
",Semantic Parsing in Question-Answering Systems,question answering over semi-structured data
"  Gated Recurrent Unit (GRU) is a recently-developed variation of the long
short-term memory (LSTM) unit, both of which are types of recurrent neural
network (RNN). Through empirical evidence, both models have been proven to be
effective in a wide variety of machine learning tasks such as natural language
processing (Wen et al., 2015), speech recognition (Chorowski et al., 2015), and
text classification (Yang et al., 2016). Conventionally, like most neural
networks, both of the aforementioned RNN variants employ the Softmax function
as its final output layer for its prediction, and the cross-entropy function
for computing its loss. In this paper, we present an amendment to this norm by
introducing linear support vector machine (SVM) as the replacement for Softmax
in the final output layer of a GRU model. Furthermore, the cross-entropy
function shall be replaced with a margin-based function. While there have been
similar studies (Alalshekmubarak & Smith, 2013; Tang, 2013), this proposal is
primarily intended for binary classification on intrusion detection using the
2013 network traffic data from the honeypot systems of Kyoto University.
Results show that the GRU-SVM model performs relatively higher than the
conventional GRU-Softmax model. The proposed model reached a training accuracy
of ~81.54% and a testing accuracy of ~84.15%, while the latter was able to
reach a training accuracy of ~63.07% and a testing accuracy of ~70.75%. In
addition, the juxtaposition of these two final output layers indicate that the
SVM would outperform Softmax in prediction time - a theoretical implication
which was supported by the actual training and testing time in the study.
",Alternative Output Layers for Recurrent Neural Networks,deep learning for intrusion detection
"  We revisit the problem of \textit{online linear optimization} in case the set
of feasible actions is accessible through an approximated linear optimization
oracle with a factor $\alpha$ multiplicative approximation guarantee. This
setting is in particular interesting since it captures natural online
extensions of well-studied \textit{offline} linear optimization problems which
are NP-hard, yet admit efficient approximation algorithms. The goal here is to
minimize the $\alpha$\textit{-regret} which is the natural extension of the
standard \textit{regret} in \textit{online learning} to this setting.
  We present new algorithms with significantly improved oracle complexity for
both the full information and bandit variants of the problem. Mainly, for both
variants, we present $\alpha$-regret bounds of $O(T^{-1/3})$, were $T$ is the
number of prediction rounds, using only $O(\log{T})$ calls to the approximation
oracle per iteration, on average. These are the first results to obtain both
average oracle complexity of $O(\log{T})$ (or even poly-logarithmic in $T$) and
$\alpha$-regret bound $O(T^{-c})$ for a constant $c>0$, for both variants.
",Online Linear Optimization with Approximation Oracle,online linear optimization with approximated oracle
"  Emotion recognition from facial expressions is tremendously useful,
especially when coupled with smart devices and wireless multimedia
applications. However, the inadequate network bandwidth often limits the
spatial resolution of the transmitted video, which will heavily degrade the
recognition reliability. We develop a novel framework to achieve robust emotion
recognition from low bit rate video. While video frames are downsampled at the
encoder side, the decoder is embedded with a deep network model for joint
super-resolution (SR) and recognition. Notably, we propose a novel max-mix
training strategy, leading to a single ""One-for-All"" model that is remarkably
robust to a vast range of downsampling factors. That makes our framework well
adapted for the varied bandwidths in real transmission scenarios, without
hampering scalability or efficiency. The proposed framework is evaluated on the
AVEC 2016 benchmark, and demonstrates significantly improved stand-alone
recognition performance, as well as rate-distortion (R-D) performance, than
either directly recognizing from LR frames, or separating SR and recognition.
",Emotion Recognition from Low-Bitrate Video,emotion recognition from low bit rate video
"  Reinforcement Learning is divided in two main paradigms: model-free and
model-based. Each of these two paradigms has strengths and limitations, and has
been successfully applied to real world domains that are appropriate to its
corresponding strengths. In this paper, we present a new approach aimed at
bridging the gap between these two paradigms. We aim to take the best of the
two paradigms and combine them in an approach that is at the same time
data-efficient and cost-savvy. We do so by learning a probabilistic dynamics
model and leveraging it as a prior for the intertwined model-free optimization.
As a result, our approach can exploit the generality and structure of the
dynamics model, but is also capable of ignoring its inevitable inaccuracies, by
directly incorporating the evidence provided by the direct observation of the
cost. Preliminary results demonstrate that our approach outperforms purely
model-based and model-free approaches, as well as the approach of simply
switching from a model-based to a model-free setting.
",Hybrid Reinforcement Learning,hybrid reinforcement learning
"  Multivariate time-series modeling and forecasting is an important problem
with numerous applications. Traditional approaches such as VAR (vector
auto-regressive) models and more recent approaches such as RNNs (recurrent
neural networks) are indispensable tools in modeling time-series data. In many
multivariate time series modeling problems, there is usually a significant
linear dependency component, for which VARs are suitable, and a nonlinear
component, for which RNNs are suitable. Modeling such times series with only
VAR or only RNNs can lead to poor predictive performance or complex models with
large training times. In this work, we propose a hybrid model called R2N2
(Residual RNN), which first models the time series with a simple linear model
(like VAR) and then models its residual errors using RNNs. R2N2s can be trained
using existing algorithms for VARs and RNNs. Through an extensive empirical
evaluation on two real world datasets (aviation and climate domains), we show
that R2N2 is competitive, usually better than VAR or RNN, used alone. We also
show that R2N2 is faster to train as compared to an RNN, while requiring less
number of hidden units.
",Hybrid Time-Series Modeling,multivariate time-series modeling and forecasting
"  Reinforcement learning studies how to balance exploration and exploitation in
real-world systems, optimizing interactions with the world while simultaneously
learning how the world operates. One general class of algorithms for such
learning is the multi-armed bandit setting. Randomized probability matching,
based upon the Thompson sampling approach introduced in the 1930s, has recently
been shown to perform well and to enjoy provable optimality properties. It
permits generative, interpretable modeling in a Bayesian setting, where prior
knowledge is incorporated, and the computed posteriors naturally capture the
full state of knowledge. In this work, we harness the information contained in
the Bayesian posterior and estimate its sufficient statistics via sampling. In
several application domains, for example in health and medicine, each
interaction with the world can be expensive and invasive, whereas drawing
samples from the model is relatively inexpensive. Exploiting this viewpoint, we
develop a double sampling technique driven by the uncertainty in the learning
process: it favors exploitation when certain about the properties of each arm,
exploring otherwise. The proposed algorithm does not make any distributional
assumption and it is applicable to complex reward distributions, as long as
Bayesian posterior updates are computable. Utilizing the estimated posterior
sufficient statistics, double sampling autonomously balances the
exploration-exploitation tradeoff to make better informed decisions. We
empirically show its reduced cumulative regret when compared to
state-of-the-art alternatives in representative bandit settings.
",Multi-Armed Bandit Algorithms,reinforcement learning
"  In many biomedical, science, and engineering problems, one must sequentially
decide which action to take next so as to maximize rewards. One general class
of algorithms for optimizing interactions with the world, while simultaneously
learning how the world operates, is the multi-armed bandit setting and, in
particular, the contextual bandit case. In this setting, for each executed
action, one observes rewards that are dependent on a given 'context', available
at each interaction with the world. The Thompson sampling algorithm has
recently been shown to enjoy provable optimality properties for this set of
problems, and to perform well in real-world settings. It facilitates generative
and interpretable modeling of the problem at hand. Nevertheless, the design and
complexity of the model limit its application, since one must both sample from
the distributions modeled and calculate their expected rewards. We here show
how these limitations can be overcome using variational inference to
approximate complex models, applying to the reinforcement learning case
advances developed for the inference case in the machine learning community
over the past two decades. We consider contextual multi-armed bandit
applications where the true reward distribution is unknown and complex, which
we approximate with a mixture model whose parameters are inferred via
variational inference. We show how the proposed variational Thompson sampling
approach is accurate in approximating the true distribution, and attains
reduced regrets even with complex reward distributions. The proposed algorithm
is valuable for practical scenarios where restrictive modeling assumptions are
undesirable.
",Variational Inference in Contextual Bandits,variational thompson sampling for contextual multi-armed bandits
"  This paper studies the problem of estimating the grahpon model - the
underlying generating mechanism of a network. Graphon estimation arises in many
applications such as predicting missing links in networks and learning user
preferences in recommender systems. The graphon model deals with a random graph
of $n$ vertices such that each pair of two vertices $i$ and $j$ are connected
independently with probability $\rho \times f(x_i,x_j)$, where $x_i$ is the
unknown $d$-dimensional label of vertex $i$, $f$ is an unknown symmetric
function, and $\rho$ is a scaling parameter characterizing the graph sparsity.
Recent studies have identified the minimax error rate of estimating the graphon
from a single realization of the random graph. However, there exists a wide gap
between the known error rates of computationally efficient estimation
procedures and the minimax optimal error rate.
  Here we analyze a spectral method, namely universal singular value
thresholding (USVT) algorithm, in the relatively sparse regime with the average
vertex degree $n\rho=\Omega(\log n)$. When $f$ belongs to H\""{o}lder or Sobolev
space with smoothness index $\alpha$, we show the error rate of USVT is at most
$(n\rho)^{ -2 \alpha / (2\alpha+d)}$, approaching the minimax optimal error
rate $\log (n\rho)/(n\rho)$ for $d=1$ as $\alpha$ increases. Furthermore, when
$f$ is analytic, we show the error rate of USVT is at most $\log^d
(n\rho)/(n\rho)$. In the special case of stochastic block model with $k$
blocks, the error rate of USVT is at most $k/(n\rho)$, which is larger than the
minimax optimal error rate by at most a multiplicative factor $k/\log k$. This
coincides with the computational gap observed for community detection. A key
step of our analysis is to derive the eigenvalue decaying rate of the edge
probability matrix using piecewise polynomial approximations of the graphon
function $f$.
",Graphon Estimation in Networks,graphon estimation
"  Semi-supervised active clustering (SSAC) utilizes the knowledge of a domain
expert to cluster data points by interactively making pairwise ""same-cluster""
queries. However, it is impractical to ask human oracles to answer every
pairwise query. In this paper, we study the influence of allowing ""not-sure""
answers from a weak oracle and propose algorithms to efficiently handle
uncertainties. Different types of model assumptions are analyzed to cover
realistic scenarios of oracle abstraction. In the first model, random-weak
oracle, an oracle randomly abstains with a certain probability. We also
proposed two distance-weak oracle models which simulate the case of getting
confused based on the distance between two points in a pairwise query. For each
weak oracle model, we show that a small query complexity is adequate for the
effective $k$ means clustering with high probability. Sufficient conditions for
the guarantee include a $\gamma$-margin property of the data, and an existence
of a point close to each cluster center. Furthermore, we provide a sample
complexity with a reduced effect of the cluster's margin and only a logarithmic
dependency on the data dimension. Our results allow significantly less number
of same-cluster queries if the margin of the clusters is tight, i.e. $\gamma
\approx 1$. Experimental results on synthetic data show the effective
performance of our approach in overcoming uncertainties.
",Uncertainty Handling in Semi-Supervised Active Clustering,clustering with uncertain oracle feedback
"  This paper defines software fairness and discrimination and develops a
testing-based method for measuring if and how much software discriminates,
focusing on causality in discriminatory behavior. Evidence of software
discrimination has been found in modern software systems that recommend
criminal sentences, grant access to financial products, and determine who is
allowed to participate in promotions. Our approach, Themis, generates efficient
test suites to measure discrimination. Given a schema describing valid system
inputs, Themis generates discrimination tests automatically and does not
require an oracle. We evaluate Themis on 20 software systems, 12 of which come
from prior work with explicit focus on avoiding discrimination. We find that
(1) Themis is effective at discovering software discrimination, (2)
state-of-the-art techniques for removing discrimination from algorithms fail in
many situations, at times discriminating against as much as 98% of an input
subdomain, (3) Themis optimizations are effective at producing efficient test
suites for measuring discrimination, and (4) Themis is more efficient on
systems that exhibit more discrimination. We thus demonstrate that fairness
testing is a critical aspect of the software development cycle in domains with
possible discrimination and provide initial tools for measuring software
discrimination.
",Software Discrimination Testing,software fairness and discrimination
"  The infinite restricted Boltzmann machine (iRBM) is an extension of the
classic RBM. It enjoys a good property of automatically deciding the size of
the hidden layer according to specific training data. With sufficient training,
the iRBM can achieve a competitive performance with that of the classic RBM.
However, the convergence of learning the iRBM is slow, due to the fact that the
iRBM is sensitive to the ordering of its hidden units, the learned filters
change slowly from the left-most hidden unit to right. To break this dependency
between neighboring hidden units and speed up the convergence of training, a
novel training strategy is proposed. The key idea of the proposed training
strategy is randomly regrouping the hidden units before each gradient descent
step. Potentially, a mixing of infinite many iRBMs with different permutations
of the hidden units can be achieved by this learning method, which has a
similar effect of preventing the model from over-fitting as the dropout. The
original iRBM is also modified to be capable of carrying out discriminative
training. To evaluate the impact of our method on convergence speed of learning
and the model's generalization ability, several experiments have been performed
on the binarized MNIST and CalTech101 Silhouettes datasets. Experimental
results indicate that the proposed training strategy can greatly accelerate
learning and enhance generalization ability of iRBMs.
",Training Strategies for Restricted Boltzmann Machines,infinite restricted boltzmann machines
"  We present a concrete design for Solomonoff's incremental machine learning
system suitable for desktop computers. We use R5RS Scheme and its standard
library with a few omissions as the reference machine. We introduce a Levin
Search variant based on a stochastic Context Free Grammar together with new
update algorithms that use the same grammar as a guiding probability
distribution for incremental machine learning. The updates include adjusting
production probabilities, re-using previous solutions, learning programming
idioms and discovery of frequent subprograms. The issues of extending the a
priori probability distribution and bootstrapping are discussed. We have
implemented a good portion of the proposed algorithms. Experiments with toy
problems show that the update algorithms work as expected.
",Incremental Machine Learning System Design,machine learning
"  Deep learning has become the state of the art approach in many machine
learning problems such as classification. It has recently been shown that deep
learning is highly vulnerable to adversarial perturbations. Taking the camera
systems of self-driving cars as an example, small adversarial perturbations can
cause the system to make errors in important tasks, such as classifying traffic
signs or detecting pedestrians. Hence, in order to use deep learning without
safety concerns a proper defense strategy is required. We propose to use
ensemble methods as a defense strategy against adversarial perturbations. We
find that an attack leading one model to misclassify does not imply the same
for other networks performing the same task. This makes ensemble methods an
attractive defense strategy against adversarial attacks. We empirically show
for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve
the accuracy of neural networks on test data but also increase their robustness
against adversarial perturbations.
",Adversarial Defense in Deep Learning,artificial intelligence security
"  How should a firm allocate its limited interviewing resources to select the
optimal cohort of new employees from a large set of job applicants? How should
that firm allocate cheap but noisy resume screenings and expensive but in-depth
in-person interviews? We view this problem through the lens of combinatorial
pure exploration (CPE) in the multi-armed bandit setting, where a central
learning agent performs costly exploration of a set of arms before selecting a
final subset with some combinatorial structure. We generalize a recent CPE
algorithm to the setting where arm pulls can have different costs and return
different levels of information. We then prove theoretical upper bounds for a
general class of arm-pulling strategies in this new setting. We apply our
general algorithm to a real-world problem with combinatorial structure:
incorporating diversity into university admissions. We take real data from
admissions at one of the largest US-based computer science graduate programs
and show that a simulation of our algorithm produces a cohort with hiring
overall utility while spending comparable budget to the current admissions
process at that university.
",Optimal Resource Allocation in Employee Selection,optimal resource allocation in employee selection
"  For complex segmentation tasks, fully automatic systems are inherently
limited in their achievable accuracy for extracting relevant objects.
Especially in cases where only few data sets need to be processed for a highly
accurate result, semi-automatic segmentation techniques exhibit a clear benefit
for the user. One area of application is medical image processing during an
intervention for a single patient. We propose a learning-based cooperative
segmentation approach which includes the computing entity as well as the user
into the task. Our system builds upon a state-of-the-art fully convolutional
artificial neural network (FCN) as well as an active user model for training.
During the segmentation process, a user of the trained system can iteratively
add additional hints in form of pictorial scribbles as seed points into the FCN
system to achieve an interactive and precise segmentation result. The
segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches
can yield superior results compared to networks without the user input channel
component, due to a consistent improvement in segmentation quality after each
interaction.
",Interactive Segmentation in Medical Imaging,medical image segmentation
"  Medical image analysis and computer-assisted intervention problems are
increasingly being addressed with deep-learning-based solutions. Established
deep-learning platforms are flexible but do not provide specific functionality
for medical image analysis and adapting them for this application requires
substantial implementation effort. Thus, there has been substantial duplication
of effort and incompatible infrastructure developed across many research
groups. This work presents the open-source NiftyNet platform for deep learning
in medical imaging. The ambition of NiftyNet is to accelerate and simplify the
development of these solutions, and to provide a common mechanism for
disseminating research outputs for the community to use, adapt and build upon.
  NiftyNet provides a modular deep-learning pipeline for a range of medical
imaging applications including segmentation, regression, image generation and
representation learning applications. Components of the NiftyNet pipeline
including data loading, data augmentation, network architectures, loss
functions and evaluation metrics are tailored to, and take advantage of, the
idiosyncracies of medical image analysis and computer-assisted intervention.
NiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D
and 3D images and computational graphs by default.
  We present 3 illustrative medical image analysis applications built using
NiftyNet: (1) segmentation of multiple abdominal organs from computed
tomography; (2) image regression to predict computed tomography attenuation
maps from brain magnetic resonance images; and (3) generation of simulated
ultrasound images for specified anatomical poses.
  NiftyNet enables researchers to rapidly develop and distribute deep learning
solutions for segmentation, regression, image generation and representation
learning applications, or extend the platform to new applications.
",Deep Learning in Medical Imaging,medical imaging and deep learning
"  For distributed computing environment, we consider the empirical risk
minimization problem and propose a distributed and communication-efficient
Newton-type optimization method. At every iteration, each worker locally finds
an Approximate NewTon (ANT) direction, which is sent to the main driver. The
main driver, then, averages all the ANT directions received from workers to
form a {\it Globally Improved ANT} (GIANT) direction. GIANT is highly
communication efficient and naturally exploits the trade-offs between local
computations and global communications in that more local computations result
in fewer overall rounds of communications. Theoretically, we show that GIANT
enjoys an improved convergence rate as compared with first-order methods and
existing distributed Newton-type methods. Further, and in sharp contrast with
many existing distributed Newton-type methods, as well as popular first-order
methods, a highly advantageous practical feature of GIANT is that it only
involves one tuning parameter. We conduct large-scale experiments on a computer
cluster and, empirically, demonstrate the superior performance of GIANT.
",Distributed Optimization Methods,distributed optimization
"  Inspired by the generation power of generative adversarial networks (GANs) in
image domains, we introduce a novel hierarchical architecture for learning
characteristic topological features from a single arbitrary input graph via
GANs. The hierarchical architecture consisting of multiple GANs preserves both
local and global topological features and automatically partitions the input
graph into representative stages for feature learning. The stages facilitate
reconstruction and can be used as indicators of the importance of the
associated topological structures. Experiments show that our method produces
subgraphs retaining a wide range of topological features, even in early
reconstruction stages (unlike a single GAN, which cannot easily identify such
features, let alone reconstruct the original graph). This paper is firstline
research on combining the use of GANs and graph topological analysis.
",Graph Topology Learning with GANs,graph neural networks
"  Research has shown that false alarms constitute more than 80% of the alarms
triggered in the intensive care unit (ICU). The high false arrhythmia alarm
rate has severe implications such as disruption of patient care, caregiver
alarm fatigue, and desensitization from clinical staff to real life-threatening
alarms. A method to reduce the false alarm rate would therefore greatly benefit
patients as well as nurses in their ability to provide care. We here develop
and describe a robust false arrhythmia alarm reduction system for use in the
ICU. Building off of work previously described in the literature, we make use
of signal processing and machine learning techniques to identify true and false
alarms for five arrhythmia types. This baseline algorithm alone is able to
perform remarkably well, with a sensitivity of 0.908, a specificity of 0.838,
and a PhysioNet/CinC challenge score of 0.756. We additionally explore dynamic
time warping techniques on both the entire alarm signal as well as on a
beat-by-beat basis in an effort to improve performance of ventricular
tachycardia, which has in the literature been one of the hardest arrhythmias to
classify. Such an algorithm with strong performance and efficiency could
potentially be translated for use in the ICU to promote overall patient care
and recovery.
",False Arrhythmia Alarm Reduction in ICU,reduction of false alarms in intensive care units
"  We consider the problem of detecting a few targets among a large number of
hierarchical data streams. The data streams are modeled as random processes
with unknown and potentially heavy-tailed distributions. The objective is an
active inference strategy that determines, sequentially, which data stream to
collect samples from in order to minimize the sample complexity under a
reliability constraint. We propose an active inference strategy that induces a
biased random walk on the tree-structured hierarchy based on confidence bounds
of sample statistics. We then establish its order optimality in terms of both
the size of the search space (i.e., the number of data streams) and the
reliability requirement. The results find applications in hierarchical heavy
hitter detection, noisy group testing, and adaptive sampling for active
learning, classification, and stochastic root finding.
",Active Inference in Hierarchical Data Streams,active inference and sampling for hierarchical data streams
"  Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been
attracting a lot of attention in recent studies. It has been shown that for
many state of the art DNNs performing image classification there exist
universal adversarial perturbations --- image-agnostic perturbations mere
addition of which to natural images with high probability leads to their
misclassification. In this work we propose a new algorithm for constructing
such universal perturbations. Our approach is based on computing the so-called
$(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a
network. Resulting perturbations present interesting visual patterns, and by
using only 64 images we were able to construct universal perturbations with
more than 60 \% fooling rate on the dataset consisting of 50000 images. We also
investigate a correlation between the maximal singular value of the Jacobian
matrix and the fooling rate of the corresponding singular vector, and show that
the constructed perturbations generalize across networks.
",Adversarial Attacks on Image Classification Models,adversarial attacks on deep neural networks
"  We study the problem of causal structure learning when the experimenter is
limited to perform at most $k$ non-adaptive experiments of size $1$. We
formulate the problem of finding the best intervention target set as an
optimization problem, which aims to maximize the average number of edges whose
directions are resolved. We prove that the corresponding objective function is
submodular and a greedy algorithm suffices to achieve
$(1-\frac{1}{e})$-approximation of the optimal value. We further present an
accelerated variant of the greedy algorithm, which can lead to orders of
magnitude performance speedup. We validate our proposed approach on synthetic
and real graphs. The results show that compared to the purely observational
setting, our algorithm orients the majority of the edges through a considerably
small number of interventions.
",Causal Structure Learning with Limited Interventions,causal structure learning with limited experiments
"  In this paper we present preliminary work examining the relationship between
the formation of expectations and the realization of musical performances,
paying particular attention to expressive tempo and dynamics. To compute
features that reflect what a listener is expecting to hear, we employ a
computational model of auditory expectation called the Information Dynamics of
Music model (IDyOM). We then explore how well these expectancy features -- when
combined with score descriptors using the Basis-Function modeling approach --
can predict expressive tempo and dynamics in a dataset of Mozart piano sonata
performances. Our results suggest that using expectancy features significantly
improves the predictions for tempo.
",Music Expectation Modeling,music information retrieval
"  Genome-wide association studies (GWA studies or GWAS) investigate the
relationships between genetic variants such as single-nucleotide polymorphisms
(SNPs) and individual traits. Recently, incorporating biological priors
together with machine learning methods in GWA studies has attracted increasing
attention. However, in real-world, nucleotide-level bio-priors have not been
well-studied to date. Alternatively, studies at gene-level, for example,
protein--protein interactions and pathways, are more rigorous and legitimate,
and it is potentially beneficial to utilize such gene-level priors in GWAS. In
this paper, we proposed a novel two-level structured sparse model, called
Sparse Group Lasso with Group-level Graph structure (SGLGG), for GWAS. It can
be considered as a sparse group Lasso along with a group-level graph Lasso.
Essentially, SGLGG penalizes the nucleotide-level sparsity as well as takes
advantages of gene-level priors (both gene groups and networks), to identifying
phenotype-associated risk SNPs. We employ the alternating direction method of
multipliers algorithm to optimize the proposed model. Our experiments on the
Alzheimer's Disease Neuroimaging Initiative whole genome sequence data and
neuroimage data demonstrate the effectiveness of SGLGG. As a regression model,
it is competitive to the state-of-the-arts sparse models; as a variable
selection method, SGLGG is promising for identifying Alzheimer's
disease-related risk SNPs.
",Integrating Biological Priors in Genome-Wide Association Studies,gene expression and genomic analysis
"  We obtain a denoising loss bound of the recently proposed neural network
based universal discrete denoiser, Neural DUDE, which can adaptively learn its
parameters solely from the noise-corrupted data, by minimizing the
\emph{empirical estimated loss}. The resulting bound resembles the
generalization error bound of the standard empirical risk minimizers (ERM) in
supervised learning, and we show that the well-known bias-variance tradeoff
also exists in our loss bound. The key tool we develop is the concentration of
the unbiased estimated loss on the true denoising loss, which is shown to hold
\emph{uniformly} for \emph{all} bounded network parameters and \emph{all}
underlying clean sequences. For proving our main results, we make a novel
application of the tools from the statistical learning theory. Finally, we show
that the hyperparameters of Neural DUDE can be chosen from a small validation
set to significantly improve the denoising performance, as predicted by the
theoretical result of this paper.
",Neural Network Denoising,machine learning theory
"  Speech enhancement model is used to map a noisy speech to a clean speech. In
the training stage, an objective function is often adopted to optimize the
model parameters. However, in most studies, there is an inconsistency between
the model optimization criterion and the evaluation criterion on the enhanced
speech. For example, in measuring speech intelligibility, most of the
evaluation metric is based on a short-time objective intelligibility (STOI)
measure, while the frame based minimum mean square error (MMSE) between
estimated and clean speech is widely used in optimizing the model. Due to the
inconsistency, there is no guarantee that the trained model can provide optimal
performance in applications. In this study, we propose an end-to-end
utterance-based speech enhancement framework using fully convolutional neural
networks (FCN) to reduce the gap between the model optimization and evaluation
criterion. Because of the utterance-based optimization, temporal correlation
information of long speech segments, or even at the entire utterance level, can
be considered when perception-based objective functions are used for the direct
optimization. As an example, we implement the proposed FCN enhancement
framework to optimize the STOI measure. Experimental results show that the STOI
of test speech is better than conventional MMSE-optimized speech due to the
consistency between the training and evaluation target. Moreover, by
integrating the STOI in model optimization, the intelligibility of human
subjects and automatic speech recognition (ASR) system on the enhanced speech
is also substantially improved compared to those generated by the MMSE
criterion.
",Speech Enhancement Model Optimization Criteria,speech enhancement
"  Multi-view graph embedding has become a widely studied problem in the area of
graph learning. Most of the existing works on multi-view graph embedding aim to
find a shared common node embedding across all the views of the graph by
combining the different views in a specific way. Hub detection, as another
essential topic in graph mining has also drawn extensive attentions in recent
years, especially in the context of brain network analysis. Both the graph
embedding and hub detection relate to the node clustering structure of graphs.
The multi-view graph embedding usually implies the node clustering structure of
the graph based on the multiple views, while the hubs are the boundary-spanning
nodes across different node clusters in the graph and thus may potentially
influence the clustering structure of the graph. However, none of the existing
works in multi-view graph embedding considered the hubs when learning the
multi-view embeddings. In this paper, we propose to incorporate the hub
detection task into the multi-view graph embedding framework so that the two
tasks could benefit each other. Specifically, we propose an auto-weighted
framework of Multi-view Graph Embedding with Hub Detection (MVGE-HD) for brain
network analysis. The MVGE-HD framework learns a unified graph embedding across
all the views while reducing the potential influence of the hubs on blurring
the boundaries between node clusters in the graph, thus leading to a clear and
discriminative node clustering structure for the graph. We apply MVGE-HD on two
real multi-view brain network datasets (i.e., HIV and Bipolar). The
experimental results demonstrate the superior performance of the proposed
framework in brain network analysis for clinical investigation and application.
",Multi-View Graph Embedding with Hub Detection,multi-view graph embedding with hub detection
"  Community recovery is a central problem that arises in a wide variety of
applications such as network clustering, motion segmentation, face clustering
and protein complex detection. The objective of the problem is to cluster data
points into distinct communities based on a set of measurements, each of which
is associated with the values of a certain number of data points. While most of
the prior works focus on a setting in which the number of data points involved
in a measurement is two, this work explores a generalized setting in which the
number can be more than two. Motivated by applications particularly in machine
learning and channel coding, we consider two types of measurements: (1)
homogeneity measurement which indicates whether or not the associated data
points belong to the same community; (2) parity measurement which denotes the
modulo-2 sum of the values of the data points. Such measurements are possibly
corrupted by Bernoulli noise. We characterize the fundamental limits on the
number of measurements required to reconstruct the communities for the
considered models.
",Community Detection in Noisy Measurements,community detection and clustering
"  Calculation of near-neighbor interactions among high dimensional, irregularly
distributed data points is a fundamental task to many graph-based or
kernel-based machine learning algorithms and applications. Such calculations,
involving large, sparse interaction matrices, expose the limitation of
conventional data-and-computation reordering techniques for improving space and
time locality on modern computer memory hierarchies. We introduce a novel
method for obtaining a matrix permutation that renders a desirable sparsity
profile. The method is distinguished by the guiding principle to obtain a
profile that is block-sparse with dense blocks. Our profile model and measure
capture the essential properties affecting space and time locality, and permit
variation in sparsity profile without imposing a restriction to a fixed
pattern. The second distinction lies in an efficient algorithm for obtaining a
desirable profile, via exploring and exploiting multi-scale cluster structure
hidden in but intrinsic to the data. The algorithm accomplishes its task with
key components for lower-dimensional embedding with data-specific principal
feature axes, hierarchical data clustering, multi-level matrix compression
storage, and multi-level interaction computations. We provide experimental
results from case studies with two important data analysis algorithms. The
resulting performance is remarkably comparable to the BLAS performance for the
best-case interaction governed by a regularly banded matrix with the same
sparsity.
",Matrix Permutation for Efficient Sparse Matrix Computation,matrix permutation for near-neighbor interactions
"  Randomized experiments have been critical tools of decision making for
decades. However, subjects can show significant heterogeneity in response to
treatments in many important applications. Therefore it is not enough to simply
know which treatment is optimal for the entire population. What we need is a
model that correctly customize treatment assignment base on subject
characteristics. The problem of constructing such models from randomized
experiments data is known as Uplift Modeling in the literature. Many algorithms
have been proposed for uplift modeling and some have generated promising
results on various data sets. Yet little is known about the theoretical
properties of these algorithms. In this paper, we propose a new tree-based
ensemble algorithm for uplift modeling. Experiments show that our algorithm can
achieve competitive results on both synthetic and industry-provided data. In
addition, by properly tuning the ""node size"" parameter, our algorithm is proved
to be consistent under mild regularity conditions. This is the first consistent
algorithm for uplift modeling that we are aware of.
",Uplift Modeling in Randomized Experiments,uplift modeling
"  In this paper, we propose a recurrent neural network (RNN) with residual
attention (RRA) to learn long-range dependencies from sequential data. We
propose to add residual connections across timesteps to RNN, which explicitly
enhances the interaction between current state and hidden states that are
several timesteps apart. This also allows training errors to be directly
back-propagated through residual connections and effectively alleviates
gradient vanishing problem. We further reformulate an attention mechanism over
residual connections. An attention gate is defined to summarize the individual
contribution from multiple previous hidden states in computing the current
state. We evaluate RRA on three tasks: the adding problem, pixel-by-pixel MNIST
classification and sentiment analysis on the IMDB dataset. Our experiments
demonstrate that RRA yields better performance, faster convergence and more
stable training compared to a standard LSTM network. Furthermore, RRA shows
highly competitive performance to the state-of-the-art methods.
",Residual Attention in RNNs,recurrent neural network (rnn) architectures
"  The goal of this paper is to propose novel strategies for adaptive learning
of signals defined over graphs, which are observed over a (randomly
time-varying) subset of vertices. We recast two classical adaptive algorithms
in the graph signal processing framework, namely, the least mean squares (LMS)
and the recursive least squares (RLS) adaptive estimation strategies. For both
methods, a detailed mean-square analysis illustrates the effect of random
sampling on the adaptive reconstruction capability and the steady-state
performance. Then, several probabilistic sampling strategies are proposed to
design the sampling probability at each node in the graph, with the aim of
optimizing the tradeoff between steady-state performance, graph sampling rate,
and convergence rate of the adaptive algorithms. Finally, a distributed RLS
strategy is derived and is shown to be convergent to its centralized
counterpart. Numerical simulations carried out over both synthetic and real
data illustrate the good performance of the proposed sampling and
reconstruction strategies for (possibly distributed) adaptive learning of
signals defined over graphs.
",Adaptive Graph Signal Processing,adaptive graph signal processing
"  Despite outperforming the human in many tasks, deep neural network models are
also criticized for the lack of transparency and interpretability in decision
making. The opaqueness results in uncertainty and low confidence when deploying
such a model in model sharing scenarios, when the model is developed by a third
party. For a supervised machine learning model, sharing training process
including training data provides an effective way to gain trust and to better
understand model predictions. However, it is not always possible to share all
training data due to privacy and policy constraints. In this paper, we propose
a method to disclose a small set of training data that is just sufficient for
users to get the insight of a complicated model. The method constructs a
boundary tree using selected training data and the tree is able to approximate
the complicated model with high fidelity. We show that traversing data points
in the tree gives users significantly better understanding of the model and
paves the way for trustworthy model sharing.
",Interpretable Model Sharing,model interpretability
"  Predicating macroscopic influences of drugs on human body, like efficacy and
toxicity, is a central problem of small-molecule based drug discovery.
Molecules can be represented as an undirected graph, and we can utilize graph
convolution networks to predication molecular properties. However, graph
convolutional networks and other graph neural networks all focus on learning
node-level representation rather than graph-level representation. Previous
works simply sum all feature vectors for all nodes in the graph to obtain the
graph feature vector for drug predication. In this paper, we introduce a dummy
super node that is connected with all nodes in the graph by a directed edge as
the representation of the graph and modify the graph operation to help the
dummy super node learn graph-level feature. Thus, we can handle graph-level
classification and regression in the same way as node-level classification and
regression. In addition, we apply focal loss to address class imbalance in drug
datasets. The experiments on MoleculeNet show that our method can effectively
improve the performance of molecular properties predication.
",Graph-Based Drug Discovery,graph convolutional networks in drug discovery
"  In this paper we introduce a natural image prior that directly represents a
Gaussian-smoothed version of the natural image distribution. We include our
prior in a formulation of image restoration as a Bayes estimator that also
allows us to solve noise-blind image restoration problems. We show that the
gradient of our prior corresponds to the mean-shift vector on the natural image
distribution. In addition, we learn the mean-shift vector field using denoising
autoencoders, and use it in a gradient descent approach to perform Bayes risk
minimization. We demonstrate competitive results for noise-blind deblurring,
super-resolution, and demosaicing.
",Image Restoration with Bayesian Priors,image restoration
"  Instance- and Label-dependent label Noise (ILN) widely exists in real-world
datasets but has been rarely studied. In this paper, we focus on Bounded
Instance- and Label-dependent label Noise (BILN), a particular case of ILN
where the label noise rates -- the probabilities that the true labels of
examples flip into the corrupted ones -- have upper bound less than $1$.
Specifically, we introduce the concept of distilled examples, i.e. examples
whose labels are identical with the labels assigned for them by the Bayes
optimal classifier, and prove that under certain conditions classifiers learnt
on distilled examples will converge to the Bayes optimal classifier. Inspired
by the idea of learning with distilled examples, we then propose a learning
algorithm with theoretical guarantees for its robustness to BILN. At last,
empirical evaluations on both synthetic and real-world datasets show
effectiveness of our algorithm in learning with BILN.
",Label Noise in Machine Learning,learning with bounded instance- and label-dependent label noise
"  We propose in this paper a novel approach to tackle the problem of mode
collapse encountered in generative adversarial network (GAN). Our idea is
intuitive but proven to be very effective, especially in addressing some key
limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and
reverse KL divergences into a unified objective function, thus it exploits the
complementary statistical properties from these divergences to effectively
diversify the estimated density in capturing multi-modes. We term our method
dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has
two discriminators; and together with a generator, it also has the analogy of a
minimax game, wherein a discriminator rewards high scores for samples from data
distribution whilst another discriminator, conversely, favoring data from the
generator, and the generator produces data to fool both two discriminators. We
develop theoretical analysis to show that, given the maximal discriminators,
optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL
divergences between data distribution and the distribution induced from the
data generated by the generator, hence effectively avoiding the mode collapsing
problem. We conduct extensive experiments on synthetic and real-world
large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made
our best effort to compare our D2GAN with the latest state-of-the-art GAN's
variants in comprehensive qualitative and quantitative evaluations. The
experimental results demonstrate the competitive and superior performance of
our approach in generating good quality and diverse samples over baselines, and
the capability of our method to scale up to ImageNet database.
",Mode Collapse in GANs,generative adversarial networks
"  This paper aims to investigate direct imitation learning from human drivers
for the task of lane keeping assistance in highway and country roads using
grayscale images from a single front view camera. The employed method utilizes
convolutional neural networks (CNN) to act as a policy that is driving a
vehicle. The policy is successfully learned via imitation learning using
real-world data collected from human drivers and is evaluated in closed-loop
simulated environments, demonstrating good driving behaviour and a robustness
for domain changes. Evaluation is based on two proposed performance metrics
measuring how well the vehicle is positioned in a lane and the smoothness of
the driven trajectory.
",Imitation Learning for Lane Keeping Assistance,autonomous vehicle lane keeping assistance
"  We investigate the learning of quantitative structure activity relationships
(QSARs) as a case-study of meta-learning. This application area is of the
highest societal importance, as it is a key step in the development of new
medicines. The standard QSAR learning problem is: given a target (usually a
protein) and a set of chemical compounds (small molecules) with associated
bioactivities (e.g. inhibition of the target), learn a predictive mapping from
molecular representation to activity. Although almost every type of machine
learning method has been applied to QSAR learning there is no agreed single
best way of learning QSARs, and therefore the problem area is well-suited to
meta-learning. We first carried out the most comprehensive ever comparison of
machine learning methods for QSAR learning: 18 regression methods, 6 molecular
representations, applied to more than 2,700 QSAR problems. (These results have
been made publicly available on OpenML and represent a valuable resource for
testing novel meta-learning methods.) We then investigated the utility of
algorithm selection for QSAR problems. We found that this meta-learning
approach outperformed the best individual QSAR learning method (random forests
using a molecular fingerprint representation) by up to 13%, on average. We
conclude that meta-learning outperforms base-learning methods for QSAR
learning, and as this investigation is one of the most extensive ever
comparisons of base and meta-learning methods ever made, it provides evidence
for the general effectiveness of meta-learning over base-learning.
",Meta-Learning in QSAR Model Selection,machine learning for quantitative structure activity relationships (qsars)
"  The sample complexity of learning a Boolean-valued function class is
precisely characterized by its Rademacher complexity. This has little bearing,
however, on the sample complexity of \emph{efficient} agnostic learning.
  We introduce \emph{refutation complexity}, a natural computational analog of
Rademacher complexity of a Boolean concept class and show that it exactly
characterizes the sample complexity of \emph{efficient} agnostic learning.
Informally, refutation complexity of a class $\mathcal{C}$ is the minimum
number of example-label pairs required to efficiently distinguish between the
case that the labels correlate with the evaluation of some member of
$\mathcal{C}$ (\emph{structure}) and the case where the labels are i.i.d.
Rademacher random variables (\emph{noise}). The easy direction of this
relationship was implicitly used in the recent framework for improper PAC
learning lower bounds of Daniely and co-authors via connections to the hardness
of refuting random constraint satisfaction problems. Our work can be seen as
making the relationship between agnostic learning and refutation implicit in
their work into an explicit equivalence. In a recent, independent work, Salil
Vadhan discovered a similar relationship between refutation and PAC-learning in
the realizable (i.e. noiseless) case.
",Refutation Complexity in Agnostic Learning,machine learning
"  In this paper, we consider the use of structure learning methods for
probabilistic graphical models to identify statistical dependencies in
high-dimensional physical processes. Such processes are often synthetically
characterized using PDEs (partial differential equations) and are observed in a
variety of natural phenomena, including geoscience data capturing atmospheric
and hydrological phenomena. Classical structure learning approaches such as the
PC algorithm and variants are challenging to apply due to their high
computational and sample requirements. Modern approaches, often based on sparse
regression and variants, do come with finite sample guarantees, but are usually
highly sensitive to the choice of hyper-parameters, e.g., parameter $\lambda$
for sparsity inducing constraint or regularization. In this paper, we present
ACLIME-ADMM, an efficient two-step algorithm for adaptive structure learning,
which estimates an edge specific parameter $\lambda_{ij}$ in the first step,
and uses these parameters to learn the structure in the second step. Both steps
of our algorithm use (inexact) ADMM to solve suitable linear programs, and all
iterations can be done in closed form in an efficient block parallel manner. We
compare ACLIME-ADMM with baselines on both synthetic data simulated by partial
differential equations (PDEs) that model advection-diffusion processes, and
real data (50 years) of daily global geopotential heights to study information
flow in the atmosphere. ACLIME-ADMM is shown to be efficient, stable, and
competitive, usually better than the baselines especially on difficult
problems. On real data, ACLIME-ADMM recovers the underlying structure of global
atmospheric circulation, including switches in wind directions at the equator
and tropics entirely from the data.
",Structure Learning in High-Dimensional Physical Processes,structure learning in high-dimensional physical processes
"  The recent development of CNN-based image dehazing has revealed the
effectiveness of end-to-end modeling. However, extending the idea to end-to-end
video dehazing has not been explored yet. In this paper, we propose an
End-to-End Video Dehazing Network (EVD-Net), to exploit the temporal
consistency between consecutive video frames. A thorough study has been
conducted over a number of structure options, to identify the best temporal
fusion strategy. Furthermore, we build an End-to-End United Video Dehazing and
Detection Network(EVDD-Net), which concatenates and jointly trains EVD-Net with
a video object detection model. The resulting augmented end-to-end pipeline has
demonstrated much more stable and accurate detection results in hazy video.
",Video Dehazing,computer vision
"  In this paper, we propose surrogate agent-environment interface (SAEI) in
reinforcement learning. We also state that learning based on probability
surrogate agent-environment interface provides optimal policy of task
agent-environment interface. We introduce surrogate probability action and
develop the probability surrogate action deterministic policy gradient (PSADPG)
algorithm based on SAEI. This algorithm enables continuous control of discrete
action. The experiments show PSADPG achieves the performance of DQN in certain
tasks with the stochastic optimal policy nature in the initial training stage.
",Reinforcement Learning with Surrogate Agent-Environment Interface,reinforcement learning: surrogate agent-environment interface
"  We generalize a support vector machine to a support spinor machine by using
the mathematical structure of wedge product over vector machine in order to
extend field from vector field to spinor field. The separated hyperplane is
extended to Kolmogorov space in time series data which allow us to extend a
structure of support vector machine to a support tensor machine and a support
tensor machine moduli space. Our performance test on support spinor machine is
done over one class classification of end point in physiology state of time
series data after empirical mode analysis and compared with support vector
machine test. We implement algorithm of support spinor machine by using
Holo-Hilbert amplitude modulation for fully nonlinear and nonstationary time
series data analysis.
",Support Spinor Machine for Time Series Analysis,machine learning for time series data
"  The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.
",Analyzing Effective Advertisements,recommender systems and advertisement effectiveness
"  Although neural machine translation (NMT) with the encoder-decoder framework
has achieved great success in recent times, it still suffers from some
drawbacks: RNNs tend to forget old information which is often useful and the
encoder only operates through words without considering word relationship. To
solve these problems, we introduce a relation networks (RN) into NMT to refine
the encoding representations of the source. In our method, the RN first
augments the representation of each source word with its neighbors and reasons
all the possible pairwise relations between them. Then the source
representations and all the relations are fed to the attention module and the
decoder together, keeping the main encoder-decoder architecture unchanged.
Experiments on two Chinese-to-English data sets in different scales both show
that our method can outperform the competitive baselines significantly.
",Relation Networks in Neural Machine Translation,neural machine translation with relation networks
"  In this paper, we propose and study opportunistic bandits - a new variant of
bandits where the regret of pulling a suboptimal arm varies under different
environmental conditions, such as network load or produce price. When the
load/price is low, so is the cost/regret of pulling a suboptimal arm (e.g.,
trying a suboptimal network configuration). Therefore, intuitively, we could
explore more when the load/price is low and exploit more when the load/price is
high. Inspired by this intuition, we propose an Adaptive Upper-Confidence-Bound
(AdaUCB) algorithm to adaptively balance the exploration-exploitation tradeoff
for opportunistic bandits. We prove that AdaUCB achieves $O(\log T)$ regret
with a smaller coefficient than the traditional UCB algorithm. Furthermore,
AdaUCB achieves $O(1)$ regret with respect to $T$ if the exploration cost is
zero when the load level is below a certain threshold. Last, based on both
synthetic data and real-world traces, experimental results show that AdaUCB
significantly outperforms other bandit algorithms, such as UCB and TS (Thompson
Sampling), under large load/price fluctuations.
",Adaptive Exploration-Exploitation in Opportunistic Bandits,adaptive exploration-exploitation in opportunistic bandits
"  We propose a simple extension to the ReLU-family of activation functions that
allows them to shift the mean activation across a layer towards zero. Combined
with proper weight initialization, this alleviates the need for normalization
layers. We explore the training of deep vanilla recurrent neural networks
(RNNs) with up to 144 layers, and show that bipolar activation functions help
learning in this setting. On the Penn Treebank and Text8 language modeling
tasks we obtain competitive results, improving on the best reported results for
non-gated networks. In experiments with convolutional neural networks without
batch normalization, we find that bipolar activations produce a faster drop in
training error, and results in a lower test error on the CIFAR-10
classification task.
",Neural Network Activation Functions,activation functions
"  Recurrent neural networks (RNNs) are widely used to model sequential data but
their non-linear dependencies between sequence elements prevent parallelizing
training over sequence length. We show the training of RNNs with only linear
sequential dependencies can be parallelized over the sequence length using the
parallel scan algorithm, leading to rapid training on long sequences even with
small minibatch size. We develop a parallel linear recurrence CUDA kernel and
show that it can be applied to immediately speed up training and inference of
several state of the art RNN architectures by up to 9x. We abstract recent work
on linear RNNs into a new framework of linear surrogate RNNs and develop a
linear surrogate model for the long short-term memory unit, the GILR-LSTM, that
utilizes parallel linear recurrence. We extend sequence learning to new
extremely long sequence regimes that were previously out of reach by
successfully training a GILR-LSTM on a synthetic sequence classification task
with a one million timestep dependency.
",Parallelizing Recurrent Neural Networks,deep learning
"  Knowledge graph (KG) is known to be helpful for the task of question
answering (QA), since it provides well-structured relational information
between entities, and allows one to further infer indirect facts. However, it
is challenging to build QA systems which can learn to reason over knowledge
graphs based on question-answer pairs alone. First, when people ask questions,
their expressions are noisy (for example, typos in texts, or variations in
pronunciations), which is non-trivial for the QA system to match those
mentioned entities to the knowledge graph. Second, many questions require
multi-hop logic reasoning over the knowledge graph to retrieve the answers. To
address these challenges, we propose a novel and unified deep learning
architecture, and an end-to-end variational learning algorithm which can handle
noise in questions, and learn multi-hop reasoning simultaneously. Our method
achieves state-of-the-art performance on a recent benchmark dataset in the
literature. We also derive a series of new benchmark datasets, including
questions for multi-hop reasoning, questions paraphrased by neural translation
model, and questions in human voice. Our method yields very promising results
on all these challenging datasets.
",Question Answering over Knowledge Graphs,question answering on knowledge graphs
"  We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)
with a constant step-size and the so called Polyak-Ruppert (PR) averaging of
iterates. LSAs are widely applied in machine learning and reinforcement
learning (RL), where the aim is to compute an appropriate $\theta_{*} \in
\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$
updates per iteration. In this paper, we are motivated by the problem (in RL)
of policy evaluation from experience replay using the \emph{temporal
difference} (TD) class of learning algorithms that are also LSAs. For LSAs with
a constant step-size, and PR averaging, we provide bounds for the mean squared
error (MSE) after $t$ iterations. We assume that data is \iid with finite
variance (underlying distribution being $P$) and that the expected dynamics is
Hurwitz. For a given LSA with PR averaging, and data distribution $P$
satisfying the said assumptions, we show that there exists a range of constant
step-sizes such that its MSE decays as $O(\frac{1}{t})$.
  We examine the conditions under which a constant step-size can be chosen
uniformly for a class of data distributions $\mathcal{P}$, and show that not
all data distributions `admit' such a uniform constant step-size. We also
suggest a heuristic step-size tuning algorithm to choose a constant step-size
of a given LSA for a given data distribution $P$. We compare our results with
related work and also discuss the implication of our results in the context of
TD algorithms that are LSAs.
",Stochastic Approximation in Reinforcement Learning,linear stochastic approximation algorithms
"  Deep reinforcement learning (deep RL) has achieved superior performance in
complex sequential tasks by using a deep neural network as its function
approximator and by learning directly from raw images. A drawback of using raw
images is that deep RL must learn the state feature representation from the raw
images in addition to learning a policy. As a result, deep RL can require a
prohibitively large amount of training time and data to reach reasonable
performance, making it difficult to use deep RL in real-world applications,
especially when data is expensive. In this work, we speed up training by
addressing half of what deep RL is trying to solve --- learning features. Our
approach is to learn some of the important features by pre-training deep RL
network's hidden layers via supervised learning using a small set of human
demonstrations. We empirically evaluate our approach using deep Q-network (DQN)
and asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600
games of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training
with human demonstrations in a supervised learning manner is better at
discovering features relative to pre-training naively in DQN, and 2)
initializing a deep RL network with a pre-trained model provides a significant
improvement in training time even when pre-training from a small number of
human demonstrations.
",Accelerating Deep Reinforcement Learning with Supervised Pre-training,deep reinforcement learning with pre-trained features
"  Determining functional brain connectivity is crucial to understanding the
brain and neural differences underlying disorders such as autism. Recent
studies have used Gaussian graphical models to learn brain connectivity via
statistical dependencies across brain regions from neuroimaging. However,
previous studies often fail to properly incorporate priors tailored to
neuroscience, such as preferring shorter connections. To remedy this problem,
the paper here introduces a novel, weighted-$\ell_1$, multi-task graphical
model (W-SIMULE). This model elegantly incorporates a flexible prior, along
with a parallelizable formulation. Additionally, W-SIMULE extends the
often-used Gaussian assumption, leading to considerable performance increases.
Here, applications to fMRI data show that W-SIMULE succeeds in determining
functional connectivity in terms of (1) log-likelihood, (2) finding edges that
differentiate groups, and (3) classifying different groups based on their
connectivity, achieving 58.6\% accuracy on the ABIDE dataset. Having
established W-SIMULE's effectiveness, it links four key areas to autism, all of
which are consistent with the literature. Due to its elegant domain adaptivity,
W-SIMULE can be readily applied to various data types to effectively estimate
connectivity.
",Brain Connectivity Modeling in Autism Research,functional brain connectivity analysis
"  Deep learning algorithms have recently produced state-of-the-art accuracy in
many classification tasks, but this success is typically dependent on access to
many annotated training examples. For domains without such data, an attractive
alternative is to train models with light, or distant supervision. In this
paper, we introduce a deep neural network for the Learning from Label
Proportion (LLP) setting, in which the training data consist of bags of
unlabeled instances with associated label distributions for each bag. We
introduce a new regularization layer, Batch Averager, that can be appended to
the last layer of any deep neural network to convert it from supervised
learning to LLP. This layer can be implemented readily with existing deep
learning packages. To further support domains in which the data consist of two
conditionally independent feature views (e.g. image and text), we propose a
co-training algorithm that iteratively generates pseudo bags and refits the
deep LLP model to improve classification accuracy. We demonstrate our models on
demographic attribute classification (gender and race/ethnicity), which has
many applications in social media analysis, public health, and marketing. We
conduct experiments to predict demographics of Twitter users based on their
tweets and profile image, without requiring any user-level annotations for
training. We find that the deep LLP approach outperforms baselines for both
text and image features separately. Additionally, we find that co-training
algorithm improves image and text classification by 4% and 8% absolute F1,
respectively. Finally, an ensemble of text and image classifiers further
improves the absolute F1 measure by 4% on average.
",Deep Learning for Weakly Supervised Classification,learning from label proportion
"  Linguistic sequence labeling is a general modeling approach that encompasses
a variety of problems, such as part-of-speech tagging and named entity
recognition. Recent advances in neural networks (NNs) make it possible to build
reliable models without handcrafted features. However, in many cases, it is
hard to obtain sufficient annotations to train these models. In this study, we
develop a novel neural framework to extract abundant knowledge hidden in raw
texts to empower the sequence labeling task. Besides word-level knowledge
contained in pre-trained word embeddings, character-aware neural language
models are incorporated to extract character-level knowledge. Transfer learning
techniques are further adopted to mediate different components and guide the
language model towards the key knowledge. Comparing to previous methods, these
task-specific knowledge allows us to adopt a more concise model and conduct
more efficient training. Different from most transfer learning methods, the
proposed framework does not rely on any additional supervision. It extracts
knowledge from self-contained order information of training sequences.
Extensive experiments on benchmark datasets demonstrate the effectiveness of
leveraging character-level knowledge and the efficiency of co-training. For
example, on the CoNLL03 NER task, model training completes in about 6 hours on
a single GPU, reaching F1 score of 91.71$\pm$0.10 without using any extra
annotation.
",Neural Sequence Labeling Models,transfer learning for sequence labeling
"  Recent studies have highlighted the vulnerability of deep neural networks
(DNNs) to adversarial examples - a visually indistinguishable adversarial image
can easily be crafted to cause a well-trained model to misclassify. Existing
methods for crafting adversarial examples are based on $L_2$ and $L_\infty$
distortion metrics. However, despite the fact that $L_1$ distortion accounts
for the total variation and encourages sparsity in the perturbation, little has
been developed for crafting $L_1$-based adversarial examples. In this paper, we
formulate the process of attacking DNNs via adversarial examples as an
elastic-net regularized optimization problem. Our elastic-net attacks to DNNs
(EAD) feature $L_1$-oriented adversarial examples and include the
state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial
examples with small $L_1$ distortion and attains similar attack performance to
the state-of-the-art methods in different attack scenarios. More importantly,
EAD leads to improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in
adversarial machine learning and security implications of DNNs.
",Crafting L1-based Adversarial Examples for Deep Neural Networks,adversarial machine learning
"  On electronic game platforms, different payment transactions have different
levels of risk. Risk is generally higher for digital goods in e-commerce.
However, it differs based on product and its popularity, the offer type
(packaged game, virtual currency to a game or subscription service), storefront
and geography. Existing fraud policies and models make decisions independently
for each transaction based on transaction attributes, payment velocities, user
characteristics, and other relevant information. However, suspicious
transactions may still evade detection and hence we propose a broad learning
approach leveraging a graph based perspective to uncover relationships among
suspicious transactions, i.e., inter-transaction dependency. Our focus is to
detect suspicious transactions by capturing common fraudulent behaviors that
would not be considered suspicious when being considered in isolation. In this
paper, we present HitFraud that leverages heterogeneous information networks
for collective fraud detection by exploring correlated and fast evolving
fraudulent behaviors. First, a heterogeneous information network is designed to
link entities of interest in the transaction database via different semantics.
Then, graph based features are efficiently discovered from the network
exploiting the concept of meta-paths, and decisions on frauds are made
collectively on test instances. Experiments on real-world payment transaction
data from Electronic Arts demonstrate that the prediction performance is
effectively boosted by HitFraud with fast convergence where the computation of
meta-path based features is largely optimized. Notably, recall can be improved
up to 7.93% and F-score 4.62% compared to baselines.
",Fraud Detection in E-Commerce,fraud detection in electronic payment transactions
"  In this paper, we investigate the online non-convex optimization problem
which generalizes the classic {online convex optimization problem by relaxing
the convexity assumption on the cost function.
  For this type of problem, the classic exponential weighting online algorithm
has recently been shown to attain a sub-linear regret of $O(\sqrt{T\log T})$.
  In this paper, we introduce a novel recursive structure to the online
algorithm to define a recursive exponential weighting algorithm that attains a
regret of $O(\sqrt{T})$, matching the well-known regret lower bound.
  To the best of our knowledge, this is the first online algorithm with
provable $O(\sqrt{T})$ regret for the online non-convex optimization problem.
",Online Non-Convex Optimization,non-convex online optimization
"  Latent Dirichlet allocation (LDA) is useful in document analysis, image
processing, and many information systems; however, its generalization
performance has been left unknown because it is a singular learning machine to
which regular statistical theory can not be applied.
  Stochastic matrix factorization (SMF) is a restricted matrix factorization in
which matrix factors are stochastic; the column of the matrix is in a simplex.
SMF is being applied to image recognition and text mining. We can understand
SMF as a statistical model by which a stochastic matrix of given data is
represented by a product of two stochastic matrices, whose generalization
performance has also been left unknown because of non-regularity.
  In this paper, by using an algebraic and geometric method, we show the
analytic equivalence of LDA and SMF, both of which have the same real log
canonical threshold (RLCT), resulting in that they asymptotically have the same
Bayesian generalization error and the same log marginal likelihood. Moreover,
we derive the upper bound of the RLCT and prove that it is smaller than the
dimension of the parameter divided by two, hence the Bayesian generalization
errors of them are smaller than those of regular statistical models.
",Equivalence of Latent Dirichlet Allocation and Stochastic Matrix Factorization,non-parametric matrix factorization and latent dirichlet allocation
"  In this paper, we introduce the Action Schema Network (ASNet): a neural
network architecture for learning generalised policies for probabilistic
planning problems. By mimicking the relational structure of planning problems,
ASNets are able to adopt a weight-sharing scheme which allows the network to be
applied to any problem from a given planning domain. This allows the cost of
training the network to be amortised over all problems in that domain. Further,
we propose a training method which balances exploration and supervised training
on small problems to produce a policy which remains robust when evaluated on
larger problems. In experiments, we show that ASNet's learning capability
allows it to significantly outperform traditional non-learning planners in
several challenging domains.
",Neural Networks for Planning Problems,artificial intelligence for planning
"  As the use of cloud computing continues to rise, controlling cost becomes
increasingly important. Yet there is evidence that 30\% - 45\% of cloud spend
is wasted. Existing tools for cloud provisioning typically rely on highly
trained human experts to specify what to monitor, thresholds for triggering
action, and actions. In this paper we explore the use of reinforcement learning
(RL) to acquire policies to balance performance and spend, allowing humans to
specify what they want as opposed to how to do it, minimizing the need for
cloud expertise. Empirical results with tabular, deep, and dueling double deep
Q-learning with the CloudSim simulator show the utility of RL and the relative
merits of the approaches. We also demonstrate effective policy transfer
learning from an extremely simple simulator to CloudSim, with the next step
being transfer from CloudSim to an Amazon Web Services physical environment.
",Cloud Cost Optimization,cloud computing cost optimization
"  In this thesis, the uses of Artificial Immune Systems (AIS) in Machine
learning is studded. the thesis focus on some of immune inspired algorithms
such as clonal selection algorithm and artificial immune network. The effect of
changing the algorithm parameter on its performance is studded. Then a new
immune inspired algorithm for unsupervised classification is proposed. The new
algorithm is based on clonal selection principle and named Unsupervised Clonal
Selection Classification (UCSC). The new proposed algorithm is almost parameter
free. The algorithm parameters are data driven and it adjusts itself to make
the classification as fast as possible. The performance of UCSC is evaluated.
The experiments show that the proposed UCSC algorithm has a good performance
and more reliable.
",Artificial Immune Systems in Machine Learning,artificial immune systems for machine learning
"  Weighted finite automata (WFA) can expressively model functions defined over
strings but are inherently linear models. Given the recent successes of
nonlinear models in machine learning, it is natural to wonder whether
ex-tending WFA to the nonlinear setting would be beneficial. In this paper, we
propose a novel model of neural network based nonlinearWFA model (NL-WFA) along
with a learning algorithm. Our learning algorithm is inspired by the spectral
learning algorithm for WFAand relies on a nonlinear decomposition of the
so-called Hankel matrix, by means of an auto-encoder network. The expressive
power of NL-WFA and the proposed learning algorithm are assessed on both
synthetic and real-world data, showing that NL-WFA can lead to smaller model
sizes and infer complex grammatical structures from data.
",Nonlinear Weighted Finite Automata,nonlinear weighted finite automata
"  Generating music medleys is about finding an optimal permutation of a given
set of music clips. Toward this goal, we propose a self-supervised learning
task, called the music puzzle game, to train neural network models to learn the
sequential patterns in music. In essence, such a game requires machines to
correctly sort a few multisecond music fragments. In the training stage, we
learn the model by sampling multiple non-overlapping fragment pairs from the
same songs and seeking to predict whether a given pair is consecutive and is in
the correct chronological order. For testing, we design a number of puzzle
games with different difficulty levels, the most difficult one being music
medley, which requiring sorting fragments from different songs. On the basis of
state-of-the-art Siamese convolutional network, we propose an improved
architecture that learns to embed frame-level similarity scores computed from
the input fragment pairs to a common space, where fragment pairs in the correct
order can be more easily identified. Our result shows that the resulting model,
dubbed as the similarity embedding network (SEN), performs better than
competing models across different games, including music jigsaw puzzle, music
sequencing, and music medley. Example results can be found at our project
website, https://remyhuang.github.io/DJnet.
",Music Fragment Sorting,music generation
"  The nonnegative matrix factorization is a widely used, flexible matrix
decomposition, finding applications in biology, image and signal processing and
information retrieval, among other areas. Here we present a related matrix
factorization. A multi-objective optimization problem finds conical
combinations of templates that approximate a given data matrix. The templates
are chosen so that as far as possible only the initial data set can be
represented this way. However, the templates are not required to be nonnegative
nor convex combinations of the original data.
",Matrix Factorization,nonnegative matrix factorization and multi-objective optimization
"  Recently a lot of progress has been made in rumor modeling and rumor
detection for micro-blogging streams. However, existing automated methods do
not perform very well for early rumor detection, which is crucial in many
settings, e.g., in crisis situations. One reason for this is that aggregated
rumor features such as propagation features, which work well on the long run,
are - due to their accumulating characteristic - not very helpful in the early
phase of a rumor. In this work, we present an approach for early rumor
detection, which leverages Convolutional Neural Networks for learning the
hidden representations of individual rumor-related tweets to gain insights on
the credibility of each tweets. We then aggregate the predictions from the very
beginning of a rumor to obtain the overall event credits (so-called wisdom),
and finally combine it with a time series based rumor classification model. Our
extensive experiments show a clearly improved classification performance within
the critical very first hours of a rumor. For a better understanding, we also
conduct an extensive feature evaluation that emphasized on the early stage and
shows that the low-level credibility has best predictability at all phases of
the rumor lifetime.
",Early Rumor Detection in Social Media,early rumor detection in micro-blogging streams
"  This paper presents a learning-based approach for impromptu trajectory
tracking for non-minimum phase systems, i.e., systems with unstable inverse
dynamics. Inversion-based feedforward approaches are commonly used for
improving tracking performance; however, these approaches are not directly
applicable to non-minimum phase systems due to their inherent instability. In
order to resolve the instability issue, existing methods have assumed that the
system model is known and used pre-actuation or inverse approximation
techniques. In this work, we propose an approach for learning a stable,
approximate inverse of a non-minimum phase baseline system directly from its
input-output data. Through theoretical discussions, simulations, and
experiments on two different platforms, we show the stability of our proposed
approach and its effectiveness for high-accuracy, impromptu tracking. Our
approach also shows that including more information in the training, as is
commonly assumed to be useful, does not lead to better performance but may
trigger instability and impact the effectiveness of the overall approach.
",Impromptu Trajectory Tracking in Non-Minimum Phase Systems,control systems
"  Deep Neural Networks (DNNs) have been shown to be vulnerable against
adversarial examples, which are data points cleverly constructed to fool the
classifier. Such attacks can be devastating in practice, especially as DNNs are
being applied to ever increasing critical tasks like image recognition in
autonomous driving. In this paper, we introduce a new perspective on the
problem. We do so by first defining robustness of a classifier to adversarial
exploitation. Next, we show that the problem of adversarial example generation
can be posed as learning problem. We also categorize attacks in literature into
high and low perturbation attacks; well-known attacks like fast-gradient sign
method (FGSM) and our attack produce higher perturbation adversarial examples
while the more potent but computationally inefficient Carlini-Wagner (CW)
attack is low perturbation. Next, we show that the dual approach of the attack
learning problem can be used as a defensive technique that is effective against
high perturbation attacks. Finally, we show that a classifier masking method
achieved by adding noise to the a neural network's logit output protects
against low distortion attacks such as the CW attack. We also show that both
our learning and masking defense can work simultaneously to protect against
multiple attacks. We demonstrate the efficacy of our techniques by
experimenting with the MNIST and CIFAR-10 datasets.
",Adversarial Attack Defense in Deep Neural Networks,adversarial attacks and defenses in deep neural networks
"  We conduct an empirical study on discovering the ordered collective dynamics
obtained by a population of intelligence agents, driven by million-agent
reinforcement learning. Our intention is to put intelligent agents into a
simulated natural context and verify if the principles developed in the real
world could also be used in understanding an artificially-created intelligent
population. To achieve this, we simulate a large-scale predator-prey world,
where the laws of the world are designed by only the findings or logical
equivalence that have been discovered in nature. We endow the agents with the
intelligence based on deep reinforcement learning (DRL). In order to scale the
population size up to millions agents, a large-scale DRL training platform with
redesigned experience buffer is proposed. Our results show that the population
dynamics of AI agents, driven only by each agent's individual self-interest,
reveals an ordered pattern that is similar to the Lotka-Volterra model studied
in population biology. We further discover the emergent behaviors of collective
adaptations in studying how the agents' grouping behaviors will change with the
environmental resources. Both of the two findings could be explained by the
self-organization theory in nature.
",Collective Dynamics in AI Agents,artificial intelligence and collective behavior
"  Generative models are used in a wide range of applications building on large
amounts of contextually rich information. Due to possible privacy violations of
the individuals whose data is used to train these models, however, publishing
or sharing generative models is not always viable. In this paper, we present a
novel technique for privately releasing generative models and entire
high-dimensional datasets produced by these models. We model the generator
distribution of the training data with a mixture of $k$ generative neural
networks. These are trained together and collectively learn the generator
distribution of a dataset. Data is divided into $k$ clusters, using a novel
differentially private kernel $k$-means, then each cluster is given to separate
generative neural networks, such as Restricted Boltzmann Machines or
Variational Autoencoders, which are trained only on their own cluster using
differentially private gradient descent. We evaluate our approach using the
MNIST dataset, as well as call detail records and transit datasets, showing
that it produces realistic synthetic samples, which can also be used to
accurately compute arbitrary number of counting queries.
",Differential Privacy in Generative Models,privately releasing generative models and high-dimensional datasets
"  Adaptive optimization algorithms, such as Adam and RMSprop, have shown better
optimization performance than stochastic gradient descent (SGD) in some
scenarios. However, recent studies show that they often lead to worse
generalization performance than SGD, especially for training deep neural
networks (DNNs). In this work, we identify the reasons that Adam generalizes
worse than SGD, and develop a variant of Adam to eliminate the generalization
gap. The proposed method, normalized direction-preserving Adam (ND-Adam),
enables more precise control of the direction and step size for updating weight
vectors, leading to significantly improved generalization performance.
Following a similar rationale, we further improve the generalization
performance in classification tasks by regularizing the softmax logits. By
bridging the gap between SGD and Adam, we also hope to shed light on why
certain optimization algorithms generalize better than others.
",Adaptive Optimization Algorithms for Deep Neural Networks,optimization algorithm performance
"  In the anomaly detection setting, the native feature embedding can be a
crucial source of bias. We present a technique, Feature Omission using Context
in Unsupervised Settings (FOCUS) to learn a feature mapping that is invariant
to changes exemplified in training sets while retaining as much descriptive
power as possible. While this method could apply to many unsupervised settings,
we focus on applications in anomaly detection, where little task-labeled data
is available. Our algorithm requires only non-anomalous sets of data, and does
not require that the contexts in the training sets match the context of the
test set. By maximizing within-set variance and minimizing between-set
variance, we are able to identify and remove distracting features while
retaining fidelity to the descriptiveness needed at test time. In the linear
case, our formulation reduces to a generalized eigenvalue problem that can be
solved quickly and applied to test sets outside the context of the training
sets. This technique allows us to align technical definitions of anomaly
detection with human definitions through appropriate mappings of the feature
space. We demonstrate that this method is able to remove uninformative parts of
the feature space for the anomaly detection setting.
",Feature Invariance in Anomaly Detection,anomaly detection
"  We address the relative paucity of empirical testing of learning algorithms
(of any type) by introducing a new public-domain, Modular, Optimal Learning
Testing Environment (MOLTE) for Bayesian ranking and selection problem,
stochastic bandits or sequential experimental design problems. The Matlab-based
simulator allows the comparison of a number of learning policies (represented
as a series of .m modules) in the context of a wide range of problems (each
represented in its own .m module) which makes it easy to add new algorithms and
new test problems. State-of-the-art policies and various problem classes are
provided in the package. The choice of problems and policies is guided through
a spreadsheet-based interface. Different graphical metrics are included. MOLTE
is designed to be compatible with parallel computing to scale up from local
desktop to clusters and clouds. We offer MOLTE as an easy-to-use tool for the
research community that will make it possible to perform much more
comprehensive testing, spanning a broader selection of algorithms and test
problems. We demonstrate the capabilities of MOLTE through a series of
comparisons of policies on a starter library of test problems. We also address
the problem of tuning and constructing priors that have been largely overlooked
in optimal learning literature. We envision MOLTE as a modest spur to provide
researchers an easy environment to study interesting questions involved in
optimal learning.
",Modular Optimal Learning Testing Environment (MOLTE),machine learning testing environments
"  The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts.
",Template-Free Reaction Outcome Prediction,molecular reaction prediction
"  We consider the problem of learning an unknown Markov Decision Process (MDP)
that is weakly communicating in the infinite horizon setting. We propose a
Thompson Sampling-based reinforcement learning algorithm with dynamic episodes
(TSDE). At the beginning of each episode, the algorithm generates a sample from
the posterior distribution over the unknown model parameters. It then follows
the optimal stationary policy for the sampled model for the rest of the
episode. The duration of each episode is dynamically determined by two stopping
criteria. The first stopping criterion controls the growth rate of episode
length. The second stopping criterion happens when the number of visits to any
state-action pair is doubled. We establish $\tilde O(HS\sqrt{AT})$ bounds on
expected regret under a Bayesian setting, where $S$ and $A$ are the sizes of
the state and action spaces, $T$ is time, and $H$ is the bound of the span.
This regret bound matches the best available bound for weakly communicating
MDPs. Numerical results show it to perform better than existing algorithms for
infinite horizon MDPs.
",Thompson Sampling for MDPs,markov decision process reinforcement learning
"  Random walks are at the heart of many existing deep learning algorithms for
graph data. However, such algorithms have many limitations that arise from the
use of random walks, e.g., the features resulting from these methods are unable
to transfer to new nodes and graphs as they are tied to node identity. In this
work, we introduce the notion of attributed random walks which serves as a
basis for generalizing existing methods such as DeepWalk, node2vec, and many
others that leverage random walks. Our proposed framework enables these methods
to be more widely applicable for both transductive and inductive learning as
well as for use on graphs with attributes (if available). This is achieved by
learning functions that generalize to new nodes and graphs. We show that our
proposed framework is effective with an average AUC improvement of 16.1% while
requiring on average 853 times less space than existing methods on a variety of
graphs from several domains.
",Random Walks in Graph Learning,generalizing graph-based deep learning algorithms with attributed random walks
"  In this paper, we revisit the portfolio optimization problems of the
minimization/maximization of investment risk under constraints of budget and
investment concentration (primal problem) and the maximization/minimization of
investment concentration under constraints of budget and investment risk (dual
problem) for the case that the variances of the return rates of the assets are
identical. We analyze both optimization problems by using the Lagrange
multiplier method and the random matrix approach. Thereafter, we compare the
results obtained from our proposed approach with the results obtained in
previous work. Moreover, we use numerical experiments to validate the results
obtained from the replica approach and the random matrix approach as methods
for analyzing both the primal and dual portfolio optimization problems.
",Portfolio Optimization Under Identical Variances,portfolio optimization
"  Subspace clustering is the unsupervised grouping of points lying near a union
of low-dimensional linear subspaces. Algorithms based directly on geometric
properties of such data tend to either provide poor empirical performance, lack
theoretical guarantees, or depend heavily on their initialization. We present a
novel geometric approach to the subspace clustering problem that leverages
ensembles of the K-subspaces (KSS) algorithm via the evidence accumulation
clustering framework. Our algorithm, referred to as ensemble K-subspaces
(EKSS), forms a co-association matrix whose (i,j)th entry is the number of
times points i and j are clustered together by several runs of KSS with random
initializations. We prove general recovery guarantees for any algorithm that
forms an affinity matrix with entries close to a monotonic transformation of
pairwise absolute inner products. We then show that a specific instance of EKSS
results in an affinity matrix with entries of this form, and hence our proposed
algorithm can provably recover subspaces under similar conditions to
state-of-the-art algorithms. The finding is, to the best of our knowledge, the
first recovery guarantee for evidence accumulation clustering and for KSS
variants. We show on synthetic data that our method performs well in the
traditionally challenging settings of subspaces with large intersection,
subspaces with small principal angles, and noisy data. Finally, we evaluate our
algorithm on six common benchmark datasets and show that unlike existing
methods, EKSS achieves excellent empirical performance when there are both a
small and large number of points per subspace.
",Subspace Clustering Algorithms,subspace clustering
"  Agricultural robots are expected to increase yields in a sustainable way and
automate precision tasks, such as weeding and plant monitoring. At the same
time, they move in a continuously changing, semi-structured field environment,
in which features can hardly be found and reproduced at a later time.
Challenges for Lidar and visual detection systems stem from the fact that
plants can be very small, overlapping and have a steadily changing appearance.
Therefore, a popular way to localize vehicles with high accuracy is based on
ex- pensive global navigation satellite systems and not on natural landmarks.
The contribution of this work is a novel image- based plant localization
technique that uses the time-invariant stem emerging point as a reference. Our
approach is based on a fully convolutional neural network that learns landmark
localization from RGB and NIR image input in an end-to-end manner. The network
performs pose regression to generate a plant location likelihood map. Our
approach allows us to cope with visual variances of plants both for different
species and different growth stages. We achieve high localization accuracies as
shown in detailed evaluations of a sugar beet cultivation phase. In experiments
with our BoniRob we demonstrate that detections can be robustly reproduced with
centimeter accuracy.
",Plant Localization in Agricultural Robotics,plant localization in agricultural robotics
"  Despite the recent developments that allowed neural networks to achieve
impressive performance on a variety of applications, these models are
intrinsically affected by the problem of overgeneralization, due to their
partitioning of the full input space into the fixed set of target classes used
during training. Thus it is possible for novel inputs belonging to categories
unknown during training or even completely unrecognizable to humans to fool the
system into classifying them as one of the known classes, even with a high
degree of confidence. Solving this problem may help improve the security of
such systems in critical applications, and may further lead to applications in
the context of open set recognition and 1-class recognition. This paper
presents a novel way to compute a confidence score using denoising autoencoders
and shows that such confidence score can correctly identify the regions of the
input space close to the training distribution by approximately identifying its
local maxima.
",Overcoming Overgeneralization in Neural Networks,overgeneralization in neural networks
"  In this paper, we consider the interpretability of the foundational
Laplacian-based semi-supervised learning approaches on graphs. We introduce a
novel flow-based learning framework that subsumes the foundational approaches
and additionally provides a detailed, transparent, and easily understood
expression of the learning process in terms of graph flows. As a result, one
can visualize and interactively explore the precise subgraph along which the
information from labeled nodes flows to an unlabeled node of interest.
Surprisingly, the proposed framework avoids trading accuracy for
interpretability, but in fact leads to improved prediction accuracy, which is
supported both by theoretical considerations and empirical results. The
flow-based framework guarantees the maximum principle by construction and can
handle directed graphs in an out-of-the-box manner.
",Graph-based Semi-supervised Learning Interpretability,graph semi-supervised learning
"  We study bilinear embedding models for the task of multi-relational link
prediction and knowledge graph completion. Bilinear models belong to the most
basic models for this task, they are comparably efficient to train and use, and
they can provide good prediction performance. The main goal of this paper is to
explore the expressiveness of and the connections between various bilinear
models proposed in the literature. In particular, a substantial number of
models can be represented as bilinear models with certain additional
constraints enforced on the embeddings. We explore whether or not these
constraints lead to universal models, which can in principle represent every
set of relations, and whether or not there are subsumption relationships
between various models. We report results of an independent experimental study
that evaluates recent bilinear models in a common experimental setup. Finally,
we provide evidence that relation-level ensembles of multiple bilinear models
can achieve state-of-the art prediction performance.
",Bilinear Embedding Models for Knowledge Graph Completion,bilinear embedding models for multi-relational link prediction and knowledge graph completion
"  We revisit the problem of robust principal component analysis with features
acting as prior side information. To this aim, a novel, elegant, non-convex
optimization approach is proposed to decompose a given observation matrix into
a low-rank core and the corresponding sparse residual. Rigorous theoretical
analysis of the proposed algorithm results in exact recovery guarantees with
low computational complexity. Aptly designed synthetic experiments demonstrate
that our method is the first to wholly harness the power of non-convexity over
convexity in terms of both recoverability and speed. That is, the proposed
non-convex approach is more accurate and faster compared to the best available
algorithms for the problem under study. Two real-world applications, namely
image classification and face denoising further exemplify the practical
superiority of the proposed method.
",Robust Principal Component Analysis,robust principal component analysis with prior side information
"  Timely accurate traffic forecast is crucial for urban traffic control and
guidance. Due to the high nonlinearity and complexity of traffic flow,
traditional methods cannot satisfy the requirements of mid-and-long term
prediction tasks and often neglect spatial and temporal dependencies. In this
paper, we propose a novel deep learning framework, Spatio-Temporal Graph
Convolutional Networks (STGCN), to tackle the time series prediction problem in
traffic domain. Instead of applying regular convolutional and recurrent units,
we formulate the problem on graphs and build the model with complete
convolutional structures, which enable much faster training speed with fewer
parameters. Experiments show that our model STGCN effectively captures
comprehensive spatio-temporal correlations through modeling multi-scale traffic
networks and consistently outperforms state-of-the-art baselines on various
real-world traffic datasets.
",Traffic Forecasting,traffic forecasting
"  This paper focuses on developing a strategy for control of systems whose
dynamics are almost entirely unknown. This situation arises naturally in a
scenario where a system undergoes a critical failure. In that case, it is
imperative to retain the ability to satisfy basic control objectives in order
to avert an imminent catastrophe. A prime example of such an objective is the
reach-avoid problem, where a system needs to move to a certain state in a
constrained state space. To deal with limitations on our knowledge of system
dynamics, we develop a theory of myopic control. The primary goal of myopic
control is to, at any given time, optimize the current direction of the system
trajectory, given solely the information obtained about the system until that
time. We propose an algorithm that uses small perturbations in the control
effort to learn local dynamics while simultaneously ensuring that the system
moves in a direction that appears to be nearly optimal, and provide hard bounds
for its suboptimality. We additionally verify the usefulness of the algorithm
on a simulation of a damaged aircraft seeking to avoid a crash, as well as on
an example of a Van der Pol oscillator.
",Myopic Control for Unknown System Dynamics,myopic control for unknown systems
"  The success of convolutional networks in learning problems involving planar
signals such as images is due to their ability to exploit the translation
symmetry of the data distribution through weight sharing. Many areas of science
and egineering deal with signals with other symmetries, such as rotation
invariant data on the sphere. Examples include climate and weather science,
astrophysics, and chemistry. In this paper we present spherical convolutional
networks. These networks use convolutions on the sphere and rotation group,
which results in rotational weight sharing and rotation equivariance. Using a
synthetic spherical MNIST dataset, we show that spherical convolutional
networks are very effective at dealing with rotationally invariant
classification problems.
",Spherical Convolutional Networks,spherical convolutional networks
"  In order for a robot to be a generalist that can perform a wide range of
jobs, it must be able to acquire a wide variety of skills quickly and
efficiently in complex unstructured environments. High-capacity models such as
deep neural networks can enable a robot to represent complex skills, but
learning each skill from scratch then becomes infeasible. In this work, we
present a meta-imitation learning method that enables a robot to learn how to
learn more efficiently, allowing it to acquire new skills from just a single
demonstration. Unlike prior methods for one-shot imitation, our method can
scale to raw pixel inputs and requires data from significantly fewer prior
tasks for effective learning of new skills. Our experiments on both simulated
and real robot platforms demonstrate the ability to learn new tasks,
end-to-end, from a single visual demonstration.
",Meta-Imitation Learning in Robotics,meta-learning for robot skill acquisition
"  Deep Reinforcement Learning has been able to achieve amazing successes in a
variety of domains from video games to continuous control by trying to maximize
the cumulative reward. However, most of these successes rely on algorithms that
require a large amount of data to train in order to obtain results on par with
human-level performance. This is not feasible if we are to deploy these systems
on real world tasks and hence there has been an increased thrust in exploring
data efficient algorithms. To this end, we propose the Shared Learning
framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving
this, we look into some principles of transfer learning which aim to study the
benefits of information exchange across tasks in reinforcement learning and
adapt transfer to learning our value function estimates in a novel manner. In
this paper, we consider the special case of transfer between the value function
estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further
empirically demonstrate how our proposed framework can help in speeding up the
learning process in $Q$-ensembles with minimum computational overhead on a
suite of Atari 2600 Games.
",Data-Efficient Reinforcement Learning,data-efficient reinforcement learning
"  Dynamic pricing of goods in a competitive environment to maximize revenue is
a natural objective and has been a subject of research over the years. In this
paper, we focus on a class of markets exhibiting the substitutes property with
sellers having divisible and replenishable goods. Depending on the prices
chosen, each seller observes a certain demand which is satisfied subject to the
supply constraint. The goal of the seller is to price her good dynamically so
as to maximize her revenue. For the static market case, when the consumer
utility satisfies the Constant Elasticity of Substitution (CES) property, we
give a $O(\sqrt{T})$ regret bound on the maximum loss in revenue of a seller
using a modified version of the celebrated Online Gradient Descent Algorithm by
Zinkevich. For a more specialized set of consumer utilities satisfying the
iso-elasticity condition, we show that when each seller uses a
regret-minimizing algorithm satisfying a certain technical property, the regret
with respect to $(1-\alpha)$ times optimal revenue is bounded as $O(T^{1/4} /
\sqrt{\alpha})$. We extend this result to markets with dynamic supplies and
prove a corresponding dynamic regret bound, whose guarantee deteriorates
smoothly with the inherent instability of the market. As a side-result, we also
extend the previously known convergence results of these algorithms in a
general game to the dynamic setting.
",Dynamic Pricing in Competitive Markets,dynamic pricing in competitive markets
"  The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD)
statistic for measuring the distance between two distributions given
finitely-many multivariate samples. When the distributions are locally
low-dimensional, the proposed test can be made more powerful to distinguish
certain alternatives by incorporating local covariance matrices and
constructing an anisotropic kernel. The kernel matrix is asymmetric; it
computes the affinity between $n$ data points and a set of $n_R$ reference
points, where $n_R$ can be drastically smaller than $n$. While the proposed
statistic can be viewed as a special class of Reproducing Kernel Hilbert Space
MMD, the consistency of the test is proved, under mild assumptions of the
kernel, as long as $\|p-q\| \sqrt{n} \to \infty $, and a finite-sample lower
bound of the testing power is obtained. Applications to flow cytometry and
diffusion MRI datasets are demonstrated, which motivate the proposed approach
to compare distributions.
",Non-parametric Statistical Testing,kernel-based maximum mean discrepancy statistic
"  Model compression is significant for the wide adoption of Recurrent Neural
Networks (RNNs) in both user devices possessing limited resources and business
clusters requiring quick responses to large-scale service requests. This work
aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the
sizes of basic structures within LSTM units, including input updates, gates,
hidden states, cell states and outputs. Independently reducing the sizes of
basic structures can result in inconsistent dimensions among them, and
consequently, end up with invalid LSTM units. To overcome the problem, we
propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS
will simultaneously decrease the sizes of all basic structures by one and
thereby always maintain the dimension consistency. By learning ISS within LSTM
units, the obtained LSTMs remain regular while having much smaller basic
structures. Based on group Lasso regularization, our method achieves 10.59x
speedup without losing any perplexity of a language modeling of Penn TreeBank
dataset. It is also successfully evaluated through a compact model with only
2.69M weights for machine Question Answering of SQuAD dataset. Our approach is
successfully extended to non- LSTM RNNs, like Recurrent Highway Networks
(RHNs). Our source code is publicly available at
https://github.com/wenwei202/iss-rnns
",Model Compression for RNNs,model compression for recurrent neural networks
"  In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning
model is proposed to handle uncontrolled imbalanced real-world image-sentence
dataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165
images and the original text descriptions uploaded by the users are utilized as
the ground truth for training. Descriptions in FlickrNYC dataset vary
dramatically ranging from short term-descriptions to long
paragraph-descriptions and can describe any visual aspects, or even refer to
objects that are not depicted. To deal with the imbalanced and noisy situation
and to fully explore the dataset itself, we propose a novel guiding textual
feature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of
m-LSTM is based on the portion of data in which the image content and the
corresponding descriptions are strongly bonded. Afterwards, during the training
of sg-LSTM on the rest training data, this guiding information serves as
additional input to the network along with the image representations and the
ground-truth descriptions. By integrating these input components into a
multimodal block, we aim to form a training scheme with the textual information
tightly coupled with the image content. The experimental results demonstrate
that the proposed sg-LSTM model outperforms the traditional state-of-the-art
multimodal RNN captioning framework in successfully describing the key
components of the input images.
",Image Captioning with Imbalanced Datasets,computer vision
"  Semi-supervised learning is attracting increasing attention due to the fact
that datasets of many domains lack enough labeled data. Variational
Auto-Encoder (VAE), in particular, has demonstrated the benefits of
semi-supervised learning. The majority of existing semi-supervised VAEs utilize
a classifier to exploit label information, where the parameters of the
classifier are introduced to the VAE. Given the limited labeled data, learning
the parameters for the classifiers may not be an optimal solution for
exploiting label information. Therefore, in this paper, we develop a novel
approach for semi-supervised VAE without classifier. Specifically, we propose a
new model called Semi-supervised Disentangled VAE (SDVAE), which encodes the
input data into disentangled representation and non-interpretable
representation, then the category information is directly utilized to
regularize the disentangled representation via the equality constraint. To
further enhance the feature learning ability of the proposed VAE, we
incorporate reinforcement learning to relieve the lack of data. The dynamic
framework is capable of dealing with both image and text data with its
corresponding encoder and decoder networks. Extensive experiments on image and
text datasets demonstrate the effectiveness of the proposed framework.
",Semi-supervised Learning with Variational Auto-Encoders,semi-supervised learning
"  We present an approach to learning features that represent the local geometry
around a point in an unstructured point cloud. Such features play a central
role in geometric registration, which supports diverse applications in robotics
and 3D vision. Current state-of-the-art local features for unstructured point
clouds have been manually crafted and none combines the desirable properties of
precision, compactness, and robustness. We show that features with these
properties can be learned from data, by optimizing deep networks that map
high-dimensional histograms into low-dimensional Euclidean spaces. The
presented approach yields a family of features, parameterized by dimension,
that are both more compact and more accurate than existing descriptors.
",Point Cloud Feature Learning,computer vision
"  We introduce a novel method to compute a rank $m$ approximation of the
inverse of the Hessian matrix in the distributed regime. By leveraging the
differences in gradients and parameters of multiple Workers, we are able to
efficiently implement a distributed approximation of the Newton-Raphson method.
We also present preliminary results which underline advantages and challenges
of second-order methods for large stochastic optimization problems. In
particular, our work suggests that novel strategies for combining gradients
provide further information on the loss surface.
",Distributed Hessian Approximation,distributed optimization methods
"  We present Shapechanger, a library for transfer reinforcement learning
specifically designed for robotic tasks. We consider three types of knowledge
transfer---from simulation to simulation, from simulation to real, and from
real to real---and a wide range of tasks with continuous states and actions.
Shapechanger is under active development and open-sourced at:
https://github.com/seba-1511/shapechanger/.
",Transfer Learning in Robotics,reinforcement learning for robotics
"  Human action recognition refers to automatic recognizing human actions from a
video clip. In reality, there often exist multiple human actions in a video
stream. Such a video stream is often weakly-annotated with a set of relevant
human action labels at a global level rather than assigning each label to a
specific video episode corresponding to a single action, which leads to a
multi-label learning problem. Furthermore, there are many meaningful human
actions in reality but it would be extremely difficult to collect/annotate
video clips regarding all of various human actions, which leads to a zero-shot
learning scenario. To the best of our knowledge, there is no work that has
addressed all the above issues together in human action recognition. In this
paper, we formulate a real-world human action recognition task as a multi-label
zero-shot learning problem and propose a framework to tackle this problem in a
holistic way. Our framework holistically tackles the issue of unknown temporal
boundaries between different actions for multi-label learning and exploits the
side information regarding the semantic relationship between different human
actions for knowledge transfer. Consequently, our framework leads to a joint
latent ranking embedding for multi-label zero-shot human action recognition. A
novel neural architecture of two component models and an alternate learning
algorithm are proposed to carry out the joint latent ranking embedding
learning. Thus, multi-label zero-shot recognition is done by measuring
relatedness scores of action labels to a test video clip in the joint latent
visual and semantic embedding spaces. We evaluate our framework with different
settings, including a novel data split scheme designed especially for
evaluating multi-label zero-shot learning, on two datasets: Breakfast and
Charades. The experimental results demonstrate the effectiveness of our
framework.
",Multi-Label Zero-Shot Human Action Recognition,multi-label zero-shot human action recognition
"  We study a variation of the classical multi-armed bandits problem. In this
problem, the learner has to make a sequence of decisions, picking from a fixed
set of choices. In each round, she receives as feedback only the loss incurred
from the chosen action. Conventionally, this problem has been studied when
losses of the actions are drawn from an unknown distribution or when they are
adversarial. In this paper, we study this problem when the losses of the
actions also satisfy certain structural properties, and especially, do show a
trend structure. When this is true, we show that using \textit{trend
detection}, we can achieve regret of order $\tilde{O} (N \sqrt{TK})$ with
respect to a switching strategy for the version of the problem where a single
action is chosen in each round and $\tilde{O} (Nm \sqrt{TK})$ when $m$ actions
are chosen each round. This guarantee is a significant improvement over the
conventional benchmark. Our approach can, as a framework, be applied in
combination with various well-known bandit algorithms, like Exp3. For both
versions of the problem, we give regret guarantees also for the
\textit{anytime} setting, i.e. when the length of the choice-sequence is not
known in advance. Finally, we pinpoint the advantages of our method by
comparing it to some well-known other strategies.
",Trend-Aware Multi-Armed Bandits,multi-armed bandits with trend detection
"  Fully convolutional neural networks (FCN) have been shown to achieve
state-of-the-art performance on the task of classifying time series sequences.
We propose the augmentation of fully convolutional networks with long short
term memory recurrent neural network (LSTM RNN) sub-modules for time series
classification. Our proposed models significantly enhance the performance of
fully convolutional networks with a nominal increase in model size and require
minimal preprocessing of the dataset. The proposed Long Short Term Memory Fully
Convolutional Network (LSTM-FCN) achieves state-of-the-art performance compared
to others. We also explore the usage of attention mechanism to improve time
series classification with the Attention Long Short Term Memory Fully
Convolutional Network (ALSTM-FCN). Utilization of the attention mechanism
allows one to visualize the decision process of the LSTM cell. Furthermore, we
propose fine-tuning as a method to enhance the performance of trained models.
An overall analysis of the performance of our model is provided and compared to
other techniques.
",Time Series Classification with Neural Networks,time series classification
"  Information Cascades Model captures dynamical properties of user activity in
a social network. In this work, we develop a novel framework for activity
shaping under the Continuous-Time Information Cascades Model which allows the
administrator for local control actions by allocating targeted resources that
can alter the spread of the process. Our framework employs the optimization of
the spectral radius of the Hazard matrix, a quantity that has been shown to
drive the maximum influence in a network, while enjoying a simple convex
relaxation when used to minimize the influence of the cascade. In addition,
use-cases such as quarantine and node immunization are discussed to highlight
the generality of the proposed activity shaping framework. Finally, we present
the NetShape influence minimization method which is compared favorably to
baseline and state-of-the-art approaches through simulations on real social
networks.
",Social Network Influence Optimization,social network activity shaping
"  Detection of interesting (e.g., coherent or anomalous) clusters has been
studied extensively on plain or univariate networks, with various applications.
Recently, algorithms have been extended to networks with multiple attributes
for each node in the real-world. In a multi-attributed network, often, a
cluster of nodes is only interesting for a subset (subspace) of attributes, and
this type of clusters is called subspace clusters. However, in the current
literature, few methods are capable of detecting subspace clusters, which
involves concurrent feature selection and network cluster detection. These
relevant methods are mostly heuristic-driven and customized for specific
application scenarios.
  In this work, we present a generic and theoretical framework for detection of
interesting subspace clusters in large multi-attributed networks. Specifically,
we propose a subspace graph-structured matching pursuit algorithm, namely,
SG-Pursuit, to address a broad class of such problems for different score
functions (e.g., coherence or anomalous functions) and topology constraints
(e.g., connected subgraphs and dense subgraphs). We prove that our algorithm 1)
runs in nearly-linear time on the network size and the total number of
attributes and 2) enjoys rigorous guarantees (geometrical convergence rate and
tight error bound) analogous to those of the state-of-the-art algorithms for
sparse feature selection problems and subgraph detection problems. As a case
study, we specialize SG-Pursuit to optimize a number of well-known score
functions for two typical tasks, including detection of coherent dense and
anomalous connected subspace clusters in real-world networks. Empirical
evidence demonstrates that our proposed generic algorithm SG-Pursuit performs
superior over state-of-the-art methods that are designed specifically for these
two tasks.
",Subspace Clustering in Multi-Attributed Networks,subspace cluster detection in multi-attributed networks
"  Learning to detect fraud in large-scale accounting data is one of the
long-standing challenges in financial statement audits or fraud investigations.
Nowadays, the majority of applied techniques refer to handcrafted rules derived
from known fraud scenarios. While fairly successful, these rules exhibit the
drawback that they often fail to generalize beyond known fraud scenarios and
fraudsters gradually find ways to circumvent them. To overcome this
disadvantage and inspired by the recent success of deep learning we propose the
application of deep autoencoder neural networks to detect anomalous journal
entries. We demonstrate that the trained network's reconstruction error
obtainable for a journal entry and regularized by the entry's individual
attribute probabilities can be interpreted as a highly adaptive anomaly
assessment. Experiments on two real-world datasets of journal entries, show the
effectiveness of the approach resulting in high f1-scores of 32.93 (dataset A)
and 16.95 (dataset B) and less false positive alerts compared to state of the
art baseline methods. Initial feedback received by chartered accountants and
fraud examiners underpinned the quality of the approach in capturing highly
relevant accounting anomalies.
",Fraud Detection in Accounting Data,fraud detection in accounting data
"  We introduce a framework to leverage knowledge acquired from a repository of
(heterogeneous) supervised datasets to new unsupervised datasets. Our
perspective avoids the subjectivity inherent in unsupervised learning by
reducing it to supervised learning, and provides a principled way to evaluate
unsupervised algorithms. We demonstrate the versatility of our framework via
simple agnostic bounds on unsupervised problems. In the context of clustering,
our approach helps choose the number of clusters and the clustering algorithm,
remove the outliers, and provably circumvent the Kleinberg's impossibility
result. Experimental results across hundreds of problems demonstrate improved
performance on unsupervised data with simple algorithms, despite the fact that
our problems come from heterogeneous domains. Additionally, our framework lets
us leverage deep networks to learn common features from many such small
datasets, and perform zero shot learning.
",Transfer Learning for Unsupervised Data,transfer learning and unsupervised learning
"  We study the necessary and sufficient complexity of ReLU neural networks---in
terms of depth and number of weights---which is required for approximating
classifier functions in $L^2$. As a model class, we consider the set
$\mathcal{E}^\beta (\mathbb R^d)$ of possibly discontinuous piecewise $C^\beta$
functions $f : [-1/2, 1/2]^d \to \mathbb R$, where the different smooth regions
of $f$ are separated by $C^\beta$ hypersurfaces. For dimension $d \geq 2$,
regularity $\beta > 0$, and accuracy $\varepsilon > 0$, we construct artificial
neural networks with ReLU activation function that approximate functions from
$\mathcal{E}^\beta(\mathbb R^d)$ up to $L^2$ error of $\varepsilon$. The
constructed networks have a fixed number of layers, depending only on $d$ and
$\beta$, and they have $O(\varepsilon^{-2(d-1)/\beta})$ many nonzero weights,
which we prove to be optimal. In addition to the optimality in terms of the
number of weights, we show that in order to achieve the optimal approximation
rate, one needs ReLU networks of a certain depth. Precisely, for piecewise
$C^\beta(\mathbb R^d)$ functions, this minimal depth is given---up to a
multiplicative constant---by $\beta/d$. Up to a log factor, our constructed
networks match this bound. This partly explains the benefits of depth for ReLU
networks by showing that deep networks are necessary to achieve efficient
approximation of (piecewise) smooth functions. Finally, we analyze
approximation in high-dimensional spaces where the function $f$ to be
approximated can be factorized into a smooth dimension reducing feature map
$\tau$ and classifier function $g$---defined on a low-dimensional feature
space---as $f = g \circ \tau$. We show that in this case the approximation rate
depends only on the dimension of the feature space and not the input dimension.
",ReLU Neural Network Complexity for Function Approximation,approximation of piecewise smooth functions with relu neural networks
"  Understanding the memory capacity of neural networks remains a challenging
problem in implementing artificial intelligence systems. In this paper, we
address the notion of capacity with respect to Hopfield networks and propose a
dynamic approach to monitoring a network's capacity. We define our
understanding of capacity as the maximum number of stored patterns which can be
retrieved when probed by the stored patterns. Prior work in this area has
presented static expressions dependent on neuron count $N$, forcing network
designers to assume worst-case input characteristics for bias and correlation
when setting the capacity of the network. Instead, our model operates
simultaneously with the learning Hopfield network and concludes on a capacity
estimate based on the patterns which were stored. By continuously updating the
crosstalk associated with the stored patterns, our model guards the network
from overwriting its memory traces and exceeding its capacity. We simulate our
model using artificially generated random patterns, which can be set to a
desired bias and correlation, and observe capacity estimates between 93% and
97% accurate. As a result, our model doubles the memory efficiency of Hopfield
networks in comparison to the static and worst-case capacity estimate while
minimizing the risk of lost patterns.
",Hopfield Network Capacity Estimation,hopfield network capacity estimation
"  In this paper, we propose and evaluate the application of unsupervised
machine learning to anomaly detection for a Cyber-Physical System (CPS). We
compare two methods: Deep Neural Networks (DNN) adapted to time series data
generated by a CPS, and one-class Support Vector Machines (SVM). These methods
are evaluated against data from the Secure Water Treatment (SWaT) testbed, a
scaled-down but fully operational raw water purification plant. For both
methods, we first train detectors using a log generated by SWaT operating under
normal conditions. Then, we evaluate the performance of both methods using a
log generated by SWaT operating under 36 different attack scenarios. We find
that our DNN generates fewer false positives than our one-class SVM while our
SVM detects slightly more anomalies. Overall, our DNN has a slightly better F
measure than our SVM. We discuss the characteristics of the DNN and one-class
SVM used in this experiment, and compare the advantages and disadvantages of
the two methods.
",Anomaly Detection in Cyber-Physical Systems,anomaly detection in cyber-physical systems
"  Reducing the interference noise in a monaural noisy speech signal has been a
challenging task for many years. Compared to traditional unsupervised speech
enhancement methods, e.g., Wiener filtering, supervised approaches, such as
algorithms based on hidden Markov models (HMM), lead to higher-quality enhanced
speech signals. However, the main practical difficulty of these approaches is
that for each noise type a model is required to be trained a priori. In this
paper, we investigate a new class of supervised speech denoising algorithms
using nonnegative matrix factorization (NMF). We propose a novel speech
enhancement method that is based on a Bayesian formulation of NMF (BNMF). To
circumvent the mismatch problem between the training and testing stages, we
propose two solutions. First, we use an HMM in combination with BNMF (BNMF-HMM)
to derive a minimum mean square error (MMSE) estimator for the speech signal
with no information about the underlying noise type. Second, we suggest a
scheme to learn the required noise BNMF model online, which is then used to
develop an unsupervised speech enhancement system. Extensive experiments are
carried out to investigate the performance of the proposed methods under
different conditions. Moreover, we compare the performance of the developed
algorithms with state-of-the-art speech enhancement schemes using various
objective measures. Our simulations show that the proposed BNMF-based methods
outperform the competing algorithms substantially.
",Speech Enhancement,speech enhancement
"  In this paper, the problem of road friction prediction from a fleet of
connected vehicles is investigated. A framework is proposed to predict the road
friction level using both historical friction data from the connected cars and
data from weather stations, and comparative results from different methods are
presented. The problem is formulated as a classification task where the
available data is used to train three machine learning models including
logistic regression, support vector machine, and neural networks to predict the
friction class (slippery or non-slippery) in the future for specific road
segments. In addition to the friction values, which are measured by moving
vehicles, additional parameters such as humidity, temperature, and rainfall are
used to obtain a set of descriptive feature vectors as input to the
classification methods. The proposed prediction models are evaluated for
different prediction horizons (0 to 120 minutes in the future) where the
evaluation shows that the neural networks method leads to more stable results
in different conditions.
",Road Friction Prediction Using Vehicle Sensor Data,road friction prediction from connected vehicles
"  We consider the exploration/exploitation problem in reinforcement learning.
For exploitation, it is well known that the Bellman equation connects the value
at any time-step to the expected value at subsequent time-steps. In this paper
we consider a similar \textit{uncertainty} Bellman equation (UBE), which
connects the uncertainty at any time-step to the expected uncertainties at
subsequent time-steps, thereby extending the potential exploratory benefit of a
policy beyond individual time-steps. We prove that the unique fixed point of
the UBE yields an upper bound on the variance of the posterior distribution of
the Q-values induced by any policy. This bound can be much tighter than
traditional count-based bonuses that compound standard deviation rather than
variance. Importantly, and unlike several existing approaches to optimism, this
method scales naturally to large systems with complex generalization.
Substituting our UBE-exploration strategy for $\epsilon$-greedy improves DQN
performance on 51 out of 57 games in the Atari suite.
",Reinforcement Learning Exploration Strategy,reinforcement learning
"  Lifelong machine learning methods acquire knowledge over a series of
consecutive tasks, continually building upon their experience. Current lifelong
learning algorithms rely upon a single learning agent that has centralized
access to all data. In this paper, we extend the idea of lifelong learning from
a single agent to a network of multiple agents that collectively learn a series
of tasks. Each agent faces some (potentially unique) set of tasks; the key idea
is that knowledge learned from these tasks may benefit other agents trying to
learn different (but related) tasks. Our Collective Lifelong Learning Algorithm
(CoLLA) provides an efficient way for a network of agents to share their
learned knowledge in a distributed and decentralized manner, while preserving
the privacy of the locally observed data. Note that a decentralized scheme is a
subclass of distributed algorithms where a central server does not exist and in
addition to data, computations are also distributed among the agents. We
provide theoretical guarantees for robust performance of the algorithm and
empirically demonstrate that CoLLA outperforms existing approaches for
distributed multi-task learning on a variety of data sets.
",Decentralized Lifelong Learning,multi-agent lifelong learning
"  We present a technique for efficiently synthesizing images of atmospheric
clouds using a combination of Monte Carlo integration and neural networks. The
intricacies of Lorenz-Mie scattering and the high albedo of cloud-forming
aerosols make rendering of clouds---e.g. the characteristic silverlining and
the ""whiteness"" of the inner body---challenging for methods based solely on
Monte Carlo integration or diffusion theory. We approach the problem
differently. Instead of simulating all light transport during rendering, we
pre-learn the spatial and directional distribution of radiant flux from tens of
cloud exemplars. To render a new scene, we sample visible points of the cloud
and, for each, extract a hierarchical 3D descriptor of the cloud geometry with
respect to the shading location and the light source. The descriptor is input
to a deep neural network that predicts the radiance function for each shading
configuration. We make the key observation that progressively feeding the
hierarchical descriptor into the network enhances the network's ability to
learn faster and predict with high accuracy while using few coefficients. We
also employ a block design with residual connections to further improve
performance. A GPU implementation of our method synthesizes images of clouds
that are nearly indistinguishable from the reference solution within seconds
interactively. Our method thus represents a viable solution for applications
such as cloud design and, thanks to its temporal stability, also for
high-quality production of animated content.
",Cloud Image Synthesis,cloud rendering using monte carlo integration and neural networks
"  There is a critical need to develop new educational technology applications
that analyze the data collected by universities to ensure that students
graduate in a timely fashion (4 to 6 years); and they are well prepared for
jobs in their respective fields of study. In this paper, we present a novel
approach for analyzing historical educational records from a large, public
university to perform next-term grade prediction; i.e., to estimate the grades
that a student will get in a course that he/she will enroll in the next term.
Accurate next-term grade prediction holds the promise for better student degree
planning, personalized advising and automated interventions to ensure that
students stay on track in their chosen degree program and graduate on time. We
present a factorization-based approach called Matrix Factorization with
Temporal Course-wise Influence that incorporates course-wise influence effects
and temporal effects for grade prediction. In this model, students and courses
are represented in a latent ""knowledge"" space. The grade of a student on a
course is modeled as the similarity of their latent representation in the
""knowledge"" space. Course-wise influence is considered as an additional factor
in the grade prediction. Our experimental results show that the proposed method
outperforms several baseline approaches and infer meaningful patterns between
pairs of courses within academic programs.
",Predictive Modeling in Education,predictive analytics in education
"  A defining feature of sampling-based motion planning is the reliance on an
implicit representation of the state space, which is enabled by a set of
probing samples. Traditionally, these samples are drawn either
probabilistically or deterministically to uniformly cover the state space. Yet,
the motion of many robotic systems is often restricted to ""small"" regions of
the state space, due to, for example, differential constraints or
collision-avoidance constraints. To accelerate the planning process, it is thus
desirable to devise non-uniform sampling strategies that favor sampling in
those regions where an optimal solution might lie. This paper proposes a
methodology for non-uniform sampling, whereby a sampling distribution is
learned from demonstrations, and then used to bias sampling. The sampling
distribution is computed through a conditional variational autoencoder,
allowing sample generation from the latent space conditioned on the specific
planning problem. This methodology is general, can be used in combination with
any sampling-based planner, and can effectively exploit the underlying
structure of a planning problem while maintaining the theoretical guarantees of
sampling-based approaches. Specifically, on several planning problems, the
proposed methodology is shown to effectively learn representations for the
relevant regions of the state space, resulting in an order of magnitude
improvement in terms of success rate and convergence to the optimal cost.
",Non-Uniform Sampling for Motion Planning,non-uniform sampling strategies for sampling-based motion planning
"  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard
unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address
multi-label learning tasks. Previous work has shown it to perform in par with
other state-of-the-art multi-label methods. Nonetheless, with increasing label
sets sizes LLDA encounters scalability issues. In this work, we introduce
Subset LLDA, a simple variant of the standard LLDA algorithm, that not only can
effectively scale up to problems with hundreds of thousands of labels but also
improves over the LLDA state-of-the-art. We conduct extensive experiments on
eight data sets, with label sets sizes ranging from hundreds to hundreds of
thousands, comparing our proposed algorithm with the previously proposed LLDA
algorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme
multi-label classification. The results show a steady advantage of our method
over the other LLDA algorithms and competitive results compared to the extreme
multi-label classification algorithms.
",Scalable Multi-Label Learning,latent dirichlet allocation for multi-label learning
"  Spectral embedding is a procedure which can be used to obtain vector
representations of the nodes of a graph. This paper proposes a generalisation
of the latent position network model known as the random dot product graph, to
allow interpretation of those vector representations as latent position
estimates. The generalisation is needed to model heterophilic connectivity
(e.g., `opposites attract') and to cope with negative eigenvalues more
generally. We show that, whether the adjacency or normalised Laplacian matrix
is used, spectral embedding produces uniformly consistent latent position
estimates with asymptotically Gaussian error (up to identifiability). The
standard and mixed membership stochastic block models are special cases in
which the latent positions take only $K$ distinct vector values, representing
communities, or live in the $(K-1)$-simplex with those vertices, respectively.
Under the stochastic block model, our theory suggests spectral clustering using
a Gaussian mixture model (rather than $K$-means) and, under mixed membership,
fitting the minimum volume enclosing simplex, existing recommendations
previously only supported under non-negative-definite assumptions. Empirical
improvements in link prediction (over the random dot product graph), and the
potential to uncover richer latent structure (than posited under the standard
or mixed membership stochastic block models) are demonstrated in a
cyber-security example.
",Spectral Graph Embedding,graph embeddings
"  In this work, we present a fully automated lung CT cancer diagnosis system,
DeepLung. DeepLung contains two parts, nodule detection and classification.
Considering the 3D nature of lung CT data, two 3D networks are designed for the
nodule detection and classification respectively. Specifically, a 3D Faster
R-CNN is designed for nodule detection with a U-net-like encoder-decoder
structure to effectively learn nodule features. For nodule classification,
gradient boosting machine (GBM) with 3D dual path network (DPN) features is
proposed. The nodule classification subnetwork is validated on a public dataset
from LIDC-IDRI, on which it achieves better performance than state-of-the-art
approaches, and surpasses the average performance of four experienced doctors.
For the DeepLung system, candidate nodules are detected first by the nodule
detection subnetwork, and nodule diagnosis is conducted by the classification
subnetwork. Extensive experimental results demonstrate the DeepLung is
comparable to the experienced doctors both for the nodule-level and
patient-level diagnosis on the LIDC-IDRI dataset.
",Lung Cancer Diagnosis using Deep Learning,computer vision
"  Tree ensembles are flexible predictive models that can capture relevant
variables and to some extent their interactions in a compact and interpretable
manner. Most algorithms for obtaining tree ensembles are based on versions of
boosting or Random Forest. Previous work showed that boosting algorithms
exhibit a cyclic behavior of selecting the same tree again and again due to the
way the loss is optimized. At the same time, Random Forest is not based on loss
optimization and obtains a more complex and less interpretable model. In this
paper we present a novel method for obtaining compact tree ensembles by growing
a large pool of trees in parallel with many independent boosting threads and
then selecting a small subset and updating their leaf weights by loss
optimization. We allow for the trees in the initial pool to have different
depths which further helps with generalization. Experiments on real datasets
show that the obtained model has usually a smaller loss than boosting, which is
also reflected in a lower misclassification error on the test set.
",Tree Ensemble Optimization,tree ensemble learning
"  Multi-task learning (MTL) has recently contributed to learning better
representations in service of various NLP tasks. MTL aims at improving the
performance of a primary task, by jointly training on a secondary task. This
paper introduces automated tasks, which exploit the sequential nature of the
input data, as secondary tasks in an MTL model. We explore next word
prediction, next character prediction, and missing word completion as potential
automated tasks. Our results show that training on a primary task in parallel
with a secondary automated task improves both the convergence speed and
accuracy for the primary task. We suggest two methods for augmenting an
existing network with automated tasks and establish better performance in topic
prediction, sentiment analysis, and hashtag recommendation. Finally, we show
that the MTL models can perform well on datasets that are small and colloquial
by nature.
",Multi-Task Learning in NLP,multi-task learning for natural language processing
"  This paper presents two single channel speech dereverberation methods to
enhance the quality of speech signals that have been recorded in an enclosed
space. For both methods, the room acoustics are modeled using a nonnegative
approximation of the convolutive transfer function (NCTF), and to additionally
exploit the spectral properties of the speech signal, such as the low rank
nature of the speech spectrogram, the speech spectrogram is modeled using
nonnegative matrix factorization (NMF). Two methods are described to combine
the NCTF and NMF models. In the first method, referred to as the integrated
method, a cost function is constructed by directly integrating the speech NMF
model into the NCTF model, while in the second method, referred to as the
weighted method, the NCTF and NMF based cost functions are weighted and summed.
Efficient update rules are derived to solve both optimization problems. In
addition, an extension of the integrated method is presented, which exploits
the temporal dependencies of the speech signal. Several experiments are
performed on reverberant speech signals with and without background noise,
where the integrated method yields a considerably higher speech quality than
the baseline NCTF method and a state of the art spectral enhancement method.
Moreover, the experimental results indicate that the weighted method can even
lead to a better performance in terms of instrumental quality measures, but
that the optimal weighting parameter depends on the room acoustics and the
utilized NMF model. Modeling the temporal dependencies in the integrated method
was found to be useful only for highly reverberant conditions.
",Speech Dereverberation Methods,speech dereverberation
"  Deriving a good model for multitalker babble noise can facilitate different
speech processing algorithms, e.g. noise reduction, to reduce the so-called
cocktail party difficulty. In the available systems, the fact that the babble
waveform is generated as a sum of N different speech waveforms is not exploited
explicitly. In this paper, first we develop a gamma hidden Markov model for
power spectra of the speech signal, and then formulate it as a sparse
nonnegative matrix factorization (NMF). Second, the sparse NMF is extended by
relaxing the sparsity constraint, and a novel model for babble noise (gamma
nonnegative HMM) is proposed in which the babble basis matrix is the same as
the speech basis matrix, and only the activation factors (weights) of the basis
vectors are different for the two signals over time. Finally, a noise reduction
algorithm is proposed using the derived speech and babble models. All of the
stationary model parameters are estimated using the expectation-maximization
(EM) algorithm, whereas the time-varying parameters, i.e. the gain parameters
of speech and babble signals, are estimated using a recursive EM algorithm. The
objective and subjective listening evaluations show that the proposed babble
model and the final noise reduction algorithm significantly outperform the
conventional methods.
",Multitalker Babble Noise Modeling,speech processing
"  Autonomous driving requires operation in different behavioral modes ranging
from lane following and intersection crossing to turning and stopping. However,
most existing deep learning approaches to autonomous driving do not consider
the behavioral mode in the training strategy. This paper describes a technique
for learning multiple distinct behavioral modes in a single deep neural network
through the use of multi-modal multi-task learning. We study the effectiveness
of this approach, denoted MultiNet, using self-driving model cars for driving
in unstructured environments such as sidewalks and unpaved roads. Using labeled
data from over one hundred hours of driving our fleet of 1/10th scale model
cars, we trained different neural networks to predict the steering angle and
driving speed of the vehicle in different behavioral modes. We show that in
each case, MultiNet networks outperform networks trained on individual modes
while using a fraction of the total number of parameters.
",Behavioral Mode Adaptation in Autonomous Driving,multi-modal learning for autonomous driving
"  Deep neural networks (DNNs) have transformed several artificial intelligence
research areas including computer vision, speech recognition, and natural
language processing. However, recent studies demonstrated that DNNs are
vulnerable to adversarial manipulations at testing time. Specifically, suppose
we have a testing example, whose label can be correctly predicted by a DNN
classifier. An attacker can add a small carefully crafted noise to the testing
example such that the DNN classifier predicts an incorrect label, where the
crafted testing example is called adversarial example. Such attacks are called
evasion attacks. Evasion attacks are one of the biggest challenges for
deploying DNNs in safety and security critical applications such as
self-driving cars. In this work, we develop new methods to defend against
evasion attacks. Our key observation is that adversarial examples are close to
the classification boundary. Therefore, we propose region-based classification
to be robust to adversarial examples. For a benign/adversarial testing example,
we ensemble information in a hypercube centered at the example to predict its
label. In contrast, traditional classifiers are point-based classification,
i.e., given a testing example, the classifier predicts its label based on the
testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets
demonstrate that our region-based classification can significantly mitigate
evasion attacks without sacrificing classification accuracy on benign examples.
Specifically, our region-based classification achieves the same classification
accuracy on testing benign examples as point-based classification, but our
region-based classification is significantly more robust than point-based
classification to various evasion attacks.
",Defending Against Adversarial Attacks in Deep Neural Networks,adversarial attacks on deep neural networks
"  Machine learning on graphs is an important and ubiquitous task with
applications ranging from drug design to friendship recommendation in social
networks. The primary challenge in this domain is finding a way to represent,
or encode, graph structure so that it can be easily exploited by machine
learning models. Traditionally, machine learning approaches relied on
user-defined heuristics to extract features encoding structural information
about a graph (e.g., degree statistics or kernel functions). However, recent
years have seen a surge in approaches that automatically learn to encode graph
structure into low-dimensional embeddings, using techniques based on deep
learning and nonlinear dimensionality reduction. Here we provide a conceptual
review of key advancements in this area of representation learning on graphs,
including matrix factorization-based methods, random-walk based algorithms, and
graph neural networks. We review methods to embed individual nodes as well as
approaches to embed entire (sub)graphs. In doing so, we develop a unified
framework to describe these recent approaches, and we highlight a number of
important applications and directions for future work.
",Graph Representation Learning,graph representation learning
"  Multi-view data are increasingly prevalent in practice. It is often relevant
to analyze the relationships between pairs of views by multi-view component
analysis techniques such as Canonical Correlation Analysis (CCA). However, data
may easily exhibit nonlinear relations, which CCA cannot reveal. We aim to
investigate the usefulness of nonlinear multi-view relations to characterize
multi-view data in an explainable manner. To address this challenge, we propose
a method to characterize globally nonlinear multi-view relationships as a
mixture of linear relationships. A clustering method, it identifies partitions
of observations that exhibit the same relationships and learns those
relationships simultaneously. It defines cluster variables by multi-view rather
than spatial relationships, unlike almost all other clustering methods.
Furthermore, we introduce a supervised classification method that builds on our
clustering method by employing multi-view relationships as discriminative
factors. The value of these methods resides in their capability to find useful
structure in the data that single-view or current multi-view methods may
struggle to find. We demonstrate the potential utility of the proposed approach
using an application in clinical informatics to detect and characterize slow
bleeding in patients whose central venous pressure (CVP) is monitored at the
bedside. Presently, CVP is considered an insensitive measure of a subject's
intravascular volume status or its change. However, we reason that features of
CVP during inspiration and expiration should be informative in early
identification of emerging changes of patient status. We empirically show how
the proposed method can help discover and analyze multiple-to-multiple
correlations, which could be nonlinear or vary throughout the population, by
finding explainable structure of operational interest to practitioners.
",Nonlinear Multi-View Relationship Analysis,nonlinear multi-view data analysis
"  Multi-Entity Dependence Learning (MEDL) explores conditional correlations
among multiple entities. The availability of rich contextual information
requires a nimble learning scheme that tightly integrates with deep neural
networks and has the ability to capture correlation structures among
exponentially many outcomes. We propose MEDL_CVAE, which encodes a conditional
multivariate distribution as a generating process. As a result, the variational
lower bound of the joint likelihood can be optimized via a conditional
variational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE was
motivated by two real-world applications in computational sustainability: one
studies the spatial correlation among multiple bird species using the eBird
data and the other models multi-dimensional landscape composition and human
footprint in the Amazon rainforest with satellite images. We show that
MEDL_CVAE captures rich dependency structures, scales better than previous
methods, and further improves on the joint likelihood taking advantage of very
large datasets that are beyond the capacity of previous methods.
",Multi-Entity Dependence Learning,multi-entity dependence learning for complex data modeling
"  Latent factor models are increasingly popular for modeling multi-relational
knowledge graphs. By their vectorial nature, it is not only hard to interpret
why this class of models works so well, but also to understand where they fail
and how they might be improved. We conduct an experimental survey of
state-of-the-art models, not towards a purely comparative end, but as a means
to get insight about their inductive abilities. To assess the strengths and
weaknesses of each model, we create simple tasks that exhibit first, atomic
properties of binary relations, and then, common inter-relational inference
through synthetic genealogies. Based on these experimental results, we propose
new research directions to improve on existing models.
",Evaluating Latent Factor Models for Knowledge Graphs,latent factor models for multi-relational knowledge graphs
"  We propose a new grayscale image denoiser, dubbed as Neural Affine Image
Denoiser (Neural AIDE), which utilizes neural network in a novel way. Unlike
other neural network based image denoising methods, which typically apply
simple supervised learning to learn a mapping from a noisy patch to a clean
patch, we formulate to train a neural network to learn an \emph{affine} mapping
that gets applied to a noisy pixel, based on its context. Our formulation
enables both supervised training of the network from the labeled training
dataset and adaptive fine-tuning of the network parameters using the given
noisy image subject to denoising. The key tool for devising Neural AIDE is to
devise an estimated loss function of the MSE of the affine mapping, solely
based on the noisy data. As a result, our algorithm can outperform most of the
recent state-of-the-art methods in the standard benchmark datasets. Moreover,
our fine-tuning method can nicely overcome one of the drawbacks of the
patch-level supervised learning methods in image denoising; namely, a
supervised trained model with a mismatched noise variance can be mostly
corrected as long as we have the matched noise variance during the fine-tuning
step.
",Image Denoising,computer vision
"  We address the problem of learning a syntactic profile for a collection of
strings, i.e. a set of regex-like patterns that succinctly describe the
syntactic variations in the strings. Real-world datasets, typically curated
from multiple sources, often contain data in various syntactic formats. Thus,
any data processing task is preceded by the critical step of data format
identification. However, manual inspection of data to identify the different
formats is infeasible in standard big-data scenarios.
  Prior techniques are restricted to a small set of pre-defined patterns (e.g.
digits, letters, words, etc.), and provide no control over granularity of
profiles. We define syntactic profiling as a problem of clustering strings
based on syntactic similarity, followed by identifying patterns that succinctly
describe each cluster. We present a technique for synthesizing such profiles
over a given language of patterns, that also allows for interactive refinement
by requesting a desired number of clusters.
  Using a state-of-the-art inductive synthesis framework, PROSE, we have
implemented our technique as FlashProfile. Across $153$ tasks over $75$ large
real datasets, we observe a median profiling time of only $\sim\,0.7\,$s.
Furthermore, we show that access to syntactic profiles may allow for more
accurate synthesis of programs, i.e. using fewer examples, in
programming-by-example (PBE) workflows such as FlashFill.
",Syntactic Profiling for Data Format Identification,machine learning for natural language processing
"  Various approaches have been proposed to learn visuo-motor policies for
real-world robotic applications. One solution is first learning in simulation
then transferring to the real world. In the transfer, most existing approaches
need real-world images with labels. However, the labelling process is often
expensive or even impractical in many robotic applications. In this paper, we
propose an adversarial discriminative sim-to-real transfer approach to reduce
the cost of labelling real data. The effectiveness of the approach is
demonstrated with modular networks in a table-top object reaching task where a
7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter
through visual observations. The adversarial transfer approach reduced the
labelled real data requirement by 50%. Policies can be transferred to real
environments with only 93 labelled and 186 unlabelled real images. The
transferred visuo-motor policies are robust to novel (not seen in training)
objects in clutter and even a moving target, achieving a 97.8% success rate and
1.8 cm control accuracy.
",Sim-to-Real Transfer Learning for Robotics,robotics: sim-to-real transfer learning for visuo-motor policies
"  In this paper, we focus on developing a novel mechanism to preserve
differential privacy in deep neural networks, such that: (1) The privacy budget
consumption is totally independent of the number of training steps; (2) It has
the ability to adaptively inject noise into features based on the contribution
of each to the output; and (3) It could be applied in a variety of different
deep neural networks. To achieve this, we figure out a way to perturb affine
transformations of neurons, and loss functions used in deep neural networks. In
addition, our mechanism intentionally adds ""more noise"" into features which are
""less relevant"" to the model output, and vice-versa. Our theoretical analysis
further derives the sensitivities and error bounds of our mechanism. Rigorous
experiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is
highly effective and outperforms existing solutions.
",Differential Privacy in Deep Learning,differential privacy in deep neural networks
"  The bag-of-words model is a standard representation of text for many linear
classifier learners. In many problem domains, linear classifiers are preferred
over more complex models due to their efficiency, robustness and
interpretability, and the bag-of-words text representation can capture
sufficient information for linear classifiers to make highly accurate
predictions. However in settings where there is a large vocabulary, large
variance in the frequency of terms in the training corpus, many classes and
very short text (e.g., single sentences or document titles) the bag-of-words
representation becomes extremely sparse, and this can reduce the accuracy of
classifiers. A particular issue in such settings is that short texts tend to
contain infrequently occurring or rare terms which lack class-conditional
evidence. In this work we introduce a method for enriching the bag-of-words
model by complementing such rare term information with related terms from both
general and domain-specific Word Vector models. By reducing sparseness in the
bag-of-words models, our enrichment approach achieves improved classification
over several baseline classifiers in a variety of text classification problems.
Our approach is also efficient because it requires no change to the linear
classifier before or during training, since bag-of-words enrichment applies
only to text being classified.
",Text Classification with Word Vector Enrichment,text classification using enriched bag-of-words model
"  As traditional neural network consumes a significant amount of computing
resources during back propagation, \citet{Sun2017mePropSB} propose a simple yet
effective technique to alleviate this problem. In this technique, only a small
subset of the full gradients are computed to update the model parameters. In
this paper we extend this technique into the Convolutional Neural Network(CNN)
to reduce calculation in back propagation, and the surprising results verify
its validity in CNN: only 5\% of the gradients are passed back but the model
still achieves the same effect as the traditional CNN, or even better. We also
show that the top-$k$ selection of gradients leads to a sparse calculation in
back propagation, which may bring significant computational benefits for high
computational complexity of convolution operation in CNN.
",Efficient Backpropagation in Convolutional Neural Networks,efficient back propagation techniques for convolutional neural networks
"  Emergency response applications for nuclear or radiological events can be
significantly improved via deep feature learning due to the hidden complexity
of the data and models involved. In this paper we present a novel methodology
for rapid source estimation during radiological releases based on deep feature
extraction and weather clustering. Atmospheric dispersions are then calculated
based on identified predominant weather patterns and are matched against
simulated incidents indicated by radiation readings on the ground. We evaluate
the accuracy of our methods over multiple years of weather reanalysis data in
the European region. We juxtapose these results with deep classification
convolution networks and discuss advantages and disadvantages.
",Radiological Source Estimation using Deep Learning,nuclear emergency response
"  This study presents a novel end-to-end architecture that learns hierarchical
representations from raw EEG data using fully convolutional deep neural
networks for the task of neonatal seizure detection. The deep neural network
acts as both feature extractor and classifier, allowing for end-to-end
optimization of the seizure detector. The designed system is evaluated on a
large dataset of continuous unedited multi-channel neonatal EEG totaling 835
hours and comprising of 1389 seizures. The proposed deep architecture, with
sample-level filters, achieves an accuracy that is comparable to the
state-of-the-art SVM-based neonatal seizure detector, which operates on a set
of carefully designed hand-crafted features. The fully convolutional
architecture allows for the localization of EEG waveforms and patterns that
result in high seizure probabilities for further clinical examination.
",Neonatal Seizure Detection using EEG,neonatal seizure detection using deep learning
"  This paper reports the analysis of audio and visual features in predicting
the continuous emotion dimensions under the seventh Audio/Visual Emotion
Challenge (AVEC 2017), which was done as part of a B.Tech. 2nd year internship
project. For visual features we used the HOG (Histogram of Gradients) features,
Fisher encodings of SIFT (Scale-Invariant Feature Transform) features based on
Gaussian mixture model (GMM) and some pretrained Convolutional Neural Network
layers as features; all these extracted for each video clip. For audio features
we used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level
descriptors) generated by openXBOW provided by the organisers of the event.
Then we trained fully connected neural network regression model on the dataset
for all these different modalities. We applied multimodal fusion on the output
models to get the Concordance correlation coefficient on Development set as
well as Test set.
",Multimodal Emotion Recognition,emotion recognition using multimodal features
"  Depression is a major mental health disorder that is rapidly affecting lives
worldwide. Depression not only impacts emotional but also physical and
psychological state of the person. Its symptoms include lack of interest in
daily activities, feeling low, anxiety, frustration, loss of weight and even
feeling of self-hatred. This report describes work done by us for Audio Visual
Emotion Challenge (AVEC) 2017 during our second year BTech summer internship.
With the increase in demand to detect depression automatically with the help of
machine learning algorithms, we present our multimodal feature extraction and
decision level fusion approach for the same. Features are extracted by
processing on the provided Distress Analysis Interview Corpus-Wizard of Oz
(DAIC-WOZ) database. Gaussian Mixture Model (GMM) clustering and Fisher vector
approach were applied on the visual data; statistical descriptors on gaze,
pose; low level audio features and head pose and text features were also
extracted. Classification is done on fused as well as independent features
using Support Vector Machine (SVM) and neural networks. The results obtained
were able to cross the provided baseline on validation data set by 17% on audio
features and 24.5% on video features.
",Depression Detection using Multimodal Features,mental health
"  In this paper we introduce ZhuSuan, a python probabilistic programming
library for Bayesian deep learning, which conjoins the complimentary advantages
of Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike
existing deep learning libraries, which are mainly designed for deterministic
neural networks and supervised tasks, ZhuSuan is featured for its deep root
into Bayesian inference, thus supporting various kinds of probabilistic models,
including both the traditional hierarchical Bayesian models and recent deep
generative models. We use running examples to illustrate the probabilistic
programming on ZhuSuan, including Bayesian logistic regression, variational
auto-encoders, deep sigmoid belief networks and Bayesian recurrent neural
networks.
",Bayesian Deep Learning Libraries,deep learning
"  Deep learning has become a promising approach for automated medical
diagnoses. When medical data samples are limited, collaboration among multiple
institutions is necessary to achieve high algorithm performance. However,
sharing patient data often has limitations due to technical, legal, or ethical
concerns. In such cases, sharing a deep learning model is a more attractive
alternative. The best method of performing such a task is unclear, however. In
this study, we simulate the dissemination of learning deep learning network
models across four institutions using various heuristics and compare the
results with a deep learning model trained on centrally hosted patient data.
The heuristics investigated include ensembling single institution models,
single weight transfer, and cyclical weight transfer. We evaluated these
approaches for image classification in three independent image collections
(retinal fundus photos, mammography, and ImageNet). We find that cyclical
weight transfer resulted in a performance (testing accuracy = 77.3%) that was
closest to that of centrally hosted patient data (testing accuracy = 78.7%). We
also found that there is an improvement in the performance of cyclical weight
transfer heuristic with high frequency of weight transfer.
",Distributed Deep Learning for Medical Diagnoses,sharing deep learning models across institutions for medical diagnoses
"  High-dimensional partial differential equations (PDE) appear in a number of
models from the financial industry, such as in derivative pricing models,
credit valuation adjustment (CVA) models, or portfolio optimization models. The
PDEs in such applications are high-dimensional as the dimension corresponds to
the number of financial assets in a portfolio. Moreover, such PDEs are often
fully nonlinear due to the need to incorporate certain nonlinear phenomena in
the model such as default risks, transaction costs, volatility uncertainty
(Knightian uncertainty), or trading constraints in the model. Such
high-dimensional fully nonlinear PDEs are exceedingly difficult to solve as the
computational effort for standard approximation methods grows exponentially
with the dimension. In this work we propose a new method for solving
high-dimensional fully nonlinear second-order PDEs. Our method can in
particular be used to sample from high-dimensional nonlinear expectations. The
method is based on (i) a connection between fully nonlinear second-order PDEs
and second-order backward stochastic differential equations (2BSDEs), (ii) a
merged formulation of the PDE and the 2BSDE problem, (iii) a temporal forward
discretization of the 2BSDE and a spatial approximation via deep neural nets,
and (iv) a stochastic gradient descent-type optimization procedure. Numerical
results obtained using ${\rm T{\small ENSOR}F{\small LOW}}$ in ${\rm P{\small
YTHON}}$ illustrate the efficiency and the accuracy of the method in the cases
of a $100$-dimensional Black-Scholes-Barenblatt equation, a $100$-dimensional
Hamilton-Jacobi-Bellman equation, and a nonlinear expectation of a $ 100
$-dimensional $ G $-Brownian motion.
",Numerical Methods for High-Dimensional PDEs in Finance,high-dimensional partial differential equations in finance
"  We consider the problem of active feature acquisition, where we sequentially
select the subset of features in order to achieve the maximum prediction
performance in the most cost-effective way. In this work, we formulate this
active feature acquisition problem as a reinforcement learning problem, and
provide a novel framework for jointly learning both the RL agent and the
classifier (environment). We also introduce a more systematic way of encoding
subsets of features that can properly handle innate challenge with missing
entries in active feature acquisition problems, that uses the orderless
LSTM-based set encoding mechanism that readily fits in the joint learning
framework. We evaluate our model on a carefully designed synthetic dataset for
the active feature acquisition as well as several real datasets such as
electric health record (EHR) datasets, on which it outperforms all baselines in
terms of prediction performance as well feature acquisition cost.
",Active Feature Acquisition,active feature acquisition
"  We present a novel and scalable label embedding framework for large-scale
multi-label learning a.k.a ExMLDS (Extreme Multi-Label Learning using
Distributional Semantics). Our approach draws inspiration from ideas rooted in
distributional semantics, specifically the Skip Gram Negative Sampling (SGNS)
approach, widely used to learn word embeddings for natural language processing
tasks. Learning such embeddings can be reduced to a certain matrix
factorization. Our approach is novel in that it highlights interesting
connections between label embedding methods used for multi-label learning and
paragraph/document embedding methods commonly used for learning representations
of text data. The framework can also be easily extended to incorporate
auxiliary information such as label-label correlations; this is crucial
especially when there are a lot of missing labels in the training data. We
demonstrate the effectiveness of our approach through an extensive set of
experiments on a variety of benchmark datasets, and show that the proposed
learning methods perform favorably compared to several baselines and
state-of-the-art methods for large-scale multi-label learning. To facilitate
end-to-end learning, we develop a joint learning algorithm that can learn the
embeddings as well as a regression model that predicts these embeddings given
input features, via efficient gradient-based methods.
",Label Embeddings for Multi-Label Learning,machine learning: multi-label classification
"  The Arcade Learning Environment (ALE) is an evaluation platform that poses
the challenge of building AI agents with general competency across dozens of
Atari 2600 games. It supports a variety of different problem settings and it
has been receiving increasing attention from the scientific community, leading
to some high-profile success stories such as the much publicized Deep
Q-Networks (DQN). In this article we take a big picture look at how the ALE is
being used by the research community. We show how diverse the evaluation
methodologies in the ALE have become with time, and highlight some key concerns
when evaluating agents in the ALE. We use this discussion to present some
methodological best practices and provide new benchmark results using these
best practices. To further the progress in the field, we introduce a new
version of the ALE that supports multiple game modes and provides a form of
stochasticity we call sticky actions. We conclude this big picture look by
revisiting challenges posed when the ALE was introduced, summarizing the
state-of-the-art in various problems and highlighting problems that remain
open.
",Evaluating AI Agents in Atari Games,arcade learning environment (ale)
"  We give a polynomial-time algorithm for learning neural networks with one
layer of sigmoids feeding into any Lipschitz, monotone activation function
(e.g., sigmoid or ReLU). We make no assumptions on the structure of the
network, and the algorithm succeeds with respect to {\em any} distribution on
the unit ball in $n$ dimensions (hidden weight vectors also have unit norm).
This is the first assumption-free, provably efficient algorithm for learning
neural networks with two nonlinear layers.
  Our algorithm-- {\em Alphatron}-- is a simple, iterative update rule that
combines isotonic regression with kernel methods. It outputs a hypothesis that
yields efficient oracle access to interpretable features. It also suggests a
new approach to Boolean learning problems via real-valued conditional-mean
functions, sidestepping traditional hardness results from computational
learning theory.
  Along these lines, we subsume and improve many longstanding results for PAC
learning Boolean functions to the more general, real-valued setting of {\em
probabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d.
noise-tolerance.
",Efficient Neural Network Learning Algorithms,neural network learning
"  In this paper, we investigate how to learn to control a group of cooperative
agents with limited sensing capabilities such as robot swarms. The agents have
only very basic sensor capabilities, yet in a group they can accomplish
sophisticated tasks, such as distributed assembly or search and rescue tasks.
Learning a policy for a group of agents is difficult due to distributed partial
observability of the state. Here, we follow a guided approach where a critic
has central access to the global state during learning, which simplifies the
policy evaluation problem from a reinforcement learning point of view. For
example, we can get the positions of all robots of the swarm using a camera
image of a scene. This camera image is only available to the critic and not to
the control policies of the robots. We follow an actor-critic approach, where
the actors base their decisions only on locally sensed information. In
contrast, the critic is learned based on the true global state. Our algorithm
uses deep reinforcement learning to approximate both the Q-function and the
policy. The performance of the algorithm is evaluated on two tasks with simple
simulated 2D agents: 1) finding and maintaining a certain distance to each
others and 2) locating a target.
",Cooperative Robot Control,multi-agent reinforcement learning
"  While bigger and deeper neural network architectures continue to advance the
state-of-the-art for many computer vision tasks, real-world adoption of these
networks is impeded by hardware and speed constraints. Conventional model
compression methods attempt to address this problem by modifying the
architecture manually or using pre-defined heuristics. Since the space of all
reduced architectures is very large, modifying the architecture of a deep
neural network in this way is a difficult task. In this paper, we tackle this
issue by introducing a principled method for learning reduced network
architectures in a data-driven way using reinforcement learning. Our approach
takes a larger `teacher' network as input and outputs a compressed `student'
network derived from the `teacher' network. In the first stage of our method, a
recurrent policy network aggressively removes layers from the large `teacher'
model. In the second stage, another recurrent policy network carefully reduces
the size of each remaining layer. The resulting network is then evaluated to
obtain a reward -- a score based on the accuracy and compression of the
network. Our approach uses this reward signal with policy gradients to train
the policies to find a locally optimal student network. Our experiments show
that we can achieve compression rates of more than 10x for models such as
ResNet-34 while maintaining similar performance to the input `teacher' network.
We also present a valuable transfer learning result which shows that policies
which are pre-trained on smaller `teacher' networks can be used to rapidly
speed up training on larger `teacher' networks.
",Neural Network Compression,deep neural network compression using reinforcement learning
"  Graph classification is a problem with practical applications in many
different domains. Most of the existing methods take the entire graph into
account when calculating graph features. In a graphlet-based approach, for
instance, the entire graph is processed to get the total count of different
graphlets or sub-graphs. In the real-world, however, graphs can be both large
and noisy with discriminative patterns confined to certain regions in the graph
only. In this work, we study the problem of attentional processing for graph
classification. The use of attention allows us to focus on small but
informative parts of the graph, avoiding noise in the rest of the graph. We
present a novel RNN model, called the Graph Attention Model (GAM), that
processes only a portion of the graph by adaptively selecting a sequence of
""interesting"" nodes. The model is equipped with an external memory component
which allows it to integrate information gathered from different parts of the
graph. We demonstrate the effectiveness of the model through various
experiments.
",Graph Attention Mechanism,attention-based graph classification
"  Power management is an expensive and important issue for large computational
infrastructures such as datacenters, large clusters, and computational grids.
However, measuring energy consumption of scalable systems may be impractical
due to both cost and complexity for deploying power metering devices on a large
number of machines. In this paper, we propose the use of information about
resource utilization (e.g. processor, memory, disk operations, and network
traffic) as proxies for estimating power consumption. We employ machine
learning techniques to estimate power consumption using such information which
are provided by common operating systems. Experiments with linear regression,
regression tree, and multilayer perceptron on data from different hardware
resulted into a model with 99.94\% of accuracy and 6.32 watts of error in the
best case.
",Energy Efficiency in Data Centers,power estimation techniques for scalable systems
"  Orthogonal matrix has shown advantages in training Recurrent Neural Networks
(RNNs), but such matrix is limited to be square for the hidden-to-hidden
transformation in RNNs. In this paper, we generalize such square orthogonal
matrix to orthogonal rectangular matrix and formulating this problem in
feed-forward Neural Networks (FNNs) as Optimization over Multiple Dependent
Stiefel Manifolds (OMDSM). We show that the rectangular orthogonal matrix can
stabilize the distribution of network activations and regularize FNNs. We also
propose a novel orthogonal weight normalization method to solve OMDSM.
Particularly, it constructs orthogonal transformation over proxy parameters to
ensure the weight matrix is orthogonal and back-propagates gradient information
through the transformation during training. To guarantee stability, we minimize
the distortions between proxy parameters and canonical weights over all
tractable orthogonal transformations. In addition, we design an orthogonal
linear module (OLM) to learn orthogonal filter banks in practice, which can be
used as an alternative to standard linear module. Extensive experiments
demonstrate that by simply substituting OLM for standard linear module without
revising any experimental protocols, our method largely improves the
performance of the state-of-the-art networks, including Inception and residual
networks on CIFAR and ImageNet datasets. In particular, we have reduced the
test error of wide residual network on CIFAR-100 from 20.04% to 18.61% with
such simple substitution. Our code is available online for result reproduction.
",Orthogonal Matrix in Neural Networks,orthogonal matrix generalization in neural networks
"  In this paper we focus on the linear algebra theory behind feedforward (FNN)
and recurrent (RNN) neural networks. We review backward propagation, including
backward propagation through time (BPTT). Also, we obtain a new exact
expression for Hessian, which represents second order effects. We show that for
$t$ time steps the weight gradient can be expressed as a rank-$t$ matrix, while
the weight Hessian is as a sum of $t^{2}$ Kronecker products of rank-$1$ and
$W^{T}AW$ matrices, for some matrix $A$ and weight matrix $W$. Also, we show
that for a mini-batch of size $r$, the weight update can be expressed as a
rank-$rt$ matrix. Finally, we briefly comment on the eigenvalues of the Hessian
matrix.
",Linear Algebra in Neural Networks,deep learning
"  In this short note we consider a dynamic assortment planning problem under
the capacitated multinomial logit (MNL) bandit model. We prove a tight lower
bound on the accumulated regret that matches existing regret upper bounds for
all parameters (time horizon $T$, number of items $N$ and maximum assortment
capacity $K$) up to logarithmic factors. Our results close an $O(\sqrt{K})$ gap
between upper and lower regret bounds from existing works.
",Dynamic Assortment Planning in Bandit Models,dynamic assortment planning
"  We present a probabilistic framework for nonlinearities, based on doubly
truncated Gaussian distributions. By setting the truncation points
appropriately, we are able to generate various types of nonlinearities within a
unified framework, including sigmoid, tanh and ReLU, the most commonly used
nonlinearities in neural networks. The framework readily integrates into
existing stochastic neural networks (with hidden units characterized as random
variables), allowing one for the first time to learn the nonlinearities
alongside model weights in these networks. Extensive experiments demonstrate
the performance improvements brought about by the proposed framework when
integrated with the restricted Boltzmann machine (RBM), temporal RBM and the
truncated Gaussian graphical model (TGGM).
",Nonlinearities in Neural Networks,machine learning
"  We analyze the convergence of (stochastic) gradient descent algorithm for
learning a convolutional filter with Rectified Linear Unit (ReLU) activation
function. Our analysis does not rely on any specific form of the input
distribution and our proofs only use the definition of ReLU, in contrast with
previous works that are restricted to standard Gaussian input. We show that
(stochastic) gradient descent with random initialization can learn the
convolutional filter in polynomial time and the convergence rate depends on the
smoothness of the input distribution and the closeness of patches. To the best
of our knowledge, this is the first recovery guarantee of gradient-based
algorithms for convolutional filter on non-Gaussian input distributions. Our
theory also justifies the two-stage learning rate strategy in deep neural
networks. While our focus is theoretical, we also present experiments that
illustrate our theoretical findings.
",Convergence of Gradient Descent for Convolutional Filters with ReLU Activation,convolutional neural networks: convergence of stochastic gradient descent
"  We consider the problem of non-parametric Conditional Independence testing
(CI testing) for continuous random variables. Given i.i.d samples from the
joint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we
determine whether $X \perp Y | Z$. We approach this by converting the
conditional independence test into a classification problem. This allows us to
harness very powerful classifiers like gradient-boosted trees and deep neural
networks. These models can handle complex probability distributions and allow
us to perform significantly better compared to the prior state of the art, for
high-dimensional CI testing. The main technical challenge in the classification
problem is the need for samples from the conditional product distribution
$f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X
\perp Y | Z.$ -- when given access only to i.i.d. samples from the true joint
distribution $f(x,y,z)$. To tackle this problem we propose a novel nearest
neighbor bootstrap procedure and theoretically show that our generated samples
are indeed close to $f^{CI}$ in terms of total variational distance. We then
develop theoretical results regarding the generalization bounds for
classification for our problem, which translate into error bounds for CI
testing. We provide a novel analysis of Rademacher type classification bounds
in the presence of non-i.i.d near-independent samples. We empirically validate
the performance of our algorithm on simulated and real datasets and show
performance gains over previous methods.
",Conditional Independence Testing,non-parametric conditional independence testing
"  Massive data exist among user local platforms that usually cannot support
deep neural network (DNN) training due to computation and storage resource
constraints. Cloud-based training schemes provide beneficial services but
suffer from potential privacy risks due to excessive user data collection. To
enable cloud-based DNN training while protecting the data privacy
simultaneously, we propose to leverage the intermediate representations of the
data, which is achieved by splitting the DNNs and deploying them separately
onto local platforms and the cloud. The local neural network (NN) is used to
generate the feature representations. To avoid local training and protect data
privacy, the local NN is derived from pre-trained NNs. The cloud NN is then
trained based on the extracted intermediate representations for the target
learning task. We validate the idea of DNN splitting by characterizing the
dependency of privacy loss and classification accuracy on the local NN topology
for a convolutional NN (CNN) based image classification task. Based on the
characterization, we further propose PrivyNet to determine the local NN
topology, which optimizes the accuracy of the target learning task under the
constraints on privacy loss, local computation, and storage. The efficiency and
effectiveness of PrivyNet are demonstrated with the CIFAR-10 dataset.
",Distributed Deep Neural Network Training with Privacy Protection,privacy-preserving deep neural network training on local platforms
"  Research at the intersection of machine learning, programming languages, and
software engineering has recently taken important steps in proposing learnable
probabilistic models of source code that exploit code's abundance of patterns.
In this article, we survey this work. We contrast programming languages against
natural languages and discuss how these similarities and differences drive the
design of probabilistic models. We present a taxonomy based on the underlying
design principles of each model and use it to navigate the literature. Then, we
review how researchers have adapted these models to application areas and
discuss cross-cutting and application-specific challenges and opportunities.
",Probabilistic Models of Source Code,learnable probabilistic models of source code
"  We analyze bias correction methods using jackknife, bootstrap, and Taylor
series. We focus on the binomial model, and consider the problem of bias
correction for estimating $f(p)$, where $f \in C[0,1]$ is arbitrary. We
characterize the supremum norm of the bias of general jackknife and bootstrap
estimators for any continuous functions, and demonstrate the in delete-$d$
jackknife, different values of $d$ may lead to drastically different behaviors
in jackknife. We show that in the binomial model, iterating the bootstrap bias
correction infinitely many times may lead to divergence of bias and variance,
and demonstrate that the bias properties of the bootstrap bias corrected
estimator after $r-1$ rounds are of the same order as that of the $r$-jackknife
estimator if a bounded coefficients condition is satisfied.
",Bias Correction in Statistical Estimation,bias correction methods
"  In recent years, a number of artificial intelligent services have been
developed such as defect detection system or diagnosis system for customer
services. Unfortunately, the core in these services is a black-box in which
human cannot understand the underlying decision making logic, even though the
inspection of the logic is crucial before launching a commercial service. Our
goal in this paper is to propose an analytic method of a model explanation that
is applicable to general classification models. To this end, we introduce the
concept of a contribution matrix and an explanation embedding in a constraint
space by using a matrix factorization. We extract a rule-like model explanation
from the contribution matrix with the help of the nonnegative matrix
factorization. To validate our method, the experiment results provide with open
datasets as well as an industry dataset of a LTE network diagnosis and the
results show our method extracts reasonable explanations.
",Model Explainability,artificial intelligence model interpretability
"  Estimating mutual information from observed samples is a basic primitive,
useful in several machine learning tasks including correlation mining,
information bottleneck clustering, learning a Chow-Liu tree, and conditional
independence testing in (causal) graphical models. While mutual information is
a well-defined quantity in general probability spaces, existing estimators can
only handle two special cases of purely discrete or purely continuous pairs of
random variables. The main challenge is that these methods first estimate the
(differential) entropies of X, Y and the pair (X;Y) and add them up with
appropriate signs to get an estimate of the mutual information. These
3H-estimators cannot be applied in general mixture spaces, where entropy is not
well-defined. In this paper, we design a novel estimator for mutual information
of discrete-continuous mixtures. We prove that the proposed estimator is
consistent. We provide numerical experiments suggesting superiority of the
proposed estimator compared to other heuristics of adding small continuous
noise to all the samples and applying standard estimators tailored for purely
continuous variables, and quantizing the samples and applying standard
estimators tailored for purely discrete variables. This significantly widens
the applicability of mutual information estimation in real-world applications,
where some variables are discrete, some continuous, and others are a mixture
between continuous and discrete components.
",Mutual Information Estimation in Mixture Spaces,mutual information estimation for discrete-continuous mixture variables
"  In this paper, a sparse Markov decision process (MDP) with novel causal
sparse Tsallis entropy regularization is proposed.The proposed policy
regularization induces a sparse and multi-modal optimal policy distribution of
a sparse MDP. The full mathematical analysis of the proposed sparse MDP is
provided.We first analyze the optimality condition of a sparse MDP. Then, we
propose a sparse value iteration method which solves a sparse MDP and then
prove the convergence and optimality of sparse value iteration using the Banach
fixed point theorem. The proposed sparse MDP is compared to soft MDPs which
utilize causal entropy regularization. We show that the performance error of a
sparse MDP has a constant bound, while the error of a soft MDP increases
logarithmically with respect to the number of actions, where this performance
error is caused by the introduced regularization term. In experiments, we apply
sparse MDPs to reinforcement learning problems. The proposed method outperforms
existing methods in terms of the convergence speed and performance.
",Sparse Markov Decision Processes,reinforcement learning
"  Generating music has a few notable differences from generating images and
videos. First, music is an art of time, necessitating a temporal model. Second,
music is usually composed of multiple instruments/tracks with their own
temporal dynamics, but collectively they unfold over time interdependently.
Lastly, musical notes are often grouped into chords, arpeggios or melodies in
polyphonic music, and thereby introducing a chronological ordering of notes is
not naturally suitable. In this paper, we propose three models for symbolic
multi-track music generation under the framework of generative adversarial
networks (GANs). The three models, which differ in the underlying assumptions
and accordingly the network architectures, are referred to as the jamming
model, the composer model and the hybrid model. We trained the proposed models
on a dataset of over one hundred thousand bars of rock music and applied them
to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings.
A few intra-track and inter-track objective metrics are also proposed to
evaluate the generative results, in addition to a subjective user study. We
show that our models can generate coherent music of four bars right from
scratch (i.e. without human inputs). We also extend our models to human-AI
cooperative music generation: given a specific track composed by human, we can
generate four additional tracks to accompany it. All code, the dataset and the
rendered audio samples are available at https://salu133445.github.io/musegan/ .
",Music Generation Models,music generation with gans
"  We consider the estimation of Dirichlet Process Mixture Models (DPMMs) in
distributed environments, where data are distributed across multiple computing
nodes. A key advantage of Bayesian nonparametric models such as DPMMs is that
they allow new components to be introduced on the fly as needed. This, however,
posts an important challenge to distributed estimation -- how to handle new
components efficiently and consistently. To tackle this problem, we propose a
new estimation method, which allows new components to be created locally in
individual computing nodes. Components corresponding to the same cluster will
be identified and merged via a probabilistic consolidation scheme. In this way,
we can maintain the consistency of estimation with very low communication cost.
Experiments on large real-world data sets show that the proposed method can
achieve high scalability in distributed and asynchronous environments without
compromising the mixing performance.
",Distributed Estimation of Dirichlet Process Mixture Models,distributed dirichlet process mixture model estimation
"  Some real-world problems revolve to solve the optimization problem
\max_{x\in\mathcal{X}}f\left(x\right) where f\left(.\right) is a black-box
function and X might be the set of non-vectorial objects (e.g., distributions)
where we can only define a symmetric and non-negative similarity score on it.
This setting requires a novel view for the standard framework of Bayesian
Optimization that generalizes the core insightful spirit of this framework.
With this spirit, in this paper, we propose Analogical-based Bayesian
Optimization that can maximize black-box function over a domain where only a
similarity score can be defined. Our pathway is as follows: we first base on
the geometric view of Gaussian Processes (GP) to define the concept of
influence level that allows us to analytically represent predictive means and
variances of GP posteriors and base on that view to enable replacing kernel
similarity by a more genetic similarity score. Furthermore, we also propose two
strategies to find a batch of query points that can efficiently handle high
dimensional data.
",Bayesian Optimization,bayesian optimization
"  Recurrent Neural Networks (RNNS) are now widely used on sequence generation
tasks due to their ability to learn long-range dependencies and to generate
sequences of arbitrary length. However, their left-to-right generation
procedure only allows a limited control from a potential user which makes them
unsuitable for interactive and creative usages such as interactive music
generation. This paper introduces a novel architecture called Anticipation-RNN
which possesses the assets of the RNN-based generative models while allowing to
enforce user-defined positional constraints. We demonstrate its efficiency on
the task of generating melodies satisfying positional constraints in the style
of the soprano parts of the J.S. Bach chorale harmonizations. Sampling using
the Anticipation-RNN is of the same order of complexity than sampling from the
traditional RNN model. This fast and interactive generation of musical
sequences opens ways to devise real-time systems that could be used for
creative purposes.
",Interactive Music Generation with RNNs,recurrent neural networks for interactive music generation
"  Despite the ubiquity of mobile and wearable text messaging applications, the
problem of keyboard text decoding is not tackled sufficiently in the light of
the enormous success of the deep learning Recurrent Neural Network (RNN) and
Convolutional Neural Networks (CNN) for natural language understanding. In
particular, considering that the keyboard decoders should operate on devices
with memory and processor resource constraints, makes it challenging to deploy
industrial scale deep neural network (DNN) models. This paper proposes a
sequence-to-sequence neural attention network system for automatic text
correction and completion. Given an erroneous sequence, our model encodes
character level hidden representations and then decodes the revised sequence
thus enabling auto-correction and completion. We achieve this by a combination
of character level CNN and gated recurrent unit (GRU) encoder along with and a
word level gated recurrent unit (GRU) attention decoder. Unlike traditional
language models that learn from billions of words, our corpus size is only 12
million words; an order of magnitude smaller. The memory footprint of our
learnt model for inference and prediction is also an order of magnitude smaller
than the conventional language model based text decoders. We report baseline
performance for neural keyboard decoders in such limited domain. Our models
achieve a word level accuracy of $90\%$ and a character error rate CER of
$2.4\%$ over the Twitter typo dataset. We present a novel dataset of noisy to
corrected mappings by inducing the noise distribution from the Twitter data
over the OpenSubtitles 2009 dataset; on which our model predicts with a word
level accuracy of $98\%$ and sequence accuracy of $68.9\%$. In our user study,
our model achieved an average CER of $2.6\%$ with the state-of-the-art
non-neural touch-screen keyboard decoder at CER of $1.6\%$.
",Mobile Text Decoding,neural keyboard decoding for text correction and completion
"  Owing to its application in solving the difficult and diverse clustering or
outlier detection problem, support-based clustering has recently drawn plenty
of attention. Support-based clustering method always undergoes two phases:
finding the domain of novelty and performing clustering assignment. To find the
domain of novelty, the training time given by the current solvers is typically
over-quadratic in the training size, and hence precluding the usage of
support-based clustering method for large-scale datasets. In this paper, we
propose applying Stochastic Gradient Descent (SGD) framework to the first phase
of support-based clustering for finding the domain of novelty and a new
strategy to perform the clustering assignment. However, the direct application
of SGD to the first phase of support-based clustering is vulnerable to the
curse of kernelization, that is, the model size linearly grows up with the data
size accumulated overtime. To address this issue, we invoke the budget approach
which allows us to restrict the model size to a small budget. Our new strategy
for clustering assignment enables a fast computation by means of reducing the
task of clustering assignment on the full training set to the same task on a
significantly smaller set. We also provide a rigorous theoretical analysis
about the convergence rate for the proposed method. Finally, we validate our
proposed method on the well-known datasets for clustering to show that the
proposed method offers a comparable clustering quality while simultaneously
achieving significant speedup in comparison with the baselines.
",Scalable Support-Based Clustering,clustering and outlier detection
"  We construct genomic predictors for heritable and extremely complex human
quantitative traits (height, heel bone density, and educational attainment)
using modern methods in high dimensional statistics (i.e., machine learning).
Replication tests show that these predictors capture, respectively, $\sim$40,
20, and 9 percent of total variance for the three traits. For example,
predicted heights correlate $\sim$0.65 with actual height; actual heights of
most individuals in validation samples are within a few cm of the prediction.
The variance captured for height is comparable to the estimated SNP
heritability from GCTA (GREML) analysis, and seems to be close to its
asymptotic value (i.e., as sample size goes to infinity), suggesting that we
have captured most of the heritability for the SNPs used. Thus, our results
resolve the common SNP portion of the ""missing heritability"" problem -- i.e.,
the gap between prediction R-squared and SNP heritability. The $\sim$20k
activated SNPs in our height predictor reveal the genetic architecture of human
height, at least for common SNPs. Our primary dataset is the UK Biobank cohort,
comprised of almost 500k individual genotypes with multiple phenotypes. We also
use other datasets and SNPs found in earlier GWAS for out-of-sample validation
of our results.
",Genomic Prediction of Human Traits,genomic predictors of heritable human traits
"  Learning to remember long sequences remains a challenging task for recurrent
neural networks. Register memory and attention mechanisms were both proposed to
resolve the issue with either high computational cost to retain memory
differentiability, or by discounting the RNN representation learning towards
encoding shorter local contexts than encouraging long sequence encoding.
Associative memory, which studies the compression of multiple patterns in a
fixed size memory, were rarely considered in recent years. Although some recent
work tries to introduce associative memory in RNN and mimic the energy decay
process in Hopfield nets, it inherits the shortcoming of rule-based memory
updates, and the memory capacity is limited. This paper proposes a method to
learn the memory update rule jointly with task objective to improve memory
capacity for remembering long sequences. Also, we propose an architecture that
uses multiple such associative memory for more complex input encoding. We
observed some interesting facts when compared to other RNN architectures on
some well-studied sequence learning tasks.
",Improving Memory Capacity in RNNs,recurrent neural network (rnn) architecture for long sequence learning
"  Generative adversarial networks (GANs) are an exciting alternative to
algorithms for solving density estimation problems---using data to assess how
likely samples are to be drawn from the same distribution. Instead of
explicitly computing these probabilities, GANs learn a generator that can match
the given probabilistic source. This paper looks particularly at this matching
capability in the context of problems with one-dimensional outputs. We identify
a class of function decompositions with properties that make them well suited
to the critic role in a leading approach to GANs known as Wasserstein GANs. We
show that Taylor and Fourier series decompositions belong to our class, provide
examples of these critics outperforming standard GAN approaches, and suggest
how they can be scaled to higher dimensional problems in the future.
",Function Decompositions in Wasserstein GANs,generative adversarial networks
"  A Triangle Generative Adversarial Network ($\Delta$-GAN) is developed for
semi-supervised cross-domain joint distribution matching, where the training
data consists of samples from each domain, and supervision of domain
correspondence is provided by only a few paired samples. $\Delta$-GAN consists
of four neural networks, two generators and two discriminators. The generators
are designed to learn the two-way conditional distributions between the two
domains, while the discriminators implicitly define a ternary discriminative
function, which is trained to distinguish real data pairs and two kinds of fake
data pairs. The generators and discriminators are trained together using
adversarial learning. Under mild assumptions, in theory the joint distributions
characterized by the two generators concentrate to the data distribution. In
experiments, three different kinds of domain pairs are considered, image-label,
image-image and image-attribute pairs. Experiments on semi-supervised image
classification, image-to-image translation and attribute-based image generation
demonstrate the superiority of the proposed approach.
",Semi-supervised Cross-Domain Learning,generative adversarial networks for semi-supervised cross-domain joint distribution matching
"  In recent years, significant progress has been made in solving challenging
problems across various domains using deep reinforcement learning (RL).
Reproducing existing work and accurately judging the improvements offered by
novel methods is vital to sustaining this progress. Unfortunately, reproducing
results for state-of-the-art deep RL methods is seldom straightforward. In
particular, non-determinism in standard benchmark environments, combined with
variance intrinsic to the methods, can make reported results tough to
interpret. Without significance metrics and tighter standardization of
experimental reporting, it is difficult to determine whether improvements over
the prior state-of-the-art are meaningful. In this paper, we investigate
challenges posed by reproducibility, proper experimental techniques, and
reporting procedures. We illustrate the variability in reported metrics and
results when comparing against common baselines and suggest guidelines to make
future results in deep RL more reproducible. We aim to spur discussion about
how to ensure continued progress in the field by minimizing wasted effort
stemming from results that are non-reproducible and easily misinterpreted.
",Reproducibility in Deep Reinforcement Learning,deep reinforcement learning reproducibility
"  While machine learning and artificial intelligence have long been applied in
networking research, the bulk of such works has focused on supervised learning.
Recently there has been a rising trend of employing unsupervised machine
learning using unstructured raw network data to improve network performance and
provide services such as traffic engineering, anomaly detection, Internet
traffic classification, and quality of service optimization. The interest in
applying unsupervised learning techniques in networking emerges from their
great success in other fields such as computer vision, natural language
processing, speech recognition, and optimal control (e.g., for developing
autonomous self-driving cars). Unsupervised learning is interesting since it
can unconstrain us from the need of labeled data and manual handcrafted feature
engineering thereby facilitating flexible, general, and automated methods of
machine learning. The focus of this survey paper is to provide an overview of
the applications of unsupervised learning in the domain of networking. We
provide a comprehensive survey highlighting the recent advancements in
unsupervised learning techniques and describe their applications for various
learning tasks in the context of networking. We also provide a discussion on
future directions and open research issues, while also identifying potential
pitfalls. While a few survey papers focusing on the applications of machine
learning in networking have previously been published, a survey of similar
scope and breadth is missing in literature. Through this paper, we advance the
state of knowledge by carefully synthesizing the insights from these survey
papers while also providing contemporary coverage of recent advances.
",Unsupervised Learning in Networking,applications of unsupervised machine learning in networking
"  An analog neural network computing engine based on CMOS-compatible
charge-trap transistor (CTT) is proposed in this paper. CTT devices are used as
analog multipliers. Compared to digital multipliers, CTT-based analog
multiplier shows significant area and power reduction. The proposed computing
engine is composed of a scalable CTT multiplier array and energy efficient
analog-digital interfaces. Through implementing the sequential analog fabric
(SAF), the engine mixed-signal interfaces are simplified and hardware overhead
remains constant regardless of the size of the array. A proof-of-concept 784 by
784 CTT computing engine is implemented using TSMC 28nm CMOS technology and
occupied 0.68mm2. The simulated performance achieves 76.8 TOPS (8-bit) with 500
MHz clock frequency and consumes 14.8 mW. As an example, we utilize this
computing engine to address a classic pattern recognition problem --
classifying handwritten digits on MNIST database and obtained a performance
comparable to state-of-the-art fully connected neural networks using 8-bit
fixed-point resolution.
",Analog Neural Network Computing Engine,analog neural networks
"  We study the generalization error of randomized learning algorithms --
focusing on stochastic gradient descent (SGD) -- using a novel combination of
PAC-Bayes and algorithmic stability. Importantly, our generalization bounds
hold for all posterior distributions on an algorithm's random hyperparameters,
including distributions that depend on the training data. This inspires an
adaptive sampling algorithm for SGD that optimizes the posterior at runtime. We
analyze this algorithm in the context of our generalization bounds and evaluate
it on a benchmark dataset. Our experiments demonstrate that adaptive sampling
can reduce empirical risk faster than uniform sampling while also improving
out-of-sample accuracy.
",Generalization Error in Randomized Learning Algorithms,machine learning
"  Inspired by biological swarms, robotic swarms are envisioned to solve
real-world problems that are difficult for individual agents. Biological swarms
can achieve collective intelligence based on local interactions and simple
rules; however, designing effective distributed policies for large-scale
robotic swarms to achieve a global objective can be challenging. Although it is
often possible to design an optimal centralized strategy for smaller numbers of
agents, those methods can fail as the number of agents increases. Motivated by
the growing success of machine learning, we develop a deep learning approach
that learns distributed coordination policies from centralized policies. In
contrast to traditional distributed control approaches, which are usually based
on human-designed policies for relatively simple tasks, this learning-based
approach can be adapted to more difficult tasks. We demonstrate the efficacy of
our proposed approach on two different tasks, the well-known rendezvous problem
and a more difficult particle assignment problem. For the latter, no known
distributed policy exists. From extensive simulations, it is shown that the
performance of the learned coordination policies is comparable to the
centralized policies, surpassing state-of-the-art distributed policies.
Thereby, our proposed approach provides a promising alternative for real-world
coordination problems that would be otherwise computationally expensive to
solve or intangible to explore.
",Distributed Robotic Swarm Control,distributed control and coordination in robotic swarms
"  Scale of data and scale of computation infrastructures together enable the
current deep learning renaissance. However, training large-scale deep
architectures demands both algorithmic improvement and careful system
configuration. In this paper, we focus on employing the system approach to
speed up large-scale training. Via lessons learned from our routine
benchmarking effort, we first identify bottlenecks and overheads that hinter
data parallelism. We then devise guidelines that help practitioners to
configure an effective system and fine-tune parameters to achieve desired
speedup. Specifically, we develop a procedure for setting minibatch size and
choosing computation algorithms. We also derive lemmas for determining the
quantity of key components such as the number of GPUs and parameter servers.
Experiments and examples show that these guidelines help effectively speed up
large-scale deep learning training.
",Optimizing Deep Learning Training Infrastructure,optimizing deep learning training with system configuration
"  Learning distributed node representations in networks has been attracting
increasing attention recently due to its effectiveness in a variety of
applications. Existing approaches usually study networks with a single type of
proximity between nodes, which defines a single view of a network. However, in
reality there usually exists multiple types of proximities between nodes,
yielding networks with multiple views. This paper studies learning node
representations for networks with multiple views, which aims to infer robust
node representations across different views. We propose a multi-view
representation learning approach, which promotes the collaboration of different
views and lets them vote for the robust representations. During the voting
process, an attention mechanism is introduced, which enables each node to focus
on the most informative views. Experimental results on real-world networks show
that the proposed approach outperforms existing state-of-the-art approaches for
network representation learning with a single view and other competitive
approaches with multiple views.
",Multi-View Network Representation Learning,multi-view network representation learning
"  The partial information decomposition (PID) is perhaps the leading proposal
for resolving information shared between a set of sources and a target into
redundant, synergistic, and unique constituents. Unfortunately, the PID
framework has been hindered by a lack of a generally agreed-upon, multivariate
method of quantifying the constituents. Here, we take a step toward rectifying
this by developing a decomposition based on a new method that quantifies unique
information. We first develop a broadly applicable method---the dependency
decomposition---that delineates how statistical dependencies influence the
structure of a joint distribution. The dependency decomposition then allows us
to define a measure of the information about a target that can be uniquely
attributed to a particular source as the least amount which the source-target
statistical dependency can influence the information shared between the sources
and the target. The result is the first measure that satisfies the core axioms
of the PID framework while not satisfying the Blackwell relation, which depends
on a particular interpretation of how the variables are related. This makes a
key step forward to a practical PID.
",Partial Information Decomposition Framework,partial information decomposition
"  Understanding properties of deep neural networks is an important challenge in
deep learning. In this paper, we take a step in this direction by proposing a
rigorous way of verifying properties of a popular class of neural networks,
Binarized Neural Networks, using the well-developed means of Boolean
satisfiability. Our main contribution is a construction that creates a
representation of a binarized neural network as a Boolean formula. Our encoding
is the first exact Boolean representation of a deep neural network. Using this
encoding, we leverage the power of modern SAT solvers along with a proposed
counterexample-guided search procedure to verify various properties of these
networks. A particular focus will be on the critical property of robustness to
adversarial perturbations. For this property, our experimental results
demonstrate that our approach scales to medium-size deep neural networks used
in image classification tasks. To the best of our knowledge, this is the first
work on verifying properties of deep neural networks using an exact Boolean
encoding of the network.
",Verifying Properties of Binarized Neural Networks,verifying properties of binarized neural networks
"  Prognostics or early detection of incipient faults is an important industrial
challenge for condition-based and preventive maintenance. Physics-based
approaches to modeling fault progression are infeasible due to multiple
interacting components, uncontrolled environmental factors and observability
constraints. Moreover, such approaches to prognostics do not generalize to new
domains. Consequently, domain-agnostic data-driven machine learning approaches
to prognostics are desirable. Damage progression is a path-dependent process
and explicitly modeling the temporal patterns is critical for accurate
estimation of both the current damage state and its progression leading to
total failure. In this paper, we present a novel data-driven approach to
prognostics that employs a novel textual representation of multivariate
temporal sensor observations for predicting the future health state of the
monitored equipment early in its life. This representation enables us to
utilize well-understood concepts from text-mining for modeling, prediction and
understanding distress patterns in a domain agnostic way. The approach has been
deployed and successfully tested on large scale multivariate time-series data
from commercial aircraft engines. We report experiments on well-known publicly
available benchmark datasets and simulation datasets. The proposed approach is
shown to be superior in terms of prediction accuracy, lead time to prediction
and interpretability.
",Data-Driven Prognostics for Industrial Equipment,data-driven prognostics for predicting equipment failure
"  Distributed word embeddings have shown superior performances in numerous
Natural Language Processing (NLP) tasks. However, their performances vary
significantly across different tasks, implying that the word embeddings learnt
by those methods capture complementary aspects of lexical semantics. Therefore,
we believe that it is important to combine the existing word embeddings to
produce more accurate and complete \emph{meta-embeddings} of words. For this
purpose, we propose an unsupervised locally linear meta-embedding learning
method that takes pre-trained word embeddings as the input, and produces more
accurate meta embeddings. Unlike previously proposed meta-embedding learning
methods that learn a global projection over all words in a vocabulary, our
proposed method is sensitive to the differences in local neighbourhoods of the
individual source word embeddings. Moreover, we show that vector concatenation,
a previously proposed highly competitive baseline approach for integrating word
embeddings, can be derived as a special case of the proposed method.
Experimental results on semantic similarity, word analogy, relation
classification, and short-text classification tasks show that our
meta-embeddings to significantly outperform prior methods in several benchmark
datasets, establishing a new state of the art for meta-embeddings.
",Meta-Embedding Learning,meta-embedding learning
"  Representing the semantic relations that exist between two given words (or
entities) is an important first step in a wide-range of NLP applications such
as analogical reasoning, knowledge base completion and relational information
retrieval. A simple, yet surprisingly accurate method for representing a
relation between two words is to compute the vector offset (\PairDiff) between
their corresponding word embeddings. Despite the empirical success, it remains
unclear as to whether \PairDiff is the best operator for obtaining a relational
representation from word embeddings. We conduct a theoretical analysis of
generalised bilinear operators that can be used to measure the $\ell_{2}$
relational distance between two word-pairs. We show that, if the word
embeddings are standardised and uncorrelated, such an operator will be
independent of bilinear terms, and can be simplified to a linear form, where
\PairDiff is a special case. For numerous word embedding types, we empirically
verify the uncorrelation assumption, demonstrating the general applicability of
our theoretical result. Moreover, we experimentally discover \PairDiff from the
bilinear relation composition operator on several benchmark analogy datasets.
",Word Embedding Operators for Relational Representation,bilinear operators for representing semantic relations between word embeddings
"  We propose learning deep models that are monotonic with respect to a
user-specified set of inputs by alternating layers of linear embeddings,
ensembles of lattices, and calibrators (piecewise linear functions), with
appropriate constraints for monotonicity, and jointly training the resulting
network. We implement the layers and projections with new computational graph
nodes in TensorFlow and use the ADAM optimizer and batched stochastic
gradients. Experiments on benchmark and real-world datasets show that six-layer
monotonic deep lattice networks achieve state-of-the art performance for
classification and regression with monotonicity guarantees.
",Monotonic Deep Learning Models,deep learning for monotonic modeling
"  Reinforcement learning has shown promise in learning policies that can solve
complex problems. However, manually specifying a good reward function can be
difficult, especially for intricate tasks. Inverse reinforcement learning
offers a useful paradigm to learn the underlying reward function directly from
expert demonstrations. Yet in reality, the corpus of demonstrations may contain
trajectories arising from a diverse set of underlying reward functions rather
than a single one. Thus, in inverse reinforcement learning, it is useful to
consider such a decomposition. The options framework in reinforcement learning
is specifically designed to decompose policies in a similar light. We therefore
extend the options framework and propose a method to simultaneously recover
reward options in addition to policy options. We leverage adversarial methods
to learn joint reward-policy options using only observed expert states. We show
that this approach works well in both simple and complex continuous control
tasks and shows significant performance increases in one-shot transfer
learning.
",Inverse Reinforcement Learning with Multiple Reward Functions,inverse reinforcement learning
"  The promise of learning to learn for robotics rests on the hope that by
extracting some information about the learning process itself we can speed up
subsequent similar learning tasks. Here, we introduce a computationally
efficient online meta-learning algorithm that builds and optimizes a memory
model of the optimal learning rate landscape from previously observed gradient
behaviors. While performing task specific optimization, this memory of learning
rates predicts how to scale currently observed gradients. After applying the
gradient scaling our meta-learner updates its internal memory based on the
observed effect its prediction had. Our meta-learner can be combined with any
gradient-based optimizer, learns on the fly and can be transferred to new
optimization tasks. In our evaluations we show that our meta-learning algorithm
speeds up learning of MNIST classification and a variety of learning control
tasks, either in batch or online learning settings.
",Meta-Learning for Robotics,meta-learning in robotics
"  We present a new technique called contrastive principal component analysis
(cPCA) that is designed to discover low-dimensional structure that is unique to
a dataset, or enriched in one dataset relative to other data. The technique is
a generalization of standard PCA, for the setting where multiple datasets are
available -- e.g. a treatment and a control group, or a mixed versus a
homogeneous population -- and the goal is to explore patterns that are specific
to one of the datasets. We conduct a wide variety of experiments in which cPCA
identifies important dataset-specific patterns that are missed by PCA,
demonstrating that it is useful for many applications: subgroup discovery,
visualizing trends, feature selection, denoising, and data-dependent
standardization. We provide geometrical interpretations of cPCA and show that
it satisfies desirable theoretical guarantees. We also extend cPCA to nonlinear
settings in the form of kernel cPCA. We have released our code as a python
package and documentation is on Github.
",Contrastive Principal Component Analysis (cPCA),contrastive principal component analysis (cpca)
"  We study a variant of the stochastic $K$-armed bandit problem, which we call
""bandits with delayed, aggregated anonymous feedback"". In this problem, when
the player pulls an arm, a reward is generated, however it is not immediately
observed. Instead, at the end of each round the player observes only the sum of
a number of previously generated rewards which happen to arrive in the given
round. The rewards are stochastically delayed and due to the aggregated nature
of the observations, the information of which arm led to a particular reward is
lost. The question is what is the cost of the information loss due to this
delayed, aggregated anonymous feedback? Previous works have studied bandits
with stochastic, non-anonymous delays and found that the regret increases only
by an additive factor relating to the expected delay. In this paper, we show
that this additive regret increase can be maintained in the harder delayed,
aggregated anonymous feedback setting when the expected delay (or a bound on
it) is known. We provide an algorithm that matches the worst case regret of the
non-anonymous problem exactly when the delays are bounded, and up to
logarithmic factors or an additive variance term for unbounded delays.
",Bandit Problems with Delayed Feedback,stochastic k-armed bandit problem with delayed and aggregated feedback
"  We consider designing a robust structured sparse sensing matrix consisting of
a sparse matrix with a few non-zero entries per row and a dense base matrix for
capturing signals efficiently We design the robust structured sparse sensing
matrix through minimizing the distance between the Gram matrix of the
equivalent dictionary and the target Gram of matrix holding small mutual
coherence. Moreover, a regularization is added to enforce the robustness of the
optimized structured sparse sensing matrix to the sparse representation error
(SRE) of signals of interests. An alternating minimization algorithm with
global sequence convergence is proposed for solving the corresponding
optimization problem. Numerical experiments on synthetic data and natural
images show that the obtained structured sensing matrix results in a higher
signal reconstruction than a random dense sensing matrix.
",Sparse Sensing Matrix Design,structured sparse sensing matrix design
"  The most data-efficient algorithms for reinforcement learning in robotics are
model-based policy search algorithms, which alternate between learning a
dynamical model of the robot and optimizing a policy to maximize the expected
return given the model and its uncertainties. Among the few proposed
approaches, the recently introduced Black-DROPS algorithm exploits a black-box
optimization algorithm to achieve both high data-efficiency and good
computation times when several cores are used; nevertheless, like all
model-based policy search approaches, Black-DROPS does not scale to high
dimensional state/action spaces. In this paper, we introduce a new model
learning procedure in Black-DROPS that leverages parameterized black-box priors
to (1) scale up to high-dimensional systems, and (2) be robust to large
inaccuracies of the prior information. We demonstrate the effectiveness of our
approach with the ""pendubot"" swing-up task in simulation and with a physical
hexapod robot (48D state space, 18D action space) that has to walk forward as
fast as possible. The results show that our new algorithm is more
data-efficient than previous model-based policy search algorithms (with and
without priors) and that it can allow a physical 6-legged robot to learn new
gaits in only 16 to 30 seconds of interaction time.
",Model-based policy search in robotics,reinforcement learning for robotics
"  One of the most interesting features of Bayesian optimization for direct
policy search is that it can leverage priors (e.g., from simulation or from
previous tasks) to accelerate learning on a robot. In this paper, we are
interested in situations for which several priors exist but we do not know in
advance which one fits best the current situation. We tackle this problem by
introducing a novel acquisition function, called Most Likely Expected
Improvement (MLEI), that combines the likelihood of the priors and the expected
improvement. We evaluate this new acquisition function on a transfer learning
task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has
to learn to walk on flat ground and on stairs, with priors corresponding to
different stairs and different kinds of damages. Our results show that MLEI
effectively identifies and exploits the priors, even when there is no obvious
match between the current situations and the priors.
",Bayesian Optimization for Robotics,bayesian optimization for transfer learning in robotics
"  In multi-echelon inventory systems the performance of a given node is
affected by events that occur at many other nodes and in many other time
periods. For example, a supply disruption upstream will have an effect on
downstream, customer-facing nodes several periods later as the disruption
""cascades"" through the system. There is very little research on stock-out
prediction in single-echelon systems and (to the best of our knowledge) none on
multi-echelon systems. However, in real the world, it is clear that there is
significant interest in techniques for this sort of stock-out prediction.
Therefore, our research aims to fill this gap by using deep neural networks
(DNN) to predict stock-outs in multi-echelon supply chains.
",Stock-out prediction in supply chains,stock-out prediction in multi-echelon supply chains
"  Recent studies have proposed that the diffusion of messenger molecules, such
as monoamines, can mediate the plastic adaptation of synapses in supervised
learning of neural networks. Based on these findings we developed a model for
neural learning, where the signal for plastic adaptation is assumed to
propagate through the extracellular space. We investigate the conditions
allowing learning of Boolean rules in a neural network. Even fully excitatory
networks show very good learning performances. Moreover, the investigation of
the plastic adaptation features optimizing the performance suggests that
learning is very sensitive to the extent of the plastic adaptation and the
spatial range of synaptic connections.
",Neural Network Plasticity Modeling,neural network learning and plastic adaptation
"  In this paper, we propose a novel progressive parameter pruning method for
Convolutional Neural Network acceleration, named Structured Probabilistic
Pruning (SPP), which effectively prunes weights of convolutional layers in a
probabilistic manner. Unlike existing deterministic pruning approaches, where
unimportant weights are permanently eliminated, SPP introduces a pruning
probability for each weight, and pruning is guided by sampling from the pruning
probabilities. A mechanism is designed to increase and decrease pruning
probabilities based on importance criteria in the training process. Experiments
show that, with 4x speedup, SPP can accelerate AlexNet with only 0.3% loss of
top-5 accuracy and VGG-16 with 0.8% loss of top-5 accuracy in ImageNet
classification. Moreover, SPP can be directly applied to accelerate
multi-branch CNN networks, such as ResNet, without specific adaptations. Our 2x
speedup ResNet-50 only suffers 0.8% loss of top-5 accuracy on ImageNet. We
further show the effectiveness of SPP on transfer learning tasks.
",Convolutional Neural Network Pruning,convolutional neural network acceleration
"  We consider image classification with estimated depth. This problem falls
into the domain of transfer learning, since we are using a model trained on a
set of depth images to generate depth maps (additional features) for use in
another classification problem using another disjoint set of images. It's
challenging as no direct depth information is provided. Though depth estimation
has been well studied, none have attempted to aid image classification with
estimated depth. Therefore, we present a way of transferring domain knowledge
on depth estimation to a separate image classification task over a disjoint set
of train, and test data. We build a RGBD dataset based on RGB dataset and do
image classification on it. Then evaluation the performance of neural networks
on the RGBD dataset compared to the RGB dataset. From our experiments, the
benefit is significant with shallow and deep networks. It improves ResNet-20 by
0.55% and ResNet-56 by 0.53%. Our code and dataset are available publicly.
",Depth-Aided Image Classification,transfer learning for image classification with estimated depth
"  Finding optimal feedback controllers for nonlinear dynamic systems from data
is hard. Recently, Bayesian optimization (BO) has been proposed as a powerful
framework for direct controller tuning from experimental trials. For selecting
the next query point and finding the global optimum, BO relies on a
probabilistic description of the latent objective function, typically a
Gaussian process (GP). As is shown herein, GPs with a common kernel choice can,
however, lead to poor learning outcomes on standard quadratic control problems.
For a first-order system, we construct two kernels that specifically leverage
the structure of the well-known Linear Quadratic Regulator (LQR), yet retain
the flexibility of Bayesian nonparametric learning. Simulations of uncertain
linear and nonlinear systems demonstrate that the LQR kernels yield superior
learning performance.
",Bayesian Optimization for Controller Tuning,bayesian optimization for nonlinear control systems
"  We study the least squares regression problem \begin{align*} \min_{\Theta \in
\mathcal{S}_{\odot D,R}} \|A\Theta-b\|_2, \end{align*} where
$\mathcal{S}_{\odot D,R}$ is the set of $\Theta$ for which $\Theta =
\sum_{r=1}^{R} \theta_1^{(r)} \circ \cdots \circ \theta_D^{(r)}$ for vectors
$\theta_d^{(r)} \in \mathbb{R}^{p_d}$ for all $r \in [R]$ and $d \in [D]$, and
$\circ$ denotes the outer product of vectors. That is, $\Theta$ is a
low-dimensional, low-rank tensor. This is motivated by the fact that the number
of parameters in $\Theta$ is only $R \cdot \sum_{d=1}^D p_d$, which is
significantly smaller than the $\prod_{d=1}^{D} p_d$ number of parameters in
ordinary least squares regression. We consider the above CP decomposition model
of tensors $\Theta$, as well as the Tucker decomposition. For both models we
show how to apply data dimensionality reduction techniques based on {\it
sparse} random projections $\Phi \in \mathbb{R}^{m \times n}$, with $m \ll n$,
to reduce the problem to a much smaller problem $\min_{\Theta} \|\Phi A \Theta
- \Phi b\|_2$, for which if $\Theta'$ is a near-optimum to the smaller problem,
then it is also a near optimum to the original problem. We obtain significantly
smaller dimension and sparsity in $\Phi$ than is possible for ordinary least
squares regression, and we also provide a number of numerical simulations
supporting our theory.
",Low-Rank Tensor Regression,tensor decomposition and dimensionality reduction for least squares regression
"  A latent-variable model is introduced for text matching, inferring sentence
representations by jointly optimizing generative and discriminative objectives.
To alleviate typical optimization challenges in latent-variable models for
text, we employ deconvolutional networks as the sequence decoder (generator),
providing learned latent codes with more semantic information and better
generalization. Our model, trained in an unsupervised manner, yields stronger
empirical predictive performance than a decoder based on Long Short-Term Memory
(LSTM), with less parameters and considerably faster training. Further, we
apply it to text sequence-matching problems. The proposed model significantly
outperforms several strong sentence-encoding baselines, especially in the
semi-supervised setting.
",Latent Variable Models for Text Matching,latent-variable models for text matching
"  Aiming to augment generative models with external memory, we interpret the
output of a memory module with stochastic addressing as a conditional mixture
distribution, where a read operation corresponds to sampling a discrete memory
address and retrieving the corresponding content from memory. This perspective
allows us to apply variational inference to memory addressing, which enables
effective training of the memory module by using the target information to
guide memory lookups. Stochastic addressing is particularly well-suited for
generative models as it naturally encourages multimodality which is a prominent
aspect of most high-dimensional datasets. Treating the chosen address as a
latent variable also allows us to quantify the amount of information gained
with a memory lookup and measure the contribution of the memory module to the
generative process. To illustrate the advantages of this approach we
incorporate it into a variational autoencoder and apply the resulting model to
the task of generative few-shot learning. The intuition behind this
architecture is that the memory module can pick a relevant template from memory
and the continuous part of the model can concentrate on modeling remaining
variations. We demonstrate empirically that our model is able to identify and
access the relevant memory contents even with hundreds of unseen Omniglot
characters in memory
",External Memory for Generative Models,memory-augmented generative models
"  In this paper, we propose a novel recurrent neural network architecture for
speech separation. This architecture is constructed by unfolding the iterations
of a sequential iterative soft-thresholding algorithm (ISTA) that solves the
optimization problem for sparse nonnegative matrix factorization (NMF) of
spectrograms. We name this network architecture deep recurrent NMF (DR-NMF).
The proposed DR-NMF network has three distinct advantages. First, DR-NMF
provides better interpretability than other deep architectures, since the
weights correspond to NMF model parameters, even after training. This
interpretability also provides principled initializations that enable faster
training and convergence to better solutions compared to conventional random
initialization. Second, like many deep networks, DR-NMF is an order of
magnitude faster at test time than NMF, since computation of the network output
only requires evaluating a few layers at each time step. Third, when a limited
amount of training data is available, DR-NMF exhibits stronger generalization
and separation performance compared to sparse NMF and state-of-the-art
long-short term memory (LSTM) networks. When a large amount of training data is
available, DR-NMF achieves lower yet competitive separation performance
compared to LSTM networks.
",Speech Separation Using Deep Recurrent NMF Architecture,deep learning for speech separation
"  By exploiting the property that the RBM log-likelihood function is the
difference of convex functions, we formulate a stochastic variant of the
difference of convex functions (DC) programming to minimize the negative
log-likelihood. Interestingly, the traditional contrastive divergence algorithm
is a special case of the above formulation and the hyperparameters of the two
algorithms can be chosen such that the amount of computation per mini-batch is
identical. We show that for a given computational budget the proposed algorithm
almost always reaches a higher log-likelihood more rapidly, compared to the
standard contrastive divergence algorithm. Further, we modify this algorithm to
use the centered gradients and show that it is more efficient and effective
compared to the standard centered gradient algorithm on benchmark datasets.
",Stochastic Optimization of Restricted Boltzmann Machines,deep learning optimization
"  Feature engineering is a crucial step in the process of predictive modeling.
It involves the transformation of given feature space, typically using
mathematical functions, with the objective of reducing the modeling error for a
given target. However, there is no well-defined basis for performing effective
feature engineering. It involves domain knowledge, intuition, and most of all,
a lengthy process of trial and error. The human attention involved in
overseeing this process significantly influences the cost of model generation.
We present a new framework to automate feature engineering. It is based on
performance driven exploration of a transformation graph, which systematically
and compactly enumerates the space of given options. A highly efficient
exploration strategy is derived through reinforcement learning on past
examples.
",Automated Feature Engineering,feature engineering
"  We study the problem of learning a latent variable model from a stream of
data. Latent variable models are popular in practice because they can explain
observed data in terms of unobserved concepts. These models have been
traditionally studied in the offline setting. In the online setting, on the
other hand, the online EM is arguably the most popular algorithm for learning
latent variable models. Although the online EM is computationally efficient, it
typically converges to a local optimum. In this work, we develop a new online
learning algorithm for latent variable models, which we call SpectralLeader.
SpectralLeader always converges to the global optimum, and we derive a
sublinear upper bound on its $n$-step regret in the bag-of-words model. In both
synthetic and real-world experiments, we show that SpectralLeader performs
similarly to or better than the online EM with tuned hyper-parameters.
",Online Latent Variable Model Learning,online learning of latent variable models
"  This paper addresses the question of emotion classification. The task
consists in predicting emotion labels (taken among a set of possible labels)
best describing the emotions contained in short video clips. Building on a
standard framework -- lying in describing videos by audio and visual features
used by a supervised classifier to infer the labels -- this paper investigates
several novel directions. First of all, improved face descriptors based on 2D
and 3D Convo-lutional Neural Networks are proposed. Second, the paper explores
several fusion methods, temporal and multimodal, including a novel hierarchical
method combining features and scores. In addition, we carefully reviewed the
different stages of the pipeline and designed a CNN architecture adapted to the
task; this is important as the size of the training set is small compared to
the difficulty of the problem, making generalization difficult. The so-obtained
model ranked 4th at the 2017 Emotion in the Wild challenge with the accuracy of
58.8 %.
",Emotion Classification in Videos,emotion classification
"  Deep learning algorithms offer a powerful means to automatically analyze the
content of medical images. However, many biological samples of interest are
primarily transparent to visible light and contain features that are difficult
to resolve with a standard optical microscope. Here, we use a convolutional
neural network (CNN) not only to classify images, but also to optimize the
physical layout of the imaging device itself. We increase the classification
accuracy of a microscope's recorded images by merging an optical model of image
formation into the pipeline of a CNN. The resulting network simultaneously
determines an ideal illumination arrangement to highlight important sample
features during image acquisition, along with a set of convolutional weights to
classify the detected images post-capture. We demonstrate our joint
optimization technique with an experimental microscope configuration that
automatically identifies malaria-infected cells with 5-10% higher accuracy than
standard and alternative microscope lighting designs.
",Microscope Image Optimization using Deep Learning,medical imaging and microscopy
"  Swarm systems constitute a challenging problem for reinforcement learning
(RL) as the algorithm needs to learn decentralized control policies that can
cope with limited local sensing and communication abilities of the agents.
While it is often difficult to directly define the behavior of the agents,
simple communication protocols can be defined more easily using prior knowledge
about the given task. In this paper, we propose a number of simple
communication protocols that can be exploited by deep reinforcement learning to
find decentralized control policies in a multi-robot swarm environment. The
protocols are based on histograms that encode the local neighborhood relations
of the agents and can also transmit task-specific information, such as the
shortest distance and direction to a desired target. In our framework, we use
an adaptation of Trust Region Policy Optimization to learn complex
collaborative tasks, such as formation building and building a communication
link. We evaluate our findings in a simulated 2D-physics environment, and
compare the implications of different communication protocols.
",Communication Protocols in Swarm Robotics,decentralized multi-agent reinforcement learning
"  Social networks involve both positive and negative relationships, which can
be captured in signed graphs. The {\em edge sign prediction problem} aims to
predict whether an interaction between a pair of nodes will be positive or
negative. We provide theoretical results for this problem that motivate natural
improvements to recent heuristics.
  The edge sign prediction problem is related to correlation clustering; a
positive relationship means being in the same cluster. We consider the
following model for two clusters: we are allowed to query any pair of nodes
whether they belong to the same cluster or not, but the answer to the query is
corrupted with some probability $0<q<\frac{1}{2}$. Let $\delta=1-2q$ be the
bias. We provide an algorithm that recovers all signs correctly with high
probability in the presence of noise with $O(\frac{n\log
n}{\delta^2}+\frac{\log^2 n}{\delta^6})$ queries. This is the best known result
for this problem for all but tiny $\delta$, improving on the recent work of
Mazumdar and Saha \cite{mazumdar2017clustering}. We also provide an algorithm
that performs $O(\frac{n\log n}{\delta^4})$ queries, and uses breadth first
search as its main algorithmic primitive. While both the running time and the
number of queries for this algorithm are sub-optimal, our result relies on
novel theoretical techniques, and naturally suggests the use of edge-disjoint
paths as a feature for predicting signs in online social networks.
Correspondingly, we experiment with using edge disjoint $s-t$ paths of short
length as a feature for predicting the sign of edge $(s,t)$ in real-world
signed networks. Empirical findings suggest that the use of such paths improves
the classification accuracy, especially for pairs of nodes with no common
neighbors.
",Edge Sign Prediction in Signed Graphs,edge sign prediction in signed graphs
"  We study the problem of learning description logic (DL) ontologies in Angluin
et al.'s framework of exact learning via queries. We admit membership queries
(""is a given subsumption entailed by the target ontology?"") and equivalence
queries (""is a given ontology equivalent to the target ontology?""). We present
three main results: (1) ontologies formulated in (two relevant versions of) the
description logic DL-Lite can be learned with polynomially many queries of
polynomial size; (2) this is not the case for ontologies formulated in the
description logic EL, even when only acyclic ontologies are admitted; and (3)
ontologies formulated in a fragment of EL related to the web ontology language
OWL 2 RL can be learned in polynomial time. We also show that neither
membership nor equivalence queries alone are sufficient in cases (1) and (3).
",Learning Description Logic Ontologies via Queries,description logic ontology learning
"  Generative Adversarial Networks (GANs) produce systematically better quality
samples when class label information is provided., i.e. in the conditional GAN
setup. This is still observed for the recently proposed Wasserstein GAN
formulation which stabilized adversarial training and allows considering high
capacity network architectures such as ResNet. In this work we show how to
boost conditional GAN by augmenting available class labels. The new classes
come from clustering in the representation space learned by the same GAN model.
The proposed strategy is also feasible when no class information is available,
i.e. in the unsupervised setup. Our generated samples reach state-of-the-art
Inception scores for CIFAR-10 and STL-10 datasets in both supervised and
unsupervised setup.
",Conditional GAN with Augmented Class Labels,improving generative adversarial networks with class labels
"  Classification of imbalanced datasets is a challenging task for standard
algorithms. Although many methods exist to address this problem in different
ways, generating artificial data for the minority class is a more general
approach compared to algorithmic modifications. SMOTE algorithm and its
variations generate synthetic samples along a line segment that joins minority
class instances. In this paper we propose Geometric SMOTE (G-SMOTE) as a
generalization of the SMOTE data generation mechanism. G-SMOTE generates
synthetic samples in a geometric region of the input space, around each
selected minority instance. While in the basic configuration this region is a
hyper-sphere, G-SMOTE allows its deformation to a hyper-spheroid and finally to
a line segment, emulating, in the last case, the SMOTE mechanism. The
performance of G-SMOTE is compared against multiple standard oversampling
algorithms. We present empirical results that show a significant improvement in
the quality of the generated data when G-SMOTE is used as an oversampling
algorithm.
",Oversampling Techniques for Imbalanced Datasets,data imbalance and classification
"  The quantum autoencoder is a recent paradigm in the field of quantum machine
learning, which may enable an enhanced use of resources in quantum
technologies. To this end, quantum neural networks with less nodes in the inner
than in the outer layers were considered. Here, we propose a useful connection
between approximate quantum adders and quantum autoencoders. Specifically, this
link allows us to employ optimized approximate quantum adders, obtained with
genetic algorithms, for the implementation of quantum autoencoders for a
variety of initial states. Furthermore, we can also directly optimize the
quantum autoencoders via genetic algorithms. Our approach opens a different
path for the design of quantum autoencoders in controllable quantum platforms.
",Quantum Autoencoder Design Optimization,quantum machine learning
"  We present an approach to automate the process of discovering optimization
methods, with a focus on deep learning architectures. We train a Recurrent
Neural Network controller to generate a string in a domain specific language
that describes a mathematical update equation based on a list of primitive
functions, such as the gradient, running average of the gradient, etc. The
controller is trained with Reinforcement Learning to maximize the performance
of a model after a few epochs. On CIFAR-10, our method discovers several update
rules that are better than many commonly used optimizers, such as Adam,
RMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two
new optimizers, named PowerSign and AddSign, which we show transfer well and
improve training on a variety of different tasks and architectures, including
ImageNet classification and Google's neural machine translation system.
",Deep Learning Optimizers,optimization methods
"  Black box variational inference (BBVI) with reparameterization gradients
triggered the exploration of divergence measures other than the
Kullback-Leibler (KL) divergence, such as alpha divergences. In this paper, we
view BBVI with generalized divergences as a form of estimating the marginal
likelihood via biased importance sampling. The choice of divergence determines
a bias-variance trade-off between the tightness of a bound on the marginal
likelihood (low bias) and the variance of its gradient estimators. Drawing on
variational perturbation theory of statistical physics, we use these insights
to construct a family of new variational bounds. Enumerated by an odd integer
order $K$, this family captures the standard KL bound for $K=1$, and converges
to the exact marginal likelihood as $K\to\infty$. Compared to
alpha-divergences, our reparameterization gradients have a lower variance. We
show in experiments on Gaussian Processes and Variational Autoencoders that the
new bounds are more mass covering, and that the resulting posterior covariances
are closer to the true posterior and lead to higher likelihoods on held-out
data.
",Variational Bounds for Marginal Likelihood Estimation,variational inference with generalized divergences
"  E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell
billions of products. Machine learning (ML) algorithms involving products are
often used to improve the customer experience and increase revenue, e.g.,
product similarity, recommendation, and price estimation. The products are
required to be represented as features before training an ML algorithm. In this
paper, we propose an approach called MRNet-Product2Vec for creating generic
embeddings of products within an e-commerce ecosystem. We learn a dense and
low-dimensional embedding where a diverse set of signals related to a product
are explicitly injected into its representation. We train a Discriminative
Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a
product title fed through a Bidirectional RNN and at the output, product labels
corresponding to fifteen different tasks are predicted. The task set includes
several intrinsic characteristics about a product such as price, weight, size,
color, popularity, and material. We evaluate the proposed embedding
quantitatively and qualitatively. We demonstrate that they are almost as good
as sparse and extremely high-dimensional TF-IDF representation in spite of
having less than 3% of the TF-IDF dimension. We also use a multimodal
autoencoder for comparing products from different language-regions and show
preliminary yet promising qualitative results.
",Product Embeddings in E-commerce,product embeddings in e-commerce
"  The goal of personalized history-based recommendation is to automatically
output a distribution over all the items given a sequence of previous purchases
of a user. In this work, we present a novel approach that uses a recurrent
network for summarizing the history of purchases, continuous vectors
representing items for scalability, and a novel attention-based recurrent
mixture density network, which outputs each component in a mixture
sequentially, for modelling a multi-modal conditional distribution. We evaluate
the proposed approach on two publicly available datasets, MovieLens-20M and
RecSys15. The experiments show that the proposed approach, which explicitly
models the multi-modal nature of the predictive distribution, is able to
improve the performance over various baselines in terms of precision, recall
and nDCG.
",Personalized Recommendation Systems,personalized recommendation systems
"  Many state-of-the-art algorithms for solving hard combinatorial problems in
artificial intelligence (AI) include elements of stochasticity that lead to
high variations in runtime, even for a fixed problem instance. Knowledge about
the resulting runtime distributions (RTDs) of algorithms on given problem
instances can be exploited in various meta-algorithmic procedures, such as
algorithm selection, portfolios, and randomized restarts. Previous work has
shown that machine learning can be used to individually predict mean, median
and variance of RTDs. To establish a new state-of-the-art in predicting RTDs,
we demonstrate that the parameters of an RTD should be learned jointly and that
neural networks can do this well by directly optimizing the likelihood of an
RTD given runtime observations. In an empirical study involving five algorithms
for SAT solving and AI planning, we show that neural networks predict the true
RTDs of unseen instances better than previous methods, and can even do so when
only few runtime observations are available per training instance.
",Runtime Distribution Prediction for AI Algorithms,predicting runtime distributions of combinatorial algorithms
"  Regularized empirical risk minimization using kernels and their corresponding
reproducing kernel Hilbert spaces (RKHSs) plays an important role in machine
learning. However, the actually used kernel often depends on one or on a few
hyperparameters or the kernel is even data dependent in a much more complicated
manner. Examples are Gaussian RBF kernels, kernel learning, and hierarchical
Gaussian kernels which were recently proposed for deep learning. Therefore, the
actually used kernel is often computed by a grid search or in an iterative
manner and can often only be considered as an approximation to the ""ideal"" or
""optimal"" kernel. The paper gives conditions under which classical kernel based
methods based on a convex Lipschitz loss function and on a bounded and smooth
kernel are stable, if the probability measure $P$, the regularization parameter
$\lambda$, and the kernel $k$ may slightly change in a simultaneous manner.
Similar results are also given for pairwise learning. Therefore, the topic of
this paper is somewhat more general than in classical robust statistics, where
usually only the influence of small perturbations of the probability measure
$P$ on the estimated function is considered.
",Kernel Stability in Machine Learning,kernel methods in machine learning
"  Recurrent neural networks (RNNs) have shown promising results in audio and
speech processing applications due to their strong capabilities in modelling
sequential data. In many applications, RNNs tend to outperform conventional
models based on GMM/UBMs and i-vectors. Increasing popularity of IoT devices
makes a strong case for implementing RNN based inferences for applications such
as acoustics based authentication, voice commands, and edge analytics for smart
homes. Nonetheless, the feasibility and performance of RNN based inferences on
resources-constrained IoT devices remain largely unexplored. In this paper, we
investigate the feasibility of using RNNs for an end-to-end authentication
system based on breathing acoustics. We evaluate the performance of RNN models
on three types of devices; smartphone, smartwatch, and Raspberry Pi and show
that unlike CNN models, RNN models can be easily ported onto
resource-constrained devices without a significant loss in accuracy.
",RNN-based Acoustic Authentication for IoT Devices,application of recurrent neural networks in iot devices for audio and speech processing
"  We present a scalable and robust Bayesian inference method for linear state
space models. The method is applied to demand forecasting in the context of a
large e-commerce platform, paying special attention to intermittent and bursty
target statistics. Inference is approximated by the Newton-Raphson algorithm,
reduced to linear-time Kalman smoothing, which allows us to operate on several
orders of magnitude larger problems than previous related work. In a study on
large real-world sales datasets, our method outperforms competing approaches on
fast and medium moving items.
",Bayesian Inference for Demand Forecasting,scalable bayesian inference for demand forecasting
"  Most machine learning and deep neural network algorithms rely on certain
iterative algorithms to optimise their utility/cost functions, e.g. Stochastic
Gradient Descent. In distributed learning, the networked nodes have to work
collaboratively to update the model parameters, and the way how they proceed is
referred to as synchronous parallel design (or barrier control). Synchronous
parallel protocol is the building block of any distributed learning framework,
and its design has direct impact on the performance and scalability of the
system.
  In this paper, we propose a new barrier control technique - Probabilistic
Synchronous Parallel (PSP). Com- paring to the previous Bulk Synchronous
Parallel (BSP), Stale Synchronous Parallel (SSP), and (Asynchronous Parallel)
ASP, the proposed solution e ectively improves both the convergence speed and
the scalability of the SGD algorithm by introducing a sampling primitive into
the system. Moreover, we also show that the sampling primitive can be applied
atop of the existing barrier control mechanisms to derive fully distributed
PSP-based synchronous parallel.
  We not only provide a thorough theoretical analysis1 on the convergence of
PSP-based SGD algorithm, but also implement a full-featured distributed
learning framework called Actor and perform intensive evaluation atop of it.
",Distributed Barrier Control for Machine Learning,distributed machine learning algorithms
"  The heavy burdens of computation and off-chip traffic impede deploying the
large scale convolution neural network on embedded platforms. As CNN is
attributed to the strong endurance to computation errors, employing block
floating point (BFP) arithmetics in CNN accelerators could save the hardware
cost and data traffics efficiently, while maintaining the classification
accuracy. In this paper, we verify the effects of word width definitions in BFP
to the CNN performance without retraining. Several typical CNN models,
including VGG16, ResNet-18, ResNet-50 and GoogLeNet, were tested in this paper.
Experiments revealed that 8-bit mantissa, including sign bit, in BFP
representation merely induced less than 0.3% accuracy loss. In addition, we
investigate the computational errors in theory and develop the noise-to-signal
ratio (NSR) upper bound, which provides the promising guidance for BFP based
CNN engine design.
",Block Floating Point Arithmetic in CNN Accelerators,efficient convolutional neural network deployment on embedded platforms using block floating point arithmetic
"  This paper provides an analysis of the tradeoff between asymptotic bias
(suboptimality with unlimited data) and overfitting (additional suboptimality
due to limited data) in the context of reinforcement learning with partial
observability. Our theoretical analysis formally characterizes that while
potentially increasing the asymptotic bias, a smaller state representation
decreases the risk of overfitting. This analysis relies on expressing the
quality of a state representation by bounding L1 error terms of the associated
belief states. Theoretical results are empirically illustrated when the state
representation is a truncated history of observations, both on synthetic POMDPs
and on a large-scale POMDP in the context of smartgrids, with real-world data.
Finally, similarly to known results in the fully observable setting, we also
briefly discuss and empirically illustrate how using function approximators and
adapting the discount factor may enhance the tradeoff between asymptotic bias
and overfitting in the partially observable context.
",Reinforcement Learning with Partial Observability,reinforcement learning
"  Conventional automatic speech recognition (ASR) typically performs
multi-level pattern recognition tasks that map the acoustic speech waveform
into a hierarchy of speech units. But, it is widely known that information loss
in the earlier stage can propagate through the later stages. After the
resurgence of deep learning, interest has emerged in the possibility of
developing a purely end-to-end ASR system from the raw waveform to the
transcription without any predefined alignments and hand-engineered models.
However, the successful attempts in end-to-end architecture still used
spectral-based features, while the successful attempts in using raw waveform
were still based on the hybrid deep neural network - Hidden Markov model
(DNN-HMM) framework. In this paper, we construct the first end-to-end
attention-based encoder-decoder model to process directly from raw speech
waveform to the text transcription. We called the model as ""Attention-based
Wav2Text"". To assist the training process of the end-to-end model, we propose
to utilize a feature transfer learning. Experimental results also reveal that
the proposed Attention-based Wav2Text model directly with raw waveform could
achieve a better result in comparison with the attentional encoder-decoder
model trained on standard front-end filterbank features.
",End-to-End Speech Recognition,end-to-end automatic speech recognition
"  Instrumenting and collecting annotated visual grasping datasets to train
modern machine learning algorithms can be extremely time-consuming and
expensive. An appealing alternative is to use off-the-shelf simulators to
render synthetic data for which ground-truth annotations are generated
automatically. Unfortunately, models trained purely on simulated data often
fail to generalize to the real world. We study how randomized simulated
environments and domain adaptation methods can be extended to train a grasping
system to grasp novel objects from raw monocular RGB images. We extensively
evaluate our approaches with a total of more than 25,000 physical test grasps,
studying a range of simulation conditions and domain adaptation methods,
including a novel extension of pixel-level domain adaptation that we term the
GraspGAN. We show that, by using synthetic data and domain adaptation, we are
able to reduce the number of real-world samples needed to achieve a given level
of performance by up to 50 times, using only randomly generated simulated
objects. We also show that by using only unlabeled real-world data and our
GraspGAN methodology, we obtain real-world grasping performance without any
real-world labels that is similar to that achieved with 939,777 labeled
real-world samples.
",Synthetic Data for Robot Grasping,domain adaptation for grasping systems
"  Machine learning (ML) is becoming a commodity. Numerous ML frameworks and
services are available to data holders who are not ML experts but want to train
predictive models on their data. It is important that ML models trained on
sensitive inputs (e.g., personal images or documents) not leak too much
information about the training data.
  We consider a malicious ML provider who supplies model-training code to the
data holder, does not observe the training, but then obtains white- or
black-box access to the resulting model. In this setting, we design and
implement practical algorithms, some of them very similar to standard ML
techniques such as regularization and data augmentation, that ""memorize""
information about the training dataset in the model yet the model is as
accurate and predictive as a conventionally trained model. We then explain how
the adversary can extract memorized information from the model.
  We evaluate our techniques on standard ML tasks for image classification
(CIFAR10), face recognition (LFW and FaceScrub), and text analysis (20
Newsgroups and IMDB). In all cases, we show how our algorithms create models
that have high predictive power yet allow accurate extraction of subsets of
their training data.
",Privacy in Machine Learning,machine learning model security
"  Active Learning (AL) methods have proven cost-saving against passive
supervised methods in many application domains. An active learner, aiming to
find some target hypothesis, formulates sequential queries to some oracle. The
set of hypotheses consistent with the already answered queries is called
version space. Several query selection measures (QSMs) for determining the best
query to ask next have been proposed. Assuming binaryoutcome queries, we
analyze various QSMs wrt. to the discrimination power of their selected queries
within the current version space. As a result, we derive superiority and
equivalence relations between these QSMs and introduce improved versions of
existing QSMs to overcome identified issues. The obtained picture gives a hint
about which QSMs should preferably be used in pool-based AL scenarios.
Moreover, we deduce properties optimal queries wrt. QSMs must satisfy. Based on
these, we demonstrate how efficient heuristic search methods for optimal
queries in query synthesis AL scenarios can be devised.
",Active Learning Query Selection Measures,active learning
"  We present a factorized hierarchical variational autoencoder, which learns
disentangled and interpretable representations from sequential data without
supervision. Specifically, we exploit the multi-scale nature of information in
sequential data by formulating it explicitly within a factorized hierarchical
graphical model that imposes sequence-dependent priors and sequence-independent
priors to different sets of latent variables. The model is evaluated on two
speech corpora to demonstrate, qualitatively, its ability to transform speakers
or linguistic content by manipulating different sets of latent variables; and
quantitatively, its ability to outperform an i-vector baseline for speaker
verification and reduce the word error rate by as much as 35% in mismatched
train/test scenarios for automatic speech recognition tasks.
",Unsupervised Representation Learning for Speech Data,deep learning for speech recognition
"  Multi-task/Multi-output learning seeks to exploit correlation among tasks to
enhance performance over learning or solving each task independently. In this
paper, we investigate this problem in the context of Gaussian Processes (GPs)
and propose a new model which learns a mixture of latent processes by
decomposing the covariance matrix into a sum of structured hidden components
each of which is controlled by a latent GP over input features and a ""weight""
over tasks. From this sum structure, we propose a parallelizable parameter
learning algorithm with a predetermined initialization for the ""weights"". We
also notice that an ensemble parameter learning approach using mini-batches of
training data not only reduces the computation complexity of learning but also
improves the regression performance. We evaluate our model on two datasets, the
smaller Swiss Jura dataset and another relatively larger ATMS dataset from
NOAA. Substantial improvements are observed compared with established
alternatives.
",Multi-Task Learning with Gaussian Processes,multi-task learning for gaussian processes
"  Imitation learning holds the promise to address challenging robotic tasks
such as autonomous navigation. It however requires a human supervisor to
oversee the training process and send correct control commands to robots
without feedback, which is always prone to error and expensive. To minimize
human involvement and avoid manual labeling of data in the robotic autonomous
navigation with imitation learning, this paper proposes a novel semi-supervised
imitation learning solution based on a multi-sensory design. This solution
includes a suboptimal sensor policy based on sensor fusion to automatically
label states encountered by a robot to avoid human supervision during training.
In addition, a recording policy is developed to throttle the adversarial affect
of learning too much from the suboptimal sensor policy. This solution allows
the robot to learn a navigation policy in a self-supervised manner. With
extensive experiments in indoor environments, this solution can achieve near
human performance in most of the tasks and even surpasses human performance in
case of unexpected events such as hardware failures or human operation errors.
To best of our knowledge, this is the first work that synthesizes sensor fusion
and imitation learning to enable robotic autonomous navigation in the real
world without human supervision.
",Semi-Supervised Imitation Learning for Autonomous Navigation,semi-supervised imitation learning for robotic autonomous navigation
"  Automatic event detection from time series signals has wide applications,
such as abnormal event detection in video surveillance and event detection in
geophysical data. Traditional detection methods detect events primarily by the
use of similarity and correlation in data. Those methods can be inefficient and
yield low accuracy. In recent years, because of the significantly increased
computational power, machine learning techniques have revolutionized many
science and engineering domains. In this study, we apply a deep-learning-based
method to the detection of events from time series seismic signals. However, a
direct adaptation of the similar ideas from 2D object detection to our problem
faces two challenges. The first challenge is that the duration of earthquake
event varies significantly; The other is that the proposals generated are
temporally correlated. To address these challenges, we propose a novel cascaded
region-based convolutional neural network to capture earthquake events in
different sizes, while incorporating contextual information to enrich features
for each individual proposal. To achieve a better generalization performance,
we use densely connected blocks as the backbone of our network. Because of the
fact that some positive events are not correctly annotated, we further
formulate the detection problem as a learning-from-noise problem. To verify the
performance of our detection methods, we employ our methods to seismic data
generated from a bi-axial ""earthquake machine"" located at Rock Mechanics
Laboratory, and we acquire labels with the help of experts. Through our
numerical tests, we show that our novel detection techniques yield high
accuracy. Therefore, our novel deep-learning-based detection methods can
potentially be powerful tools for locating events from time series data in
various applications.
",Deep Learning for Earthquake Event Detection in Time Series Signals,event detection from time series signals
"  We present a method for efficient learning of control policies for multiple
related robotic motor skills. Our approach consists of two stages, joint
training and specialization training. During the joint training stage, a neural
network policy is trained with minimal information to disambiguate the motor
skills. This forces the policy to learn a common representation of the
different tasks. Then, during the specialization training stage we selectively
split the weights of the policy based on a per-weight metric that measures the
disagreement among the multiple tasks. By splitting part of the control policy,
it can be further trained to specialize to each task. To update the control
policy during learning, we use Trust Region Policy Optimization with
Generalized Advantage Function (TRPOGAE). We propose a modification to the
gradient update stage of TRPO to better accommodate multi-task learning
scenarios. We evaluate our approach on three continuous motor skill learning
problems in simulation: 1) a locomotion task where three single legged robots
with considerable difference in shape and size are trained to hop forward, 2) a
manipulation task where three robot manipulators with different sizes and joint
types are trained to reach different locations in 3D space, and 3) locomotion
of a two-legged robot, whose range of motion of one leg is constrained in
different ways. We compare our training method to three baselines. The first
baseline uses only joint training for the policy, the second trains independent
policies for each task, and the last randomly selects weights to split. We show
that our approach learns more efficiently than each of the baseline methods.
",Multi-Task Robot Control Policy Learning,multi-task learning for robotic motor skills
"  One of the main problems in Network Intrusion Detection comes from constant
rise of new attacks, so that not enough labeled examples are available for the
new classes of attacks. Traditional Machine Learning approaches hardly address
such problem. This can be overcome with Zero-Shot Learning, a new approach in
the field of Computer Vision, which can be described in two stages: the
Attribute Learning and the Inference Stage. The goal of this paper is to
propose a new Inference Stage algorithm for Network Intrusion Detection. In
order to attain this objective, we firstly put forward an experimental setup
for the evaluation of the Zero-Shot Learning in Network Intrusion Detection
related tasks. Secondly, a decision tree based algorithm is applied to extract
rules for generating the attributes in the AL stage. Finally, using a
representation of a Zero-Shot Class as a point in the Grassmann manifold, an
explicit formula for the shortest distance between points in that manifold can
be used to compute the geodesic distance between the Zero-Shot Classes which
represent the new attacks and the Known Classes corresponding to the attack
categories. The experimental results in the datasets KDD Cup 99 and NSL-KDD
show that our approach with Zero-Shot Learning successfully addresses the
Network Intrusion Detection problem.
",Zero-Shot Learning for Network Intrusion Detection,zero-shot learning for network intrusion detection
"  Mobile edge computing (MEC) is a promising approach for enabling
cloud-computing capabilities at the edge of cellular networks. Nonetheless,
security is becoming an increasingly important issue in MEC-based applications.
In this paper, we propose a deep-learning-based model to detect security
threats. The model uses unsupervised learning to automate the detection
process, and uses location information as an important feature to improve the
performance of detection. Our proposed model can be used to detect malicious
applications at the edge of a cellular network, which is a serious security
threat. Extensive experiments are carried out with 10 different datasets, the
results of which illustrate that our deep-learning-based model achieves an
average gain of 6% accuracy compared with state-of-the-art machine learning
algorithms.
",MEC Security Threat Detection,mobile edge computing security
"  A method for statistical parametric speech synthesis incorporating generative
adversarial networks (GANs) is proposed. Although powerful deep neural networks
(DNNs) techniques can be applied to artificially synthesize speech waveform,
the synthetic speech quality is low compared with that of natural speech. One
of the issues causing the quality degradation is an over-smoothing effect often
observed in the generated speech parameters. A GAN introduced in this paper
consists of two neural networks: a discriminator to distinguish natural and
generated samples, and a generator to deceive the discriminator. In the
proposed framework incorporating the GANs, the discriminator is trained to
distinguish natural and generated speech parameters, while the acoustic models
are trained to minimize the weighted sum of the conventional minimum generation
loss and an adversarial loss for deceiving the discriminator. Since the
objective of the GANs is to minimize the divergence (i.e., distribution
difference) between the natural and generated speech parameters, the proposed
method effectively alleviates the over-smoothing effect on the generated speech
parameters. We evaluated the effectiveness for text-to-speech and voice
conversion, and found that the proposed method can generate more natural
spectral parameters and $F_0$ than conventional minimum generation error
training algorithm regardless its hyper-parameter settings. Furthermore, we
investigated the effect of the divergence of various GANs, and found that a
Wasserstein GAN minimizing the Earth-Mover's distance works the best in terms
of improving synthetic speech quality.
",Generative Adversarial Networks for Speech Synthesis,generative adversarial networks for speech synthesis
"  This work presents an introduction to feature-based time-series analysis. The
time series as a data type is first described, along with an overview of the
interdisciplinary time-series analysis literature. I then summarize the range
of feature-based representations for time series that have been developed to
aid interpretable insights into time-series structure. Particular emphasis is
given to emerging research that facilitates wide comparison of feature-based
representations that allow us to understand the properties of a time-series
dataset that make it suited to a particular feature-based representation or
analysis algorithm. The future of time-series analysis is likely to embrace
approaches that exploit machine learning methods to partially automate human
learning to aid understanding of the complex dynamical patterns in the time
series we measure from the world.
",Feature-Based Time Series Representations,feature-based time-series analysis
"  We analyse multimodal time-series data corresponding to weight, sleep and
steps measurements. We focus on predicting whether a user will successfully
achieve his/her weight objective. For this, we design several deep long
short-term memory (LSTM) architectures, including a novel cross-modal LSTM
(X-LSTM), and demonstrate their superiority over baseline approaches. The
X-LSTM improves parameter efficiency by processing each modality separately and
allowing for information flow between them by way of recurrent
cross-connections. We present a general hyperparameter optimisation technique
for X-LSTMs, which allows us to significantly improve on the LSTM and a prior
state-of-the-art cross-modal approach, using a comparable number of parameters.
Finally, we visualise the model's predictions, revealing implications about
latent variables in this task.
",Multimodal Time-Series Analysis for Weight Loss Prediction,predictive analytics for weight loss
"  This work studies the problem of stochastic dynamic filtering and state
propagation with complex beliefs. The main contribution is GP-SUM, a filtering
algorithm tailored to dynamic systems and observation models expressed as
Gaussian Processes (GP), and to states represented as a weighted sum of
Gaussians. The key attribute of GP-SUM is that it does not rely on
linearizations of the dynamic or observation models, or on unimodal Gaussian
approximations of the belief, hence enables tracking complex state
distributions. The algorithm can be seen as a combination of a sampling-based
filter with a probabilistic Bayes filter. On the one hand, GP-SUM operates by
sampling the state distribution and propagating each sample through the dynamic
system and observation models. On the other hand, it achieves effective
sampling and accurate probabilistic propagation by relying on the GP form of
the system, and the sum-of-Gaussian form of the belief. We show that GP-SUM
outperforms several GP-Bayes and Particle Filters on a standard benchmark. We
also demonstrate its use in a pushing task, predicting with experimental
accuracy the naturally occurring non-Gaussian distributions.
",Gaussian Process-based State Estimation,stochastic dynamic filtering and state propagation
"  Self-supervised learning (SSL) is a reliable learning mechanism in which a
robot enhances its perceptual capabilities. Typically, in SSL a trusted,
primary sensor cue provides supervised training data to a secondary sensor cue.
In this article, a theoretical analysis is performed on the fusion of the
primary and secondary cue in a minimal model of SSL. A proof is provided that
determines the specific conditions under which it is favorable to perform
fusion. In short, it is favorable when (i) the prior on the target value is
strong or (ii) the secondary cue is sufficiently accurate. The theoretical
findings are validated with computational experiments. Subsequently, a
real-world case study is performed to investigate if fusion in SSL is also
beneficial when assumptions of the minimal model are not met. In particular, a
flying robot learns to map pressure measurements to sonar height measurements
and then fuses the two, resulting in better height estimation. Fusion is also
beneficial in the opposite case, when pressure is the primary cue. The analysis
and results are encouraging to study SSL fusion also for other robots and
sensors.
",Fusion in Self-Supervised Learning,self-supervised learning in robotics
"  A zonal function (ZF) network on the $q$ dimensional sphere $\mathbb{S}^q$ is
a network of the form $\mathbf{x}\mapsto \sum_{k=1}^n
a_k\phi(\mathbf{x}\cdot\mathbf{x}_k)$ where $\phi :[-1,1]\to\mathbf{R}$ is the
activation function, $\mathbf{x}_k\in\mathbb{S}^q$ are the centers, and
$a_k\in\mathbb{R}$. While the approximation properties of such networks are
well studied in the context of positive definite activation functions, recent
interest in deep and shallow networks motivate the study of activation
functions of the form $\phi(t)=|t|$, which are not positive definite. In this
paper, we define an appropriate smoothess class and establish approximation
properties of such networks for functions in this class. The centers can be
chosen independently of the target function, and the coefficients are linear
combinations of the training data. The constructions preserve rotational
symmetries.
",Neural Networks on Spheres,machine learning
"  Transfer learning significantly accelerates the reinforcement learning
process by exploiting relevant knowledge from previous experiences. The problem
of optimally selecting source policies during the learning process is of great
importance yet challenging. There has been little theoretical analysis of this
problem. In this paper, we develop an optimal online method to select source
policies for reinforcement learning. This method formulates online source
policy selection as a multi-armed bandit problem and augments Q-learning with
policy reuse. We provide theoretical guarantees of the optimal selection
process and convergence to the optimal policy. In addition, we conduct
experiments on a grid-based robot navigation domain to demonstrate its
efficiency and robustness by comparing to the state-of-the-art transfer
learning method.
",Optimal Source Policy Selection in Transfer Reinforcement Learning,reinforcement learning
"  Digital image correlation (DIC) is a well-established, non-invasive technique
for tracking and quantifying the deformation of mechanical samples under
strain. While it provides an obvious way to observe incremental and aggregate
displacement information, it seems likely that DIC data sets, which after all
reflect the spatially-resolved response of a microstructure to loads, contain
much richer information than has generally been extracted from them. In this
paper, we demonstrate a machine-learning approach to quantifying the prior
deformation history of a crystalline sample based on its response to a
subsequent DIC test. This prior deformation history is encoded in the
microstructure through the inhomogeneity of the dislocation microstructure, and
in the spatial correlations of the dislocation patterns, which mediate the
system's response to the DIC test load. Our domain consists of deformed
crystalline thin films generated by a discrete dislocation plasticity
simulation. We explore the range of applicability of machine learning (ML) for
typical experimental protocols, and as a function of possible size effects and
stochasticity. Plasticity size effects may directly influence the data,
rendering unsupervised techniques unable to distinguish different plasticity
regimes.
",Machine Learning for Digital Image Correlation Data Analysis,machine learning in digital image correlation
"  The continually increasing number of documents produced each year
necessitates ever improving information processing methods for searching,
retrieving, and organizing text. Central to these information processing
methods is document classification, which has become an important application
for supervised learning. Recently the performance of these traditional
classifiers has degraded as the number of documents has increased. This is
because along with this growth in the number of documents has come an increase
in the number of categories. This paper approaches this problem differently
from current document classification methods that view the problem as
multi-class classification. Instead we perform hierarchical classification
using an approach we call Hierarchical Deep Learning for Text classification
(HDLTex). HDLTex employs stacks of deep learning architectures to provide
specialized understanding at each level of the document hierarchy.
",Hierarchical Text Classification,document classification
"  We introduce Graph-Structured Sum-Product Networks (GraphSPNs), a
probabilistic approach to structured prediction for problems where dependencies
between latent variables are expressed in terms of arbitrary, dynamic graphs.
While many approaches to structured prediction place strict constraints on the
interactions between inferred variables, many real-world problems can be only
characterized using complex graph structures of varying size, often
contaminated with noise when obtained from real data. Here, we focus on one
such problem in the domain of robotics. We demonstrate how GraphSPNs can be
used to bolster inference about semantic, conceptual place descriptions using
noisy topological relations discovered by a robot exploring large-scale office
spaces. Through experiments, we show that GraphSPNs consistently outperform the
traditional approach based on undirected graphical models, successfully
disambiguating information in global semantic maps built from uncertain, noisy
local evidence. We further exploit the probabilistic nature of the model to
infer marginal distributions over semantic descriptions of as yet unexplored
places and detect spatial environment configurations that are novel and
incongruent with the known evidence.
",Structured Prediction in Robotics,structured prediction with graph-structured sum-product networks
"  We present a robust multi-robot convoying approach that relies on visual
detection of the leading agent, thus enabling target following in unstructured
3-D environments. Our method is based on the idea of tracking-by-detection,
which interleaves efficient model-based object detection with temporal
filtering of image-based bounding box estimation. This approach has the
important advantage of mitigating tracking drift (i.e. drifting away from the
target object), which is a common symptom of model-free trackers and is
detrimental to sustained convoying in practice. To illustrate our solution, we
collected extensive footage of an underwater robot in ocean settings, and
hand-annotated its location in each frame. Based on this dataset, we present an
empirical comparison of multiple tracker variants, including the use of several
convolutional neural networks, both with and without recurrent connections, as
well as frequency-based model-free trackers. We also demonstrate the
practicality of this tracking-by-detection strategy in real-world scenarios by
successfully controlling a legged underwater robot in five degrees of freedom
to follow another robot's independent motion.
",Visual Object Tracking in Robotics,multi-robot convoying
"  Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.
",Context-Aware CNN Filters for NLP,contextual convolutional neural networks for natural language processing
"  Graph based semi-supervised learning (GSSL) has intuitive representation and
can be improved by exploiting the matrix calculation. However, it has to
perform iterative optimization to achieve a preset objective, which usually
leads to low efficiency. Another inconvenience lying in GSSL is that when new
data come, the graph construction and the optimization have to be conducted all
over again. We propose a sound assumption, arguing that: the neighboring data
points are not in peer-to-peer relation, but in a partial-ordered relation
induced by the local density and distance between the data; and the label of a
center can be regarded as the contribution of its followers. Starting from the
assumption, we develop a highly efficient non-iterative label propagation
algorithm based on a novel data structure named as optimal leading forest
(LaPOLeaF). The major weaknesses of the traditional GSSL are addressed by this
study. We further scale LaPOLeaF to accommodate big data by utilizing block
distance matrix technique, parallel computing, and Locality-Sensitive Hashing
(LSH). Experiments on large datasets have shown the promising results of the
proposed methods.
",Efficient Label Propagation in Graph-Based Semi-Supervised Learning,graph-based semi-supervised learning
"  In this paper we focus on developing a control algorithm for multi-terrain
tracked robots with flippers using a reinforcement learning (RL) approach. The
work is based on the deep deterministic policy gradient (DDPG) algorithm,
proven to be very successful in simple simulation environments. The algorithm
works in an end-to-end fashion in order to control the continuous position of
the flippers. This end-to-end approach makes it easy to apply the controller to
a wide array of circumstances, but the huge flexibility comes to the cost of an
increased difficulty of solution. The complexity of the task is enlarged even
more by the fact that real multi-terrain robots move in partially observable
environments. Notwithstanding these complications, being able to smoothly
control a multi-terrain robot can produce huge benefits in impaired people
daily lives or in search and rescue situations.
",Robot Flipper Control using Reinforcement Learning,multi-terrain tracked robot control with reinforcement learning
"  In this paper, we use the house price data ranging from January 2004 to
October 2016 to predict the average house price of November and December in
2016 for each district in Beijing, Shanghai, Guangzhou and Shenzhen. We apply
Autoregressive Integrated Moving Average model to generate the baseline while
LSTM networks to build prediction model. These algorithms are compared in terms
of Mean Squared Error. The result shows that the LSTM model has excellent
properties with respect to predict time series. Also, stateful LSTM networks
and stack LSTM networks are employed to further study the improvement of
accuracy of the house prediction model.
",House Price Prediction Models,time series forecasting
"  In this paper, I will introduce a fast and novel clustering algorithm based
on Gaussian distribution and it can guarantee the separation of each cluster
centroid as a given parameter, $d_s$. The worst run time complexity of this
algorithm is approximately $\sim$O$(T\times N \times \log(N))$ where $T$ is the
iteration steps and $N$ is the number of features.
",Gaussian Clustering Algorithm,clustering algorithms
"  In this work, we propose an end-to-end deep architecture that jointly learns
to detect obstacles and estimate their depth for MAV flight applications. Most
of the existing approaches either rely on Visual SLAM systems or on depth
estimation models to build 3D maps and detect obstacles. However, for the task
of avoiding obstacles this level of complexity is not required. Recent works
have proposed multi task architectures to both perform scene understanding and
depth estimation. We follow their track and propose a specific architecture to
jointly estimate depth and obstacles, without the need to compute a global map,
but maintaining compatibility with a global SLAM system if needed. The network
architecture is devised to exploit the joint information of the obstacle
detection task, that produces more reliable bounding boxes, with the depth
estimation one, increasing the robustness of both to scenario changes. We call
this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
experiments on sequences with different appearance and focal lengths and
compare it to SotA multi task methods that jointly perform semantic
segmentation and depth estimation. In addition, we show the integration in a
full system using a set of simulated navigation experiments where a MAV
explores an unknown scenario and plans safe trajectories by using our detection
model.
",Obstacle Detection in MAV Flight,obstacle detection and depth estimation for autonomous aerial vehicles
"  We study the quantum synchronization between a pair of two-level systems
inside two coupled cavities. By using a digital-analog decomposition of the
master equation that rules the system dynamics, we show that this approach
leads to quantum synchronization between both two-level systems. Moreover, we
can identify in this digital-analog block decomposition the fundamental
elements of a quantum machine learning protocol, in which the agent and the
environment (learning units) interact through a mediating system, namely, the
register. If we can additionally equip this algorithm with a classical feedback
mechanism, which consists of projective measurements in the register,
reinitialization of the register state and local conditional operations on the
agent and environment subspace, a powerful and flexible quantum machine
learning protocol emerges. Indeed, numerical simulations show that this
protocol enhances the synchronization process, even when every subsystem
experience different loss/decoherence mechanisms, and give us the flexibility
to choose the synchronization state. Finally, we propose an implementation
based on current technologies in superconducting circuits.
",Quantum Synchronization in Coupled Cavities,quantum machine learning
"  Recurrent neural networks (RNNs) are a vital modeling technique that rely on
internal states learned indirectly by optimization of a supervised,
unsupervised, or reinforcement training loss. RNNs are used to model dynamic
processes that are characterized by underlying latent states whose form is
often unknown, precluding its analytic representation inside an RNN. In the
Predictive-State Representation (PSR) literature, latent state processes are
modeled by an internal state representation that directly models the
distribution of future observations, and most recent work in this area has
relied on explicitly representing and targeting sufficient statistics of this
probability distribution. We seek to combine the advantages of RNNs and PSRs by
augmenting existing state-of-the-art recurrent neural networks with
Predictive-State Decoders (PSDs), which add supervision to the network's
internal state representation to target predicting future observations.
Predictive-State Decoders are simple to implement and easily incorporated into
existing training pipelines via additional loss regularization. We demonstrate
the effectiveness of PSDs with experimental results in three different domains:
probabilistic filtering, Imitation Learning, and Reinforcement Learning. In
each, our method improves statistical performance of state-of-the-art recurrent
baselines and does so with fewer iterations and less data.
",Predictive-State Decoders for Recurrent Neural Networks,predictive-state decoders in recurrent neural networks
"  Learning, taking into account full distribution of the data, referred to as
generative, is not feasible with deep neural networks (DNNs) because they model
only the conditional distribution of the outputs given the inputs. Current
solutions are either based on joint probability models facing difficult
estimation problems or learn two separate networks, mapping inputs to outputs
(recognition) and vice-versa (generation). We propose an intermediate approach.
First, we show that forward computation in DNNs with logistic sigmoid
activations corresponds to a simplified approximate Bayesian inference in a
directed probabilistic multi-layer model. This connection allows to interpret
DNN as a probabilistic model of the output and all hidden units given the
input. Second, we propose that in order for the recognition and generation
networks to be more consistent with the joint model of the data, weights of the
recognition and generator network should be related by transposition. We
demonstrate in a tentative experiment that such a coupled pair can be learned
generatively, modelling the full distribution of the data, and has enough
capacity to perform well in both recognition and generation.
",Deep Generative Modeling,generative deep learning
"  The lasso and elastic net linear regression models impose a
double-exponential prior distribution on the model parameters to achieve
regression shrinkage and variable selection, allowing the inference of robust
models from large data sets. However, there has been limited success in
deriving estimates for the full posterior distribution of regression
coefficients in these models, due to a need to evaluate analytically
intractable partition function integrals. Here, the Fourier transform is used
to express these integrals as complex-valued oscillatory integrals over
""regression frequencies"". This results in an analytic expansion and stationary
phase approximation for the partition functions of the Bayesian lasso and
elastic net, where the non-differentiability of the double-exponential prior
has so far eluded such an approach. Use of this approximation leads to highly
accurate numerical estimates for the expectation values and marginal posterior
distributions of the regression coefficients, and allows for Bayesian inference
of much higher dimensional models than previously possible.
",Bayesian Linear Regression,bayesian linear regression
"  A new prior is proposed for learning representations of high-level concepts
of the kind we manipulate with language. This prior can be combined with other
priors in order to help disentangling abstract factors from each other. It is
inspired by cognitive neuroscience theories of consciousness, seen as a
bottleneck through which just a few elements, after having been selected by
attention from a broader pool, are then broadcast and condition further
processing, both in perception and decision-making. The set of recently
selected elements one becomes aware of is seen as forming a low-dimensional
conscious state. This conscious state is combining the few concepts
constituting a conscious thought, i.e., what one is immediately conscious of at
a particular moment. We claim that this architectural and
information-processing constraint corresponds to assumptions about the joint
distribution between high-level concepts. To the extent that these assumptions
are generally true (and the form of natural language seems consistent with
them), they can form a useful prior for representation learning. A
low-dimensional thought or conscious state is analogous to a sentence: it
involves only a few variables and yet can make a statement with very high
probability of being true. This is consistent with a joint distribution (over
high-level concepts) which has the form of a sparse factor graph, i.e., where
the dependencies captured by each factor of the factor graph involve only very
few variables while creating a strong dip in the overall energy function. The
consciousness prior also makes it natural to map conscious states to natural
language utterances or to express classical AI knowledge in a form similar to
facts and rules, albeit capturing uncertainty as well as efficient search
mechanisms implemented by attention mechanisms.
","""Cognitive Neuroscience-Inspired Priors for Representation Learning""",consciousness and representation learning
"  Many real-world applications require automated data annotation, such as
identifying tissue origins based on gene expressions and classifying images
into semantic categories. Annotation classes are often numerous and subject to
changes over time, and annotating examples has become the major bottleneck for
supervised learning methods. In science and other high-value domains, large
repositories of data samples are often available, together with two sources of
organic supervision: a lexicon for the annotation classes, and text
descriptions that accompany some data samples. Distant supervision has emerged
as a promising paradigm for exploiting such indirect supervision by
automatically annotating examples where the text description contains a class
mention in the lexicon. However, due to linguistic variations and ambiguities,
such training data is inherently noisy, which limits the accuracy of this
approach. In this paper, we introduce an auxiliary natural language processing
system for the text modality, and incorporate co-training to reduce noise and
augment signal in distant supervision. Without using any manually labeled data,
our EZLearn system learned to accurately annotate data samples in functional
genomics and scientific figure comprehension, substantially outperforming
state-of-the-art supervised methods trained on tens of thousands of annotated
examples.
",Distant Supervision for Automated Data Annotation,distant supervision for automated data annotation
"  Daily operation of a large-scale experiment is a challenging task,
particularly from perspectives of routine monitoring of quality for data being
taken. We describe an approach that uses Machine Learning for the automated
system to monitor data quality, which is based on partial use of data qualified
manually by detector experts. The system automatically classifies marginal
cases: both of good an bad data, and use human expert decision to classify
remaining ""grey area"" cases.
  This study uses collision data collected by the CMS experiment at LHC in
2010. We demonstrate that proposed workflow is able to automatically process at
least 20\% of samples without noticeable degradation of the result.
",Automated Data Quality Monitoring,data quality monitoring
"  Automatically generating coherent and semantically meaningful text has many
applications in machine translation, dialogue systems, image captioning, etc.
Recently, by combining with policy gradient, Generative Adversarial Nets (GAN)
that use a discriminative model to guide the training of the generative model
as a reinforcement learning policy has shown promising results in text
generation. However, the scalar guiding signal is only available after the
entire text has been generated and lacks intermediate information about text
structure during the generative process. As such, it limits its success when
the length of the generated text samples is long (more than 20 words). In this
paper, we propose a new framework, called LeakGAN, to address the problem for
long text generation. We allow the discriminative net to leak its own
high-level extracted features to the generative net to further help the
guidance. The generator incorporates such informative signals into all
generation steps through an additional Manager module, which takes the
extracted features of current generated words and outputs a latent vector to
guide the Worker module for next-word generation. Our extensive experiments on
synthetic data and various real-world tasks with Turing test demonstrate that
LeakGAN is highly effective in long text generation and also improves the
performance in short text generation scenarios. More importantly, without any
supervision, LeakGAN would be able to implicitly learn sentence structures only
through the interaction between Manager and Worker.
",Long Text Generation with GANs,text generation
"  Recently proposed models which learn to write computer programs from data use
either input/output examples or rich execution traces. Instead, we argue that a
novel alternative is to use a glass-box loss function, given as a program
itself that can be directly inspected. Glass-box optimization covers a wide
range of problems, from computing the greatest common divisor of two integers,
to learning-to-learn problems.
  In this paper, we present an intelligent search system which learns, given
the partial program and the glass-box problem, the probabilities over the space
of programs. We empirically demonstrate that our informed search procedure
leads to significant improvements compared to brute-force program search, both
in terms of accuracy and time. For our experiments we use rich context free
grammars inspired by number theory, text processing, and algebra. Our results
show that (i) performing 4 rounds of our framework typically solves about 70%
of the target problems, (ii) our framework can improve itself even in domain
agnostic scenarios, and (iii) it can solve problems that would be otherwise too
slow to solve with brute-force search.
",Program Induction with Glass-Box Optimization,program learning and optimization
"  In this paper, we present the methodology and the results obtained by our
teams, dubbed Blue Man Group, in the ASSIN (from the Portuguese {\it
Avalia\c{c}\~ao de Similaridade Sem\^antica e Infer\^encia Textual})
competition, held at PROPOR 2016\footnote{International Conference on the
Computational Processing of the Portuguese Language -
http://propor2016.di.fc.ul.pt/}. Our team's strategy consisted of evaluating
methods based on semantic word vectors, following two distinct directions: 1)
to make use of low-dimensional, compact, feature sets, and 2) deep
learning-based strategies dealing with high-dimensional feature vectors.
Evaluation results demonstrated that the first strategy was more promising, so
that the results from the second strategy have been discarded. As a result, by
considering the best run of each of the six teams, we have been able to achieve
the best accuracy and F1 values in entailment recognition, in the Brazilian
Portuguese set, and the best F1 score overall. In the semantic similarity task,
our team was ranked second in the Brazilian Portuguese set, and third
considering both sets.
",Semantic Textual Inference,natural language processing
"  We study stochastic optimization of nonconvex loss functions, which are
typical objectives for training neural networks. We propose stochastic
approximation algorithms which optimize a series of regularized, nonlinearized
losses on large minibatches of samples, using only first-order gradient
information. Our algorithms provably converge to an approximate critical point
of the expected objective with faster rates than minibatch stochastic gradient
descent, and facilitate better parallelization by allowing larger minibatches.
",Stochastic Optimization of Nonconvex Loss Functions,stochastic optimization of nonconvex loss functions
"  In this paper, we analyze the behavior of the multivariate symmetric
uncertainty (MSU) measure through the use of statistical simulation techniques
under various mixes of informative and non-informative randomly generated
features. Experiments show how the number of attributes, their cardinalities,
and the sample size affect the MSU. We discovered a condition that preserves
good quality in the MSU under different combinations of these three factors,
providing a new useful criterion to help drive the process of dimension
reduction.
",Multivariate Symmetric Uncertainty Analysis,dimensionality reduction
"  This study proposes a Deep Belief Network model to classify traffic flow
states. The model is capable of processing massive, high-density, and
noise-contaminated data sets generated from smartphone sensors. The statistical
features of Vehicle acceleration, angular acceleration, and GPS speed data,
recorded by smartphone software, are analyzed, and then used as input for
traffic flow state classification. Data from a five-day experiment is used to
train and test the proposed model. A total of 747,856 sets of data are
generated and used for both traffic flow states classification and sensitivity
analysis of input variables. The result shows that the proposed Deep Belief
Network model is superior to traditional machine learning methods in both
classification performance and computational efficiency.
",Traffic Flow State Classification using Smartphone Sensors,traffic flow state classification using deep belief networks
"  A significant challenge in energy system cyber security is the current
inability to detect cyber-physical attacks targeting and originating from
distributed grid-edge devices such as photovoltaics (PV) panels, smart flexible
loads, and electric vehicles. We address this concern by designing and
developing a distributed, multi-modal anomaly detection approach that can sense
the health of the device and the electric power grid from the edge. This is
realized by exploiting unsupervised machine learning algorithms on multiple
sources of time-series data, fusing these multiple local observations and
flagging anomalies when a deviation from the normal behavior is observed.
  We particularly focus on the cyber-physical threats to the distributed PVs
that has the potential to cause local disturbances or grid instabilities by
creating supply-demand mismatch, reverse power flow conditions etc. We use an
open source power system simulation tool called GridLAB-D, loaded with real
smart home and solar datasets to simulate the smart grid scenarios and to
illustrate the impact of PV attacks on the power system. Various attacks
targeting PV panels that create voltage fluctuations, reverse power flow etc
were designed and performed. We observe that while individual unsupervised
learning algorithms such as OCSVMs, Corrupt RF and PCA surpasses in identifying
particular attack type, PCA with Convex Hull outperforms all algorithms in
identifying all designed attacks with a true positive rate of 83.64% and an
accuracy of 95.78%. Our key insight is that due to the heterogeneous nature of
the distribution grid and the uncertainty in the type of the attack being
launched, relying on single mode of information for defense can lead to
increased false alarms and missed detection rates as one can design attacks to
hide within those uncertainties and remain stealthy.
",Anomaly Detection in Smart Grid Systems,cyber-physical attacks on smart grids
"  Predictive models for music are studied by researchers of algorithmic
composition, the cognitive sciences and machine learning. They serve as base
models for composition, can simulate human prediction and provide a
multidisciplinary application domain for learning algorithms. A particularly
well established and constantly advanced subtask is the prediction of
monophonic melodies. As melodies typically involve non-Markovian dependencies
their prediction requires a capable learning algorithm. In this thesis, I apply
the recent feature discovery and learning method PULSE to the realm of symbolic
music modeling. PULSE is comprised of a feature generating operation and
L1-regularized optimization. These are used to iteratively expand and cull the
feature set, effectively exploring feature spaces that are too large for common
feature selection approaches. I design a general Python framework for PULSE,
propose task-optimized feature generating operations and various
music-theoretically motivated features that are evaluated on a standard corpus
of monophonic folk and chorale melodies. The proposed method significantly
outperforms comparable state-of-the-art models. I further discuss the free
parameters of the learning algorithm and analyze the feature composition of the
learned models. The models learned by PULSE afford an easy inspection and are
musicologically interpreted for the first time.
",Music Melody Prediction,music generation and prediction
"  Structured prediction is ubiquitous in applications of machine learning such
as knowledge extraction and natural language processing. Structure often can be
formulated in terms of logical constraints. We consider the question of how to
perform efficient active learning in the presence of logical constraints among
variables inferred by different classifiers. We propose several methods and
provide theoretical results that demonstrate the inappropriateness of employing
uncertainty guided sampling, a commonly used active learning method.
Furthermore, experiments on ten different datasets demonstrate that the methods
significantly outperform alternatives in practice. The results are of practical
significance in situations where labeled data is scarce.
",Active Learning with Logical Constraints,active learning with logical constraints
"  We propose Object-oriented Neural Programming (OONP), a framework for
semantically parsing documents in specific domains. Basically, OONP reads a
document and parses it into a predesigned object-oriented data structure
(referred to as ontology in this paper) that reflects the domain-specific
semantics of the document. An OONP parser models semantic parsing as a decision
process: a neural net-based Reader sequentially goes through the document, and
during the process it builds and updates an intermediate ontology to summarize
its partial understanding of the text it covers. OONP supports a rich family of
operations (both symbolic and differentiable) for composing the ontology, and a
big variety of forms (both symbolic and differentiable) for representing the
state and the document. An OONP parser can be trained with supervision of
different forms and strength, including supervised learning (SL) ,
reinforcement learning (RL) and hybrid of the two. Our experiments on both
synthetic and real-world document parsing tasks have shown that OONP can learn
to handle fairly complicated ontology with training data of modest sizes.
",Document Parsing Frameworks,object-oriented neural programming
"  We propose a new generative model of sentences that first samples a prototype
sentence from the training corpus and then edits it into a new sentence.
Compared to traditional models that generate from scratch either left-to-right
or by first sampling a latent sentence vector, our prototype-then-edit model
improves perplexity on language modeling and generates higher quality outputs
according to human evaluation. Furthermore, the model gives rise to a latent
edit vector that captures interpretable semantics such as sentence similarity
and sentence-level analogies.
",Prototype-to-Edit Sentence Generation,language modeling
"  Since their invention, generative adversarial networks (GANs) have become a
popular approach for learning to model a distribution of real (unlabeled) data.
Convergence problems during training are overcome by Wasserstein GANs which
minimize the distance between the model and the empirical distribution in terms
of a different metric, but thereby introduce a Lipschitz constraint into the
optimization problem. A simple way to enforce the Lipschitz constraint on the
class of functions, which can be modeled by the neural network, is weight
clipping. It was proposed that training can be improved by instead augmenting
the loss by a regularization term that penalizes the deviation of the gradient
of the critic (as a function of the network's input) from one. We present
theoretical arguments why using a weaker regularization term enforcing the
Lipschitz constraint is preferable. These arguments are supported by
experimental results on toy data sets.
",Regularization Techniques for GANs,generative adversarial networks (gans)
"  We present a physically-inspired model and an efficient algorithm to infer
hierarchical rankings of nodes in directed networks. It assigns real-valued
ranks to nodes rather than simply ordinal ranks, and it formalizes the
assumption that interactions are more likely to occur between individuals with
similar ranks. It provides a natural statistical significance test for the
inferred hierarchy, and it can be used to perform inference tasks such as
predicting the existence or direction of edges. The ranking is obtained by
solving a linear system of equations, which is sparse if the network is; thus
the resulting algorithm is extremely efficient and scalable. We illustrate
these findings by analyzing real and synthetic data, including datasets from
animal behavior, faculty hiring, social support networks, and sports
tournaments. We show that our method often outperforms a variety of others, in
both speed and accuracy, in recovering the underlying ranks and predicting edge
directions.
",Directed Network Ranking,network node ranking
"  Auto-encoding is an important task which is typically realized by deep neural
networks (DNNs) such as convolutional neural networks (CNN). In this paper, we
propose EncoderForest (abbrv. eForest), the first tree ensemble based
auto-encoder. We present a procedure for enabling forests to do backward
reconstruction by utilizing the equivalent classes defined by decision paths of
the trees, and demonstrate its usage in both supervised and unsupervised
setting. Experiments show that, compared with DNN autoencoders, eForest is able
to obtain lower reconstruction error with fast training speed, while the model
itself is reusable and damage-tolerable.
",Tree-Based Autoencoders,tree-based auto-encoders
"  The OpenAI Gym provides researchers and enthusiasts with simple to use
environments for reinforcement learning. Even the simplest environment have a
level of complexity that can obfuscate the inner workings of RL approaches and
make debugging difficult. This whitepaper describes a Python framework that
makes it very easy to create simple Markov-Decision-Process environments
programmatically by specifying state transitions and rewards of deterministic
and non-deterministic MDPs in a domain-specific language in Python. It then
presents results and visualizations created with this MDP framework.
",MDP Framework for Reinforcement Learning,reinforcement learning
"  Deep neural networks (NN) are extensively used for machine learning tasks
such as image classification, perception and control of autonomous systems.
Increasingly, these deep NNs are also been deployed in high-assurance
applications. Thus, there is a pressing need for developing techniques to
verify neural networks to check whether certain user-expected properties are
satisfied. In this paper, we study a specific verification problem of computing
a guaranteed range for the output of a deep neural network given a set of
inputs represented as a convex polyhedron. Range estimation is a key primitive
for verifying deep NNs. We present an efficient range estimation algorithm that
uses a combination of local search and linear programming problems to
efficiently find the maximum and minimum values taken by the outputs of the NN
over the given input set. In contrast to recently proposed ""monolithic""
optimization approaches, we use local gradient descent to repeatedly find and
eliminate local minima of the function. The final global optimum is certified
using a mixed integer programming instance. We implement our approach and
compare it with Reluplex, a recently proposed solver for deep neural networks.
We demonstrate the effectiveness of the proposed approach for verification of
NNs used in automated control as well as those used in classification.
",Neural Network Verification,verification of deep neural networks
"  Deep neural networks continue to show improved performance with increasing
depth, an encouraging trend that implies an explosion in the possible
permutations of network architectures and hyperparameters for which there is
little intuitive guidance. To address this increasing complexity, we propose
Evolutionary DEep Networks (EDEN), a computationally efficient
neuro-evolutionary algorithm which interfaces to any deep neural network
platform, such as TensorFlow. We show that EDEN evolves simple yet successful
architectures built from embedding, 1D and 2D convolutional, max pooling and
fully connected layers along with their hyperparameters. Evaluation of EDEN
across seven image and sentiment classification datasets shows that it reliably
finds good networks -- and in three cases achieves state-of-the-art results --
even on a single GPU, in just 6-24 hours. Our study provides a first attempt at
applying neuro-evolution to the creation of 1D convolutional networks for
sentiment analysis including the optimisation of the embedding layer.
",Neuro-Evolutionary Optimization in Deep Learning,neuro-evolutionary computation
"  With the advancement of treatment modalities in radiation therapy for cancer
patients, outcomes have improved, but at the cost of increased treatment plan
complexity and planning time. The accurate prediction of dose distributions
would alleviate this issue by guiding clinical plan optimization to save time
and maintain high quality plans. We have modified a convolutional deep network
model, U-net (originally designed for segmentation purposes), for predicting
dose from patient image contours of the planning target volume (PTV) and organs
at risk (OAR). We show that, as an example, we are able to accurately predict
the dose of intensity-modulated radiation therapy (IMRT) for prostate cancer
patients, where the average Dice similarity coefficient is 0.91 when comparing
the predicted vs. true isodose volumes between 0% and 100% of the prescription
dose. The average value of the absolute differences in [max, mean] dose is
found to be under 5% of the prescription dose, specifically for each structure
is [1.80%, 1.03%](PTV), [1.94%, 4.22%](Bladder), [1.80%, 0.48%](Body), [3.87%,
1.79%](L Femoral Head), [5.07%, 2.55%](R Femoral Head), and [1.26%,
1.62%](Rectum) of the prescription dose. We thus managed to map a desired
radiation dose distribution from a patient's PTV and OAR contours. As an
additional advantage, relatively little data was used in the techniques and
models described in this paper.
",Deep Learning for Radiation Therapy Dose Prediction,radiation therapy planning and dose prediction
"  This paper introduces a novel real-time Fuzzy Supervised Learning with Binary
Meta-Feature (FSL-BM) for big data classification task. The study of real-time
algorithms addresses several major concerns, which are namely: accuracy, memory
consumption, and ability to stretch assumptions and time complexity. Attaining
a fast computational model providing fuzzy logic and supervised learning is one
of the main challenges in the machine learning. In this research paper, we
present FSL-BM algorithm as an efficient solution of supervised learning with
fuzzy logic processing using binary meta-feature representation using Hamming
Distance and Hash function to relax assumptions. While many studies focused on
reducing time complexity and increasing accuracy during the last decade, the
novel contribution of this proposed solution comes through integration of
Hamming Distance, Hash function, binary meta-features, binary classification to
provide real time supervised method. Hash Tables (HT) component gives a fast
access to existing indices; and therefore, the generation of new indices in a
constant time complexity, which supersedes existing fuzzy supervised algorithms
with better or comparable results. To summarize, the main contribution of this
technique for real-time Fuzzy Supervised Learning is to represent hypothesis
through binary input as meta-feature space and creating the Fuzzy Supervised
Hash table to train and validate model.
",Fuzzy Supervised Learning for Real-Time Big Data Classification,fuzzy supervised learning with binary meta-features for big data classification
"  Policy-gradient approaches to reinforcement learning have two common and
undesirable overhead procedures, namely warm-start training and sample variance
reduction. In this paper, we describe a reinforcement learning method based on
a softmax value function that requires neither of these procedures. Our method
combines the advantages of policy-gradient methods with the efficiency and
simplicity of maximum-likelihood approaches. We apply this new cold-start
reinforcement learning method in training sequence generation models for
structured output prediction problems. Empirical evidence validates this method
on automatic summarization and image captioning tasks.
",Reinforcement Learning Optimizations,reinforcement learning
"  Data parallelism has emerged as a necessary technique to accelerate the
training of deep neural networks (DNN). In a typical data parallelism approach,
the local workers push the latest updates of all the parameters to the
parameter server and pull all merged parameters back periodically. However,
with the increasing size of DNN models and the large number of workers in
practice, this typical data parallelism cannot achieve satisfactory training
acceleration, since it usually suffers from the heavy communication cost due to
transferring huge amount of information between workers and the parameter
server. In-depth understanding on DNN has revealed that it is usually highly
redundant, that deleting a considerable proportion of the parameters will not
significantly decline the model performance. This redundancy property exposes a
great opportunity to reduce the communication cost by only transferring the
information of those significant parameters during the parallel training.
However, if we only transfer information of temporally significant parameters
of the latest snapshot, we may miss the parameters that are insignificant now
but have potential to become significant as the training process goes on. To
this end, we design an Explore-Exploit framework to dynamically choose the
subset to be communicated, which is comprised of the significant parameters in
the latest snapshot together with a random explored set of other parameters. We
propose to measure the significance of the parameter by the combination of its
magnitude and gradient. Our experimental results demonstrate that our proposed
Slim-DP can achieve better training acceleration than standard data parallelism
and its communication-efficient version by saving communication time without
loss of accuracy.
",Communication-Efficient Data Parallelism for Deep Neural Networks,efficient data parallelism for deep neural network training
"  In the research area of reinforcement learning (RL), frequently novel and
promising methods are developed and introduced to the RL community. However,
although many researchers are keen to apply their methods on real-world
problems, implementing such methods in real industry environments often is a
frustrating and tedious process. Generally, academic research groups have only
limited access to real industrial data and applications. For this reason, new
methods are usually developed, evaluated and compared by using artificial
software benchmarks. On one hand, these benchmarks are designed to provide
interpretable RL training scenarios and detailed insight into the learning
process of the method on hand. On the other hand, they usually do not share
much similarity with industrial real-world applications. For this reason we
used our industry experience to design a benchmark which bridges the gap
between freely available, documented, and motivated artificial benchmarks and
properties of real industrial problems. The resulting industrial benchmark (IB)
has been made publicly available to the RL community by publishing its Java and
Python code, including an OpenAI Gym wrapper, on Github. In this paper we
motivate and describe in detail the IB's dynamics and identify prototypic
experimental settings that capture common situations in real-world industry
control problems.
",Industrial Benchmarking for Reinforcement Learning,reinforcement learning in industry
"  In this research, we propose a deep learning based approach for speeding up
the topology optimization methods. The problem we seek to solve is the layout
problem. The main novelty of this work is to state the problem as an image
segmentation task. We leverage the power of deep learning methods as the
efficient pixel-wise image labeling technique to perform the topology
optimization. We introduce convolutional encoder-decoder architecture and the
overall approach of solving the above-described problem with high performance.
The conducted experiments demonstrate the significant acceleration of the
optimization process. The proposed approach has excellent generalization
properties. We demonstrate the ability of the application of the proposed model
to other problems. The successful results, as well as the drawbacks of the
current method, are discussed.
",Deep Learning for Topology Optimization,deep learning for topology optimization
"  While much of the work in the design of convolutional networks over the last
five years has revolved around the empirical investigation of the importance of
depth, filter sizes, and number of feature channels, recent studies have shown
that branching, i.e., splitting the computation along parallel but distinct
threads and then aggregating their outputs, represents a new promising
dimension for significant improvements in performance. To combat the complexity
of design choices in multi-branch architectures, prior work has adopted simple
strategies, such as a fixed branching factor, the same input being fed to all
parallel branches, and an additive combination of the outputs produced by all
branches at aggregation points.
  In this work we remove these predefined choices and propose an algorithm to
learn the connections between branches in the network. Instead of being chosen
a priori by the human designer, the multi-branch connectivity is learned
simultaneously with the weights of the network by optimizing a single loss
function defined with respect to the end task. We demonstrate our approach on
the problem of multi-class image classification using three different datasets
where it yields consistently higher accuracy compared to the state-of-the-art
""ResNeXt"" multi-branch network given the same learning capacity.
",Learning Multi-Branch Connectivity in Convolutional Networks,learning multi-branch connectivity in convolutional neural networks
"  Batch Normalization (BN) has proven to be an effective algorithm for deep
neural network training by normalizing the input to each neuron and reducing
the internal covariate shift. The space of weight vectors in the BN layer can
be naturally interpreted as a Riemannian manifold, which is invariant to linear
scaling of weights. Following the intrinsic geometry of this manifold provides
a new learning rule that is more efficient and easier to analyze. We also
propose intuitive and effective gradient clipping and regularization methods
for the proposed algorithm by utilizing the geometry of the manifold. The
resulting algorithm consistently outperforms the original BN on various types
of network architectures and datasets.
",Riemannian Geometry in Batch Normalization,batch normalization
"  This paper is concerned with the problem of representing and learning a
linear transformation using a linear neural network. In recent years, there has
been a growing interest in the study of such networks in part due to the
successes of deep learning. The main question of this body of research and also
of this paper pertains to the existence and optimality properties of the
critical points of the mean-squared loss function. The primary concern here is
the robustness of the critical points with regularization of the loss function.
An optimal control model is introduced for this purpose and a learning
algorithm (regularized form of backprop) derived for the same using the
Hamilton's formulation of optimal control. The formulation is used to provide a
complete characterization of the critical points in terms of the solutions of a
nonlinear matrix-valued equation, referred to as the characteristic equation.
Analytical and numerical tools from bifurcation theory are used to compute the
critical points via the solutions of the characteristic equation. The main
conclusion is that the critical point diagram can be fundamentally different
even with arbitrary small amounts of regularization.
",Critical Points in Linear Neural Networks,linear neural networks
"  We consider the problem of aggregating pairwise comparisons to obtain a
consensus ranking order over a collection of objects. We use the popular
Bradley-Terry-Luce (BTL) model which allows us to probabilistically describe
pairwise comparisons between objects. In particular, we employ the Bayesian BTL
model which allows for meaningful prior assumptions and to cope with situations
where the number of objects is large and the number of comparisons between some
objects is small or even zero. For the conventional Bayesian BTL model, we
derive information-theoretic lower bounds on the Bayes risk of estimators for
norm-based distortion functions. We compare the information-theoretic lower
bound with the Bayesian Cram\'{e}r-Rao lower bound we derive for the case when
the Bayes risk is the mean squared error. We illustrate the utility of the
bounds through simulations by comparing them with the error performance of an
expectation-maximization based inference algorithm proposed for the Bayesian
BTL model. We draw parallels between pairwise comparisons in the BTL model and
inter-player games represented as edges in a comparison graph and analyze the
effect of various graph structures on the lower bounds. We also extend the
information-theoretic and Bayesian Cram\'{e}r-Rao lower bounds to the more
general Bayesian BTL model which takes into account home-field advantage.
",Bayesian Inference for Ranking Models,bayesian methods for pairwise comparison aggregation
"  Previous studies have demonstrated the empirical success of word embeddings
in various applications. In this paper, we investigate the problem of learning
distributed representations for text documents which many machine learning
algorithms take as input for a number of NLP tasks.
  We propose a neural network model, KeyVec, which learns document
representations with the goal of preserving key semantics of the input text. It
enables the learned low-dimensional vectors to retain the topics and important
information from the documents that will flow to downstream tasks. Our
empirical evaluations show the superior quality of KeyVec representations in
two different document understanding tasks.
",Document Embeddings,word embeddings and document representations
"  In this work, we study how to use sampling to speed up mechanisms for
answering adaptive queries into datasets without reducing the accuracy of those
mechanisms. This is important to do when both the datasets and the number of
queries asked are very large. In particular, we describe a mechanism that
provides a polynomial speed-up per query over previous mechanisms, without
needing to increase the total amount of data required to maintain the same
generalization error as before. We prove that this speed-up holds for arbitrary
statistical queries. We also provide an even faster method for achieving
statistically-meaningful responses wherein the mechanism is only allowed to see
a constant number of samples from the data per query. Finally, we show that our
general results yield a simple, fast, and unified approach for adaptively
optimizing convex and strongly convex functions over a dataset.
",Adaptive Query Processing,sampling methods for accelerating adaptive query mechanisms
"  Generative Adversarial Networks (GANs) have shown impressive performance in
generating photo-realistic images. They fit generative models by minimizing
certain distance measure between the real image distribution and the generated
data distribution. Several distance measures have been used, such as
Jensen-Shannon divergence, $f$-divergence, and Wasserstein distance, and
choosing an appropriate distance measure is very important for training the
generative network. In this paper, we choose to use the maximum mean
discrepancy (MMD) as the distance metric, which has several nice theoretical
guarantees. In fact, generative moment matching network (GMMN) (Li, Swersky,
and Zemel 2015) is such a generative model which contains only one generator
network $G$ trained by directly minimizing MMD between the real and generated
distributions. However, it fails to generate meaningful samples on challenging
benchmark datasets, such as CIFAR-10 and LSUN. To improve on GMMN, we propose
to add an extra network $F$, called mapper. $F$ maps both real data
distribution and generated data distribution from the original data space to a
feature representation space $\mathcal{R}$, and it is trained to maximize MMD
between the two mapped distributions in $\mathcal{R}$, while the generator $G$
tries to minimize the MMD. We call the new model generative adversarial mapping
networks (GAMNs). We demonstrate that the adversarial mapper $F$ can help $G$
to better capture the underlying data distribution. We also show that GAMN
significantly outperforms GMMN, and is also superior to or comparable with
other state-of-the-art GAN based methods on MNIST, CIFAR-10 and LSUN-Bedrooms
datasets.
",Generative Adversarial Networks for Image Generation,generative adversarial networks
"  The reliable measurement of confidence in classifiers' predictions is very
important for many applications and is, therefore, an important part of
classifier design. Yet, although deep learning has received tremendous
attention in recent years, not much progress has been made in quantifying the
prediction confidence of neural network classifiers. Bayesian models offer a
mathematically grounded framework to reason about model uncertainty, but
usually come with prohibitive computational costs. In this paper we propose a
simple, scalable method to achieve a reliable confidence score, based on the
data embedding derived from the penultimate layer of the network. We
investigate two ways to achieve desirable embeddings, by using either a
distance-based loss or Adversarial Training. We then test the benefits of our
method when used for classification error prediction, weighting an ensemble of
classifiers, and novelty detection. In all tasks we show significant
improvement over traditional, commonly used confidence scores.
",Measuring Prediction Confidence in Neural Network Classifiers,confidence estimation in neural network classifiers
"  We propose a theoretical framework for thinking about score normalization,
which confirms that normalization is not needed under (admittedly fragile)
ideal conditions. If, however, these conditions are not met, e.g. under
data-set shift between training and runtime, our theory reveals dependencies
between scores that could be exploited by strategies such as score
normalization. Indeed, it has been demonstrated over and over experimentally,
that various ad-hoc score normalization recipes do work. We present a first
attempt at using probability theory to design a generative score-space
normalization model which gives similar improvements to ZT-norm on the
text-dependent RSR 2015 database.
",Score Normalization Theory,score normalization in machine learning
"  We report on an extensive study of the benefits and limitations of current
deep learning approaches to object recognition in robot vision scenarios,
introducing a novel dataset used for our investigation. To avoid the biases in
currently available datasets, we consider a natural human-robot interaction
setting to design a data-acquisition protocol for visual object recognition on
the iCub humanoid robot. Analyzing the performance of off-the-shelf models
trained off-line on large-scale image retrieval datasets, we show the necessity
for knowledge transfer. We evaluate different ways in which this last step can
be done, and identify the major bottlenecks affecting robotic scenarios. By
studying both object categorization and identification problems, we highlight
key differences between object recognition in robotics applications and in
image retrieval tasks, for which the considered deep learning approaches have
been originally designed. In a nutshell, our results confirm the remarkable
improvements yield by deep learning in this setting, while pointing to specific
open challenges that need be addressed for seamless deployment in robotics.
",Object Recognition in Robotics,deep learning for object recognition in robot vision
"  This paper focuses on an examination of an applicability of Recurrent Neural
Network models for detecting anomalous behavior of the CERN superconducting
magnets. In order to conduct the experiments, the authors designed and
implemented an adaptive signal quantization algorithm and a custom GRU-based
detector and developed a method for the detector parameters selection. Three
different datasets were used for testing the detector. Two artificially
generated datasets were used to assess the raw performance of the system
whereas the 231 MB dataset composed of the signals acquired from HiLumi magnets
was intended for real-life experiments and model training. Several different
setups of the developed anomaly detection system were evaluated and compared
with state-of-the-art OC-SVM reference model operating on the same data. The
OC-SVM model was equipped with a rich set of feature extractors accounting for
a range of the input signal properties. It was determined in the course of the
experiments that the detector, along with its supporting design methodology,
reaches F1 equal or very close to 1 for almost all test sets. Due to the
profile of the data, the best_length setup of the detector turned out to
perform the best among all five tested configuration schemes of the detection
system. The quantization parameters have the biggest impact on the overall
performance of the detector with the best values of input/output grid equal to
16 and 8, respectively. The proposed solution of the detection significantly
outperformed OC-SVM-based detector in most of the cases, with much more stable
performance across all the datasets.
",Anomaly Detection in High-Energy Physics,anomaly detection in cern superconducting magnets using recurrent neural networks
"  Traditional medicine typically applies one-size-fits-all treatment for the
entire patient population whereas precision medicine develops tailored
treatment schemes for different patient subgroups. The fact that some factors
may be more significant for a specific patient subgroup motivates clinicians
and medical researchers to develop new approaches to subgroup detection and
analysis, which is an effective strategy to personalize treatment. In this
study, we propose a novel patient subgroup detection method, called Supervised
Biclustring (SUBIC) using convex optimization and apply our approach to detect
patient subgroups and prioritize risk factors for hypertension (HTN) in a
vulnerable demographic subgroup (African-American). Our approach not only finds
patient subgroups with guidance of a clinically relevant target variable but
also identifies and prioritizes risk factors by pursuing sparsity of the input
variables and encouraging similarity among the input variables and between the
input and target variables
",Personalized Medicine for Hypertension,precision medicine and patient subgroup detection
"  We propose a deep learning-based approach to the problem of premise
selection: selecting mathematical statements relevant for proving a given
conjecture. We represent a higher-order logic formula as a graph that is
invariant to variable renaming but still fully preserves syntactic and semantic
information. We then embed the graph into a vector via a novel embedding method
that preserves the information of edge ordering. Our approach achieves
state-of-the-art results on the HolStep dataset, improving the classification
accuracy from 83% to 90.3%.
",Mathematical Premise Selection,deep learning for premise selection
"  We present a novel method for exact hierarchical sparse polynomial
regression. Our regressor is that degree $r$ polynomial which depends on at
most $k$ inputs, counting at most $\ell$ monomial terms, which minimizes the
sum of the squares of its prediction errors. The previous hierarchical sparse
specification aligns well with modern big data settings where many inputs are
not relevant for prediction purposes and the functional complexity of the
regressor needs to be controlled as to avoid overfitting. We present a two-step
approach to this hierarchical sparse regression problem. First, we discard
irrelevant inputs using an extremely fast input ranking heuristic. Secondly, we
take advantage of modern cutting plane methods for integer optimization to
solve our resulting reduced hierarchical $(k, \ell)$-sparse problem exactly.
The ability of our method to identify all $k$ relevant inputs and all $\ell$
monomial terms is shown empirically to experience a phase transition.
Crucially, the same transition also presents itself in our ability to reject
all irrelevant features and monomials as well. In the regime where our method
is statistically powerful, its computational complexity is interestingly on par
with Lasso based heuristics. The presented work fills a void in terms of a lack
of powerful disciplined nonlinear sparse regression methods in high-dimensional
settings. Our method is shown empirically to scale to regression problems with
$n\approx 10,000$ observations for input dimension $p\approx 1,000$.
",Hierarchical Sparse Polynomial Regression,hierarchical sparse polynomial regression
"  Class imbalance problems manifest in domains such as financial fraud
detection or network intrusion analysis, where the prevalence of one class is
much higher than another. Typically, practitioners are more interested in
predicting the minority class than the majority class as the minority class may
carry a higher misclassification cost. However, classifier performance
deteriorates in the face of class imbalance as oftentimes classifiers may
predict every point as the majority class. Methods for dealing with class
imbalance include cost-sensitive learning or resampling techniques. In this
paper, we introduce DeepBalance, an ensemble of deep belief networks trained
with balanced bootstraps and random feature selection. We demonstrate that our
proposed method outperforms baseline resampling methods such as SMOTE and
under- and over-sampling in metrics such as AUC and sensitivity when applied to
a highly imbalanced financial transaction data. Additionally, we explore
performance and training time implications of various model parameters.
Furthermore, we show that our model is easily parallelizable, which can reduce
training times. Finally, we present an implementation of DeepBalance in R.
",Class Imbalance Handling in Financial Fraud Detection,class imbalance problem
"  Developing a safe and efficient collision avoidance policy for multiple
robots is challenging in the decentralized scenarios where each robot generate
its paths without observing other robots' states and intents. While other
distributed multi-robot collision avoidance systems exist, they often require
extracting agent-level features to plan a local collision-free action, which
can be computationally prohibitive and not robust. More importantly, in
practice the performance of these methods are much lower than their centralized
counterparts.
  We present a decentralized sensor-level collision avoidance policy for
multi-robot systems, which directly maps raw sensor measurements to an agent's
steering commands in terms of movement velocity. As a first step toward
reducing the performance gap between decentralized and centralized methods, we
present a multi-scenario multi-stage training framework to find an optimal
policy which is trained over a large number of robots on rich, complex
environments simultaneously using a policy gradient based reinforcement
learning algorithm. We validate the learned sensor-level collision avoidance
policy in a variety of simulated scenarios with thorough performance
evaluations and show that the final learned policy is able to find time
efficient, collision-free paths for a large-scale robot system. We also
demonstrate that the learned policy can be well generalized to new scenarios
that do not appear in the entire training period, including navigating a
heterogeneous group of robots and a large-scale scenario with 100 robots.
Videos are available at https://sites.google.com/view/drlmaca
",Decentralized Multi-Robot Collision Avoidance,decentralized multi-robot collision avoidance policy
"  Dexterous multi-fingered hands are extremely versatile and provide a generic
way to perform a multitude of tasks in human-centric environments. However,
effectively controlling them remains challenging due to their high
dimensionality and large number of potential contacts. Deep reinforcement
learning (DRL) provides a model-agnostic approach to control complex dynamical
systems, but has not been shown to scale to high-dimensional dexterous
manipulation. Furthermore, deployment of DRL on physical systems remains
challenging due to sample inefficiency. Consequently, the success of DRL in
robotics has thus far been limited to simpler manipulators and tasks. In this
work, we show that model-free DRL can effectively scale up to complex
manipulation tasks with a high-dimensional 24-DoF hand, and solve them from
scratch in simulated experiments. Furthermore, with the use of a small number
of human demonstrations, the sample complexity can be significantly reduced,
which enables learning with sample sizes equivalent to a few hours of robot
experience. The use of demonstrations result in policies that exhibit very
natural movements and, surprisingly, are also substantially more robust.
",Dexterous Hand Control with Deep Reinforcement Learning,deep reinforcement learning for dexterous manipulation
"  Exploration in environments with sparse rewards has been a persistent problem
in reinforcement learning (RL). Many tasks are natural to specify with a sparse
reward, and manually shaping a reward function can result in suboptimal
performance. However, finding a non-zero reward is exponentially more difficult
with increasing task horizon or action dimensionality. This puts many
real-world tasks out of practical reach of RL methods. In this work, we use
demonstrations to overcome the exploration problem and successfully learn to
perform long-horizon, multi-step robotics tasks with continuous control such as
stacking blocks with a robot arm. Our method, which builds on top of Deep
Deterministic Policy Gradients and Hindsight Experience Replay, provides an
order of magnitude of speedup over RL on simulated robotics tasks. It is simple
to implement and makes only the additional assumption that we can collect a
small set of demonstrations. Furthermore, our method is able to solve tasks not
solvable by either RL or behavior cloning alone, and often ends up
outperforming the demonstrator policy.
",Reinforcement Learning with Sparse Rewards,reinforcement learning
"  In a cognitive radio network, a secondary user learns the spectrum
environment and dynamically accesses the channel where the primary user is
inactive. At the same time, a primary user emulation (PUE) attacker can send
falsified primary user signals and prevent the secondary user from utilizing
the available channel. The best attacking strategies that an attacker can apply
have not been well studied. In this paper, for the first time, we study optimal
PUE attack strategies by formulating an online learning problem where the
attacker needs to dynamically decide the attacking channel in each time slot
based on its attacking experience. The challenge in our problem is that since
the PUE attack happens in the spectrum sensing phase, the attacker cannot
observe the reward on the attacked channel. To address this challenge, we
utilize the attacker's observation capability. We propose online learning-based
attacking strategies based on the attacker's observation capabilities. Through
our analysis, we show that with no observation within the attacking slot, the
attacker loses on the regret order, and with the observation of at least one
channel, there is a significant improvement on the attacking performance.
Observation of multiple channels does not give additional benefit to the
attacker (only a constant scaling) though it gives insight on the number of
observations required to achieve the minimum constant factor. Our proposed
algorithms are optimal in the sense that their regret upper bounds match their
corresponding regret lower-bounds. We show consistency between simulation and
analytical results under various system parameters.
",Optimal Primary User Emulation Attack Strategies in Cognitive Radio Networks,optimal primary user emulation (pue) attack strategies in cognitive radio networks
"  We present an algorithm for L1-norm kernel PCA and provide a convergence
analysis for it. While an optimal solution of L2-norm kernel PCA can be
obtained through matrix decomposition, finding that of L1-norm kernel PCA is
not trivial due to its non-convexity and non-smoothness. We provide a novel
reformulation through which an equivalent, geometrically interpretable problem
is obtained. Based on the geometric interpretation of the reformulated problem,
we present a fixed-point type algorithm that iteratively computes a binary
weight for each observation. As the algorithm requires only inner products of
data vectors, it is computationally efficient and the kernel trick is
applicable. In the convergence analysis, we show that the algorithm converges
to a local optimal solution in a finite number of steps. Moreover, we provide a
rate of convergence analysis, which has been never done for any L1-norm PCA
algorithm, proving that the sequence of objective values converges at a linear
rate. In numerical experiments, we show that the algorithm is robust in the
presence of entry-wise perturbations and computationally scalable, especially
in a large-scale setting. Lastly, we introduce an application to outlier
detection where the model based on the proposed algorithm outperforms the
benchmark algorithms.
",L1-Norm Kernel PCA Algorithm,kernel principal component analysis
"  While recent advances in deep reinforcement learning have allowed autonomous
learning agents to succeed at a variety of complex tasks, existing algorithms
generally require a lot of training data. One way to increase the speed at
which agents are able to learn to perform tasks is by leveraging the input of
human trainers. Although such input can take many forms, real-time,
scalar-valued feedback is especially useful in situations where it proves
difficult or impossible for humans to provide expert demonstrations. Previous
approaches have shown the usefulness of human input provided in this fashion
(e.g., the TAMER framework), but they have thus far not considered
high-dimensional state spaces or employed the use of deep learning. In this
paper, we do both: we propose Deep TAMER, an extension of the TAMER framework
that leverages the representational power of deep neural networks in order to
learn complex tasks in just a short amount of time with a human trainer. We
demonstrate Deep TAMER's success by using it and just 15 minutes of
human-provided feedback to train an agent that performs better than humans on
the Atari game of Bowling - a task that has proven difficult for even
state-of-the-art reinforcement learning methods.
",Human-in-the-Loop Reinforcement Learning,reinforcement learning
"  This paper proposes a novel neural machine reading model for open-domain
question answering at scale. Existing machine comprehension models typically
assume that a short piece of relevant text containing answers is already
identified and given to the models, from which the models are designed to
extract answers. This assumption, however, is not realistic for building a
large-scale open-domain question answering system which requires both deep text
understanding and identifying relevant text from corpus simultaneously.
  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates
both passage ranking and answer extraction in one single framework. A Q&A
system based on this framework allows users to issue an open-domain question
without needing to provide a piece of text that must contain the answer.
Experiments show that the unified NCR model is able to outperform the
states-of-the-art in both retrieval of relevant text and answer extraction.
",Open-Domain Question Answering,open-domain question answering
"  The ability to deploy neural networks in real-world, safety-critical systems
is severely limited by the presence of adversarial examples: slightly perturbed
inputs that are misclassified by the network. In recent years, several
techniques have been proposed for increasing robustness to adversarial examples
--- and yet most of these have been quickly shown to be vulnerable to future
attacks. For example, over half of the defenses proposed by papers accepted at
ICLR 2018 have already been broken. We propose to address this difficulty
through formal verification techniques. We show how to construct provably
minimally distorted adversarial examples: given an arbitrary neural network and
input sample, we can construct adversarial examples which we prove are of
minimal distortion. Using this approach, we demonstrate that one of the recent
ICLR defense proposals, adversarial retraining, provably succeeds at increasing
the distortion required to construct adversarial examples by a factor of 4.2.
",Adversarial Robustness in Neural Networks,adversarial examples in neural networks
"  We performed an empirical comparison of ICA and PCA algorithms by applying
them on two simulated noisy time series with varying distribution parameters
and level of noise. In general, ICA shows better results than PCA because it
takes into account higher moments of data distribution. On the other hand, PCA
remains quite sensitive to the level of correlations among signals.
",Comparative Analysis of ICA and PCA on Noisy Time Series,dimensionality reduction algorithms
"  We propose a linear-time, single-pass, top-down algorithm for multiple
testing on directed acyclic graphs (DAGs), where nodes represent hypotheses and
edges specify a partial ordering in which hypotheses must be tested. The
procedure is guaranteed to reject a sub-DAG with bounded false discovery rate
(FDR) while satisfying the logical constraint that a rejected node's parents
must also be rejected. It is designed for sequential testing settings, when the
DAG structure is known a priori, but the $p$-values are obtained selectively
(such as in a sequence of experiments), but the algorithm is also applicable in
non-sequential settings when all $p$-values can be calculated in advance (such
as variable/model selection). Our DAGGER algorithm, shorthand for Greedily
Evolving Rejections on DAGs, provably controls the false discovery rate under
independence, positive dependence or arbitrary dependence of the $p$-values.
The DAGGER procedure specializes to known algorithms in the special cases of
trees and line graphs, and simplifies to the classical Benjamini-Hochberg
procedure when the DAG has no edges. We explore the empirical performance of
DAGGER using simulations, as well as a real dataset corresponding to a gene
ontology, showing favorable performance in terms of time and power.
",Multiple Testing on Directed Acyclic Graphs,multiple hypothesis testing on directed acyclic graphs
"  In this paper, we consider a privacy preserving encoding framework for
identification applications covering biometrics, physical object security and
the Internet of Things (IoT). The proposed framework is based on a sparsifying
transform, which consists of a trained linear map, an element-wise
nonlinearity, and privacy amplification. The sparsifying transform and privacy
amplification are not symmetric for the data owner and data user. We
demonstrate that the proposed approach is closely related to sparse ternary
codes (STC), a recent information-theoretic concept proposed for fast
approximate nearest neighbor (ANN) search in high dimensional feature spaces
that being machine learning in nature also offers significant benefits in
comparison to sparse approximation and binary embedding approaches. We
demonstrate that the privacy of the database outsourced to a server as well as
the privacy of the data user are preserved at a low computational cost, storage
and communication burdens.
",Privacy-Preserving Encoding in IoT Biometrics,privacy preserving data encoding for biometrics and iot applications
"  A recent theoretical analysis shows the equivalence between non-negative
matrix factorization (NMF) and spectral clustering based approach to subspace
clustering. As NMF and many of its variants are essentially linear, we
introduce a nonlinear NMF with explicit orthogonality and derive general
kernel-based orthogonal multiplicative update rules to solve the subspace
clustering problem. In nonlinear orthogonal NMF framework, we propose two
subspace clustering algorithms, named kernel-based non-negative subspace
clustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral
normalized cut and ratio cut clustering. We further extend the nonlinear
orthogonal NMF framework and introduce a graph regularization to obtain a
factorization that respects a local geometric structure of the data after the
nonlinear mapping. The proposed NMF-based approach to subspace clustering takes
into account the nonlinear nature of the manifold, as well as its intrinsic
local geometry, which considerably improves the clustering performance when
compared to the several recently proposed state-of-the-art methods.
",Nonlinear Subspace Clustering,nonlinear subspace clustering
"  Word embeddings are a powerful approach for analyzing language, and
exponential family embeddings (EFE) extend them to other types of data. Here we
develop structured exponential family embeddings (S-EFE), a method for
discovering embeddings that vary across related groups of data. We study how
the word usage of U.S. Congressional speeches varies across states and party
affiliation, how words are used differently across sections of the ArXiv, and
how the co-purchase patterns of groceries can vary across seasons. Key to the
success of our method is that the groups share statistical information. We
develop two sharing strategies: hierarchical modeling and amortization. We
demonstrate the benefits of this approach in empirical studies of speeches,
abstracts, and shopping baskets. We show how S-EFE enables group-specific
interpretation of word usage, and outperforms EFE in predicting held-out data.
",Exponential Family Embeddings for Grouped Data Analysis,exponential family embeddings for structured data
"  Rule extraction from black-box models is critical in domains that require
model validation before implementation, as can be the case in credit scoring
and medical diagnosis. Though already a challenging problem in statistical
learning in general, the difficulty is even greater when highly non-linear,
recursive models, such as recurrent neural networks (RNNs), are fit to data.
Here, we study the extraction of rules from second-order recurrent neural
networks trained to recognize the Tomita grammars. We show that production
rules can be stably extracted from trained RNNs and that in certain cases the
rules outperform the trained RNNs.
",Rule Extraction from Recurrent Neural Networks,rule extraction from recurrent neural networks
"  We present an optimised multi-modal dialogue agent for interactive learning
of visually grounded word meanings from a human tutor, trained on real
human-human tutoring data. Within a life-long interactive learning period, the
agent, trained using Reinforcement Learning (RL), must be able to handle
natural conversations with human users and achieve good learning performance
(accuracy) while minimising human effort in the learning process. We train and
evaluate this system in interaction with a simulated human tutor, which is
built on the BURCHAK corpus -- a Human-Human Dialogue dataset for the visual
learning task. The results show that: 1) The learned policy can coherently
interact with the simulated user to achieve the goal of the task (i.e. learning
visual attributes of objects, e.g. colour and shape); and 2) it finds a better
trade-off between classifier accuracy and tutoring costs than hand-crafted
rule-based policies, including ones with dynamic policies.
",Multimodal Dialogue Agent for Visual Learning,multi-modal dialogue systems
"  We present a multi-modal dialogue system for interactive learning of
perceptually grounded word meanings from a human tutor. The system integrates
an incremental, semantic parsing/generation framework - Dynamic Syntax and Type
Theory with Records (DS-TTR) - with a set of visual classifiers that are
learned throughout the interaction and which ground the meaning representations
that it produces. We use this system in interaction with a simulated human
tutor to study the effects of different dialogue policies and capabilities on
the accuracy of learned meanings, learning rates, and efforts/costs to the
tutor. We show that the overall performance of the learning agent is affected
by (1) who takes initiative in the dialogues; (2) the ability to express/use
their confidence level about visual attributes; and (3) the ability to process
elliptical and incrementally constructed dialogue turns. Ultimately, we train
an adaptive dialogue policy which optimises the trade-off between classifier
accuracy and tutoring costs.
",Dialogue Systems for Interactive Learning,multimodal dialogue systems for interactive learning
"  We motivate and describe a new freely available human-human dialogue dataset
for interactive learning of visually grounded word meanings through ostensive
definition by a tutor to a learner. The data has been collected using a novel,
character-by-character variant of the DiET chat tool (Healey et al., 2003;
Mills and Healey, submitted) with a novel task, where a Learner needs to learn
invented visual attribute words (such as "" burchak "" for square) from a tutor.
As such, the text-based interactions closely resemble face-to-face conversation
and thus contain many of the linguistic phenomena encountered in natural,
spontaneous dialogue. These include self-and other-correction, mid-sentence
continuations, interruptions, overlaps, fillers, and hedges. We also present a
generic n-gram framework for building user (i.e. tutor) simulations from this
type of incremental data, which is freely available to researchers. We show
that the simulations produce outputs that are similar to the original data
(e.g. 78% turn match similarity). Finally, we train and evaluate a
Reinforcement Learning dialogue control agent for learning visually grounded
word meanings, trained from the BURCHAK corpus. The learned policy shows
comparable performance to a rule-based system built previously.
",Dialogue Systems for Visually Grounded Word Learning,visually grounded word meaning learning
"  When using stochastic gradient descent to solve large-scale machine learning
problems, a common practice of data processing is to shuffle the training data,
partition the data across multiple machines if needed, and then perform several
epochs of training on the re-shuffled (either locally or globally) data. The
above procedure makes the instances used to compute the gradients no longer
independently sampled from the training data set. Then does the distributed SGD
method have desirable convergence properties in this practical situation? In
this paper, we give answers to this question. First, we give a mathematical
formulation for the practical data processing procedure in distributed machine
learning, which we call data partition with global/local shuffling. We observe
that global shuffling is equivalent to without-replacement sampling if the
shuffling operations are independent. We prove that SGD with global shuffling
has convergence guarantee in both convex and non-convex cases. An interesting
finding is that, the non-convex tasks like deep learning are more suitable to
apply shuffling comparing to the convex tasks. Second, we conduct the
convergence analysis for SGD with local shuffling. The convergence rate for
local shuffling is slower than that for global shuffling, since it will lose
some information if there's no communication between partitioned data. Finally,
we consider the situation when the permutation after shuffling is not uniformly
distributed (insufficient shuffling), and discuss the condition under which
this insufficiency will not influence the convergence rate. Our theoretical
results provide important insights to large-scale machine learning, especially
in the selection of data processing methods in order to achieve faster
convergence and good speedup. Our theoretical findings are verified by
extensive experiments on logistic regression and deep neural networks.
",Convergence Analysis of Distributed SGD with Shuffling in Machine Learning,distributed machine learning algorithms
"  In this paper we provide a finite-sample and an infinite-sample representer
theorem for the concatenation of (linear combinations of) kernel functions of
reproducing kernel Hilbert spaces. These results serve as mathematical
foundation for the analysis of machine learning algorithms based on
compositions of functions. As a direct consequence in the finite-sample case,
the corresponding infinite-dimensional minimization problems can be recast into
(nonlinear) finite-dimensional minimization problems, which can be tackled with
nonlinear optimization algorithms. Moreover, we show how concatenated machine
learning problems can be reformulated as neural networks and how our
representer theorem applies to a broad class of state-of-the-art deep learning
methods.
",Representer Theorem for Kernel Methods,deep learning
"  GANs provide a framework for training generative models which mimic a data
distribution. However, in many cases we wish to train these generative models
to optimize some auxiliary objective function within the data it generates,
such as making more aesthetically pleasing images. In some cases, these
objective functions are difficult to evaluate, e.g. they may require human
interaction. Here, we develop a system for efficiently improving a GAN to
target an objective involving human interaction, specifically generating images
that increase rates of positive user interactions. To improve the generative
model, we build a model of human behavior in the targeted domain from a
relatively small set of interactions, and then use this behavioral model as an
auxiliary loss function to improve the generative model. We show that this
system is successful at improving positive interaction rates, at least on
simulated data, and characterize some of the factors that affect its
performance.
",Human-in-the-Loop GAN Optimization,generative adversarial networks (gans) with human interaction
"  Enabling robots to autonomously navigate complex environments is essential
for real-world deployment. Prior methods approach this problem by having the
robot maintain an internal map of the world, and then use a localization and
planning method to navigate through the internal map. However, these approaches
often include a variety of assumptions, are computationally intensive, and do
not learn from failures. In contrast, learning-based methods improve as the
robot acts in the environment, but are difficult to deploy in the real-world
due to their high sample complexity. To address the need to learn complex
policies with few samples, we propose a generalized computation graph that
subsumes value-based model-free methods and model-based methods, with specific
instantiations interpolating between model-free and model-based. We then
instantiate this graph to form a navigation model that learns from raw images
and is sample efficient. Our simulated car experiments explore the design
decisions of our navigation model, and show our approach outperforms
single-step and $N$-step double Q-learning. We also evaluate our approach on a
real-world RC car and show it can learn to navigate through a complex indoor
environment with a few hours of fully autonomous, self-supervised training.
Videos of the experiments and code can be found at github.com/gkahn13/gcg
",Robot Navigation,autonomous robot navigation
"  We introduce a novel family of distances, called the chord gap divergences,
that generalizes the Jensen divergences (also called the Burbea-Rao distances),
and study its properties. It follows a generalization of the celebrated
statistical Bhattacharyya distance that is frequently met in applications. We
report an iterative concave-convex procedure for computing centroids, and
analyze the performance of the $k$-means++ clustering with respect to that new
dissimilarity measure by introducing the Taylor-Lagrange remainder form of the
skew Jensen divergences.
",Chord Gap Divergences in Clustering,mathematics: information theory
"  We study the task of unsupervised domain adaptation, where no labeled data
from the target domain is provided during training time. To deal with the
potential discrepancy between the source and target distributions, both in
features and labels, we exploit a copula-based regression framework. The
benefits of this approach are two-fold: (a) it allows us to model a broader
range of conditional predictive densities beyond the common exponential family,
(b) we show how to leverage Sklar's theorem, the essence of the copula
formulation relating the joint density to the copula dependency functions, to
find effective feature mappings that mitigate the domain mismatch. By
transforming the data to a copula domain, we show on a number of benchmark
datasets (including human emotion estimation), and using different regression
models for prediction, that we can achieve a more robust and accurate
estimation of target labels, compared to recently proposed feature
transformation (adaptation) methods.
",Unsupervised Domain Adaptation,unsupervised domain adaptation
"  In this article, we present a method to learn the interaction topology of a
network of agents undergoing linear consensus updates in a non invasive manner.
Our approach is based on multivariate Wiener filtering, which is known to
recover spurious edges apart from the true edges in the topology. The main
contribution of this work is to show that in the case of undirected consensus
networks, all spurious links obtained using Wiener filtering can be identified
using frequency response of the Wiener filters. Thus, the exact interaction
topology of the agents is unveiled. The method presented requires time series
measurements of the state of the agents and does not require any knowledge of
link weights. To the best of our knowledge this is the first approach that
provably reconstructs the structure of undirected consensus networks with
correlated noise. We illustrate the effectiveness of the method developed
through numerical simulations as well as experiments on a five node network of
Raspberry Pis.
",Network Topology Inference,network reconstruction
"  A standard recipe for spoken language recognition is to apply a Gaussian
back-end to i-vectors. This ignores the uncertainty in the i-vector extraction,
which could be important especially for short utterances. A recent paper by
Cumani, Plchot and Fer proposes a solution to propagate that uncertainty into
the backend. We propose an alternative method of propagating the uncertainty.
",Uncertainty Propagation in I-Vector Extraction,uncertainty propagation in spoken language recognition
"  In this paper, we study the problem of sampling from a given probability
density function that is known to be smooth and strongly log-concave. We
analyze several methods of approximate sampling based on discretizations of the
(highly overdamped) Langevin diffusion and establish guarantees on its error
measured in the Wasserstein-2 distance. Our guarantees improve or extend the
state-of-the-art results in three directions. First, we provide an upper bound
on the error of the first-order Langevin Monte Carlo (LMC) algorithm with
optimized varying step-size. This result has the advantage of being horizon
free (we do not need to know in advance the target precision) and to improve by
a logarithmic factor the corresponding result for the constant step-size.
Second, we study the case where accurate evaluations of the gradient of the
log-density are unavailable, but one can have access to approximations of the
aforementioned gradient. In such a situation, we consider both deterministic
and stochastic approximations of the gradient and provide an upper bound on the
sampling error of the first-order LMC that quantifies the impact of the
gradient evaluation inaccuracies. Third, we establish upper bounds for two
versions of the second-order LMC, which leverage the Hessian of the
log-density. We nonasymptotic guarantees on the sampling error of these
second-order LMCs. These guarantees reveal that the second-order LMC algorithms
improve on the first-order LMC in ill-conditioned settings.
",Log-concave Density Sampling,approximate sampling from smooth log-concave distributions
"  In an effort to overcome the data deluge in computational biology and
bioinformatics and to facilitate bioinformatics research in the era of big
data, we identify some of the most influential algorithms that have been widely
used in the bioinformatics community. These top data mining and machine
learning algorithms cover classification, clustering, regression, graphical
model-based learning, and dimensionality reduction. The goal of this study is
to guide the focus of scalable computing experts in the endeavor of applying
new storage and scalable computation designs to bioinformatics algorithms that
merit their attention most, following the engineering maxim of ""optimize the
common case"".
",Influential Bioinformatics Algorithms,bioinformatics algorithms
"  It is well known that for some tasks, labeled data sets may be hard to
gather. Therefore, we wished to tackle here the problem of having insufficient
training data. We examined learning methods from unlabeled data after an
initial training on a limited labeled data set. The suggested approach can be
used as an online learning method on the unlabeled test set. In the general
classification task, whenever we predict a label with high enough confidence,
we treat it as a true label and train the data accordingly. For the semantic
segmentation task, a classic example for an expensive data labeling process, we
do so pixel-wise. Our suggested approaches were applied on the MNIST data-set
as a proof of concept for a vision classification task and on the ADE20K
data-set in order to tackle the semi-supervised semantic segmentation problem.
",Semi-supervised Learning,semi-supervised learning
"  We propose a deep learning based method, the Deep Ritz Method, for
numerically solving variational problems, particularly the ones that arise from
partial differential equations. The Deep Ritz method is naturally nonlinear,
naturally adaptive and has the potential to work in rather high dimensions. The
framework is quite simple and fits well with the stochastic gradient descent
method used in deep learning. We illustrate the method on several problems
including some eigenvalue problems.
",Deep Ritz Method for Solving Variational Problems,numerical methods for solving partial differential equations
"  We propose an efficient meta-algorithm for Bayesian estimation problems that
is based on low-degree polynomials, semidefinite programming, and tensor
decomposition. The algorithm is inspired by recent lower bound constructions
for sum-of-squares and related to the method of moments. Our focus is on sample
complexity bounds that are as tight as possible (up to additive lower-order
terms) and often achieve statistical thresholds or conjectured computational
thresholds.
  Our algorithm recovers the best known bounds for community detection in the
sparse stochastic block model, a widely-studied class of estimation problems
for community detection in graphs. We obtain the first recovery guarantees for
the mixed-membership stochastic block model (Airoldi et el.) in constant
average degree graphs---up to what we conjecture to be the computational
threshold for this model. We show that our algorithm exhibits a sharp
computational threshold for the stochastic block model with multiple
communities beyond the Kesten--Stigum bound---giving evidence that this task
may require exponential time.
  The basic strategy of our algorithm is strikingly simple: we compute the
best-possible low-degree approximation for the moments of the posterior
distribution of the parameters and use a robust tensor decomposition algorithm
to recover the parameters from these approximate posterior moments.
",Bayesian Estimation via Tensor Decomposition,bayesian estimation
"  Evaluation and validation of complicated control systems are crucial to
guarantee usability and safety. Usually, failure happens in some very rarely
encountered situations, but once triggered, the consequence is disastrous.
Accelerated Evaluation is a methodology that efficiently tests those
rarely-occurring yet critical failures via smartly-sampled test cases. The
distribution used in sampling is pivotal to the performance of the method, but
building a suitable distribution requires case-by-case analysis. This paper
proposes a versatile approach for constructing sampling distribution using
kernel method. The approach uses statistical learning tools to approximate the
critical event sets and constructs distributions based on the unique properties
of Gaussian distributions. We applied the method to evaluate the automated
vehicles. Numerical experiments show proposed approach can robustly identify
the rare failures and significantly reduce the evaluation time.
",Sampling Distributions for Rare Failure Detection in Complex Systems,reliability engineering
"  libact is a Python package designed to make active learning easier for
general users. The package not only implements several popular active learning
strategies, but also features the active-learning-by-learning meta-algorithm
that assists the users to automatically select the best strategy on the fly.
Furthermore, the package provides a unified interface for implementing more
strategies, models and application-specific labelers. The package is
open-source on Github, and can be easily installed from Python Package Index
repository.
",Active Learning Frameworks,active learning
"  We study the central problem in data privacy: how to share data with an
analyst while providing both privacy and utility guarantees to the user that
owns the data. In this setting, we present an estimation-theoretic analysis of
the privacy-utility trade-off (PUT). Here, an analyst is allowed to reconstruct
(in a mean-squared error sense) certain functions of the data (utility), while
other private functions should not be reconstructed with distortion below a
certain threshold (privacy). We demonstrate how chi-square information captures
the fundamental PUT in this case and provide bounds for the best PUT. We
propose a convex program to compute privacy-assuring mappings when the
functions to be disclosed and hidden are known a priori and the data
distribution is known. We derive lower bounds on the minimum mean-squared error
of estimating a target function from the disclosed data and evaluate the
robustness of our approach when an empirical distribution is used to compute
the privacy-assuring mappings instead of the true data distribution. We
illustrate the proposed approach through two numerical experiments.
",Data Privacy Trade-Offs,data privacy
"  This paper presents a class of Dynamic Multi-Armed Bandit problems where the
reward can be modeled as the noisy output of a time varying linear stochastic
dynamic system that satisfies some boundedness constraints. The class allows
many seemingly different problems with time varying option characteristics to
be considered in a single framework. It also opens up the possibility of
considering many new problems of practical importance. For instance it affords
the simultaneous consideration of temporal option unavailabilities and the
depen- dencies between options with time varying option characteristics in a
seamless manner. We show that, for this class of problems, the combination of
any Upper Confidence Bound type algorithm with any efficient reward estimator
for the expected reward ensures the logarithmic bounding of the expected
cumulative regret. We demonstrate the versatility of the approach by the
explicit consideration of a new example of practical interest.
",Dynamic Multi-Armed Bandit Problems,dynamic multi-armed bandit problems
"  We examine the problem of learning and planning on high-dimensional domains
with long horizons and sparse rewards. Recent approaches have shown great
successes in many Atari 2600 domains. However, domains with long horizons and
sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for
existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup,
and Singh 1999) have shown to be useful in tackling long-horizon problems. We
combine recent techniques of deep reinforcement learning with existing
model-based approaches using an expert-provided state abstraction. We construct
toy domains that elucidate the problem of long horizons, sparse rewards and
high-dimensional inputs, and show that our algorithm significantly outperforms
previous methods on these domains. Our abstraction-based approach outperforms
Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and
exhibits backtracking behavior that is absent from previous methods.
",Reinforcement Learning with High-Dimensional Domains,reinforcement learning
"  The Matrix Factorization models, sometimes called the latent factor models,
are a family of methods in the recommender system research area to (1) generate
the latent factors for the users and the items and (2) predict users' ratings
on items based on their latent factors. However, current Matrix Factorization
models presume that all the latent factors are equally weighted, which may not
always be a reasonable assumption in practice. In this paper, we propose a new
model, called Weighted-SVD, to integrate the linear regression model with the
SVD model such that each latent factor accompanies with a corresponding weight
parameter. This mechanism allows the latent factors have different weights to
influence the final ratings. The complexity of the Weighted-SVD model is
slightly larger than the SVD model but much smaller than the SVD++ model. We
compared the Weighted-SVD model with several latent factor models on five
public datasets based on the Root-Mean-Squared-Errors (RMSEs). The results show
that the Weighted-SVD model outperforms the baseline methods in all the
experimental datasets under almost all settings.
",Weighted Latent Factor Models in Recommender Systems,matrix factorization and weighted latent factors in recommender systems
"  Deep neural networks have become widely used, obtaining remarkable results in
domains such as computer vision, speech recognition, natural language
processing, audio recognition, social network filtering, machine translation,
and bio-informatics, where they have produced results comparable to human
experts. However, these networks can be easily fooled by adversarial
perturbations: minimal changes to correctly-classified inputs, that cause the
network to mis-classify them. This phenomenon represents a concern for both
safety and security, but it is currently unclear how to measure a network's
robustness against such perturbations. Existing techniques are limited to
checking robustness around a few individual input points, providing only very
limited guarantees. We propose a novel approach for automatically identifying
safe regions of the input space, within which the network is robust against
adversarial perturbations. The approach is data-guided, relying on clustering
to identify well-defined geometric regions as candidate safe regions. We then
utilize verification techniques to confirm that these regions are safe or to
provide counter-examples showing that they are not safe. We also introduce the
notion of targeted robustness which, for a given target label and region,
ensures that a NN does not map any input in the region to the target label. We
evaluated our technique on the MNIST dataset and on a neural network
implementation of a controller for the next-generation Airborne Collision
Avoidance System for unmanned aircraft (ACAS Xu). For these networks, our
approach identified multiple regions which were completely safe as well as some
which were only safe for specific labels. It also discovered several
adversarial perturbations of interest.
",Adversarial Robustness in Deep Neural Networks,robustness of deep neural networks
"  In the online multiple testing problem, p-values corresponding to different
null hypotheses are observed one by one, and the decision of whether or not to
reject the current hypothesis must be made immediately, after which the next
p-value is observed. Alpha-investing algorithms to control the false discovery
rate (FDR), formulated by Foster and Stine, have been generalized and applied
to many settings, including quality-preserving databases in science and
multiple A/B or multi-armed bandit tests for internet commerce. This paper
improves the class of generalized alpha-investing algorithms (GAI) in four
ways: (a) we show how to uniformly improve the power of the entire class of
monotone GAI procedures by awarding more alpha-wealth for each rejection,
giving a win-win resolution to a recent dilemma raised by Javanmard and
Montanari, (b) we demonstrate how to incorporate prior weights to indicate
domain knowledge of which hypotheses are likely to be non-null, (c) we allow
for differing penalties for false discoveries to indicate that some hypotheses
may be more important than others, (d) we define a new quantity called the
decaying memory false discovery rate (mem-FDR) that may be more meaningful for
truly temporal applications, and which alleviates problems that we describe and
refer to as ""piggybacking"" and ""alpha-death"". Our GAI++ algorithms incorporate
all four generalizations simultaneously, and reduce to more powerful variants
of earlier algorithms when the weights and decay are all set to unity. Finally,
we also describe a simple method to derive new online FDR rules based on an
estimated false discovery proportion.
",Online Multiple Testing,alpha-investing algorithms for controlling the false discovery rate in online multiple testing
"  Current remote sensing image classification problems have to deal with an
unprecedented amount of heterogeneous and complex data sources. Upcoming
missions will soon provide large data streams that will make land cover/use
classification difficult. Machine learning classifiers can help at this, and
many methods are currently available. A popular kernel classifier is the
Gaussian process classifier (GPC), since it approaches the classification
problem with a solid probabilistic treatment, thus yielding confidence
intervals for the predictions as well as very competitive results to
state-of-the-art neural networks and support vector machines. However, its
computational cost is prohibitive for large scale applications, and constitutes
the main obstacle precluding wide adoption. This paper tackles this problem by
introducing two novel efficient methodologies for Gaussian Process (GP)
classification. We first include the standard random Fourier features
approximation into GPC, which largely decreases its computational cost and
permits large scale remote sensing image classification. In addition, we
propose a model which avoids randomly sampling a number of Fourier frequencies,
and alternatively learns the optimal ones within a variational Bayes approach.
The performance of the proposed methods is illustrated in complex problems of
cloud detection from multispectral imagery and infrared sounding data.
Excellent empirical results support the proposal in both computational cost and
accuracy.
",Efficient Gaussian Process Classification for Remote Sensing Image Analysis,gaussian process classification for remote sensing image classification
"  Speech recognition is largely taking advantage of deep learning, showing that
substantial benefits can be obtained by modern Recurrent Neural Networks
(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which
typically reach state-of-the-art performance in many tasks thanks to their
ability to learn long-term dependencies and robustness to vanishing gradients.
Nevertheless, LSTMs have a rather complex design with three multiplicative
gates, that might impair their efficient implementation. An attempt to simplify
LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just
two multiplicative gates.
  This paper builds on these efforts by further revising GRUs and proposing a
simplified architecture potentially more suitable for speech recognition. The
contribution of this work is two-fold. First, we suggest to remove the reset
gate in the GRU design, resulting in a more efficient single-gate architecture.
Second, we propose to replace tanh with ReLU activations in the state update
equations. Results show that, in our implementation, the revised architecture
reduces the per-epoch training time with more than 30% and consistently
improves recognition performance across different tasks, input features, and
noisy conditions when compared to a standard GRU.
",Simplified Recurrent Neural Networks for Speech Recognition,deep learning for speech recognition
"  The area under the ROC curve (AUC) is a measure of interest in various
machine learning and data mining applications. It has been widely used to
evaluate classification performance on heavily imbalanced data. The kernelized
AUC maximization machines have established a superior generalization ability
compared to linear AUC machines because of their capability in modeling the
complex nonlinear structure underlying most real-world data. However, the high
training complexity renders the kernelized AUC machines infeasible for
large-scale data. In this paper, we present two nonlinear AUC maximization
algorithms that optimize pairwise linear classifiers over a finite-dimensional
feature space constructed via the k-means Nystr\""{o}m method. Our first
algorithm maximize the AUC metric by optimizing a pairwise squared hinge loss
function using the truncated Newton method. However, the second-order batch AUC
maximization method becomes expensive to optimize for extremely massive
datasets. This motivate us to develop a first-order stochastic AUC maximization
algorithm that incorporates a scheduled regularization update and scheduled
averaging techniques to accelerate the convergence of the classifier.
Experiments on several benchmark datasets demonstrate that the proposed AUC
classifiers are more efficient than kernelized AUC machines while they are able
to surpass or at least match the AUC performance of the kernelized AUC
machines. The experiments also show that the proposed stochastic AUC classifier
outperforms the state-of-the-art online AUC maximization methods in terms of
AUC classification accuracy.
",Efficient AUC Maximization Algorithms for Large-Scale Data,machine learning
"  Analysis of an organization's computer network activity is a key component of
early detection and mitigation of insider threat, a growing concern for many
organizations. Raw system logs are a prototypical example of streaming data
that can quickly scale beyond the cognitive power of a human analyst. As a
prospective filter for the human analyst, we present an online unsupervised
deep learning approach to detect anomalous network activity from system logs in
real time. Our models decompose anomaly scores into the contributions of
individual user behavior features for increased interpretability to aid
analysts reviewing potential cases of insider threat. Using the CERT Insider
Threat Dataset v6.2 and threat detection recall as our performance metric, our
novel deep and recurrent neural network models outperform Principal Component
Analysis, Support Vector Machine and Isolation Forest based anomaly detection
baselines. For our best model, the events labeled as insider threat activity in
our dataset had an average anomaly score in the 95.53 percentile, demonstrating
our approach's potential to greatly reduce analyst workloads.
",Insider Threat Detection,insider threat detection using deep learning
"  Deep reinforcement learning has shown promising results in learning control
policies for complex sequential decision-making tasks. However, these neural
network-based policies are known to be vulnerable to adversarial examples. This
vulnerability poses a potentially serious threat to safety-critical systems
such as autonomous vehicles. In this paper, we propose a defense mechanism to
defend reinforcement learning agents from adversarial attacks by leveraging an
action-conditioned frame prediction module. Our core idea is that the
adversarial examples targeting at a neural network-based policy are not
effective for the frame prediction model. By comparing the action distribution
produced by a policy from processing the current observed frame to the action
distribution produced by the same policy from processing the predicted frame
from the action-conditioned frame prediction module, we can detect the presence
of adversarial examples. Beyond detecting the presence of adversarial examples,
our method allows the agent to continue performing the task using the predicted
frame when the agent is under attack. We evaluate the performance of our
algorithm using five games in Atari 2600. Our results demonstrate that the
proposed defense mechanism achieves favorable performance against baseline
algorithms in detecting adversarial examples and in earning rewards when the
agents are under attack.
",Adversarial Attacks on Reinforcement Learning,reinforcement learning defense
"  Online social networks, World Wide Web, media and technological networks, and
other types of so-called information networks are ubiquitous nowadays. These
information networks are inherently heterogeneous and dynamic. They are
heterogeneous as they consist of multi-typed objects and relations, and they
are dynamic as they are constantly evolving over time. One of the challenging
issues in such heterogeneous and dynamic environments is to forecast those
relationships in the network that will appear in the future. In this paper, we
try to solve the problem of continuous-time relationship prediction in dynamic
and heterogeneous information networks. This implies predicting the time it
takes for a relationship to appear in the future, given its features that have
been extracted by considering both heterogeneity and temporal dynamics of the
underlying network. To this end, we first introduce a feature extraction
framework that combines the power of meta-path-based modeling and recurrent
neural networks to effectively extract features suitable for relationship
prediction regarding heterogeneity and dynamicity of the networks. Next, we
propose a supervised non-parametric approach, called Non-Parametric Generalized
Linear Model (NP-GLM), which infers the hidden underlying probability
distribution of the relationship building time given its features. We then
present a learning algorithm to train NP-GLM and an inference method to answer
time-related queries. Extensive experiments conducted on synthetic data and
three real-world datasets, namely Delicious, MovieLens, and DBLP, demonstrate
the effectiveness of NP-GLM in solving continuous-time relationship prediction
problem vis-a-vis competitive baselines
",Relationship Prediction in Dynamic Networks,relationship prediction in dynamic and heterogeneous information networks
"  Using a recently proposed privacy definition of R\'enyi Differential Privacy
(RDP), we re-examine the inherent privacy of releasing a single sample from a
posterior distribution. We exploit the impact of the prior distribution in
mitigating the influence of individual data points. In particular, we focus on
sampling from an exponential family and specific generalized linear models,
such as logistic regression. We propose novel RDP mechanisms as well as
offering a new RDP analysis for an existing method in order to add value to the
RDP framework. Each method is capable of achieving arbitrary RDP privacy
guarantees, and we offer experimental results of their efficacy.
",Differential Privacy in Statistical Analysis,differential privacy in bayesian inference
"  In today's era of big data, robust least-squares regression becomes a more
challenging problem when considering the adversarial corruption along with
explosive growth of datasets. Traditional robust methods can handle the noise
but suffer from several challenges when applied in huge dataset including 1)
computational infeasibility of handling an entire dataset at once, 2) existence
of heterogeneously distributed corruption, and 3) difficulty in corruption
estimation when data cannot be entirely loaded. This paper proposes online and
distributed robust regression approaches, both of which can concurrently
address all the above challenges. Specifically, the distributed algorithm
optimizes the regression coefficients of each data block via heuristic hard
thresholding and combines all the estimates in a distributed robust
consolidation. Furthermore, an online version of the distributed algorithm is
proposed to incrementally update the existing estimates with new incoming data.
We also prove that our algorithms benefit from strong robustness guarantees in
terms of regression coefficient recovery with a constant upper bound on the
error of state-of-the-art batch methods. Extensive experiments on synthetic and
real datasets demonstrate that our approaches are superior to those of existing
methods in effectiveness, with competitive efficiency.
",Robust Regression in Big Data,distributed robust regression
"  Facial Key Points (FKPs) Detection is an important and challenging problem in
the fields of computer vision and machine learning. It involves predicting the
co-ordinates of the FKPs, e.g. nose tip, center of eyes, etc, for a given face.
In this paper, we propose a LeNet adapted Deep CNN model - NaimishNet, to
operate on facial key points data and compare our model's performance against
existing state of the art approaches.
",Facial Key Points Detection,facial key points detection
"  Training feedforward neural networks with standard logistic activations is
considered difficult because of the intrinsic properties of these sigmoidal
functions. This work aims at showing that these networks can be trained to
achieve generalization performance comparable to those based on hyperbolic
tangent activations. The solution consists on applying a set of conditions in
parameter initialization, which have been derived from the study of the
properties of a single neuron from an information-theoretic perspective. The
proposed initialization is validated through an extensive experimental
analysis.
",Neural Network Initialization Techniques,deep learning
"  In this paper, we propose spatial propagation networks for learning the
affinity matrix for vision tasks. We show that by constructing a row/column
linear propagation model, the spatially varying transformation matrix exactly
constitutes an affinity matrix that models dense, global pairwise relationships
of an image. Specifically, we develop a three-way connection for the linear
propagation model, which (a) formulates a sparse transformation matrix, where
all elements can be the output from a deep CNN, but (b) results in a dense
affinity matrix that effectively models any task-specific pairwise similarity
matrix. Instead of designing the similarity kernels according to image features
of two points, we can directly output all the similarities in a purely
data-driven manner. The spatial propagation network is a generic framework that
can be applied to many affinity-related tasks, including but not limited to
image matting, segmentation and colorization, to name a few. Essentially, the
model can learn semantically-aware affinity values for high-level vision tasks
due to the powerful learning capability of the deep neural network classifier.
We validate the framework on the task of refinement for image segmentation
boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic
segmentation tasks show that the spatial propagation network provides a
general, effective and efficient solution for generating high-quality
segmentation results.
",Image Segmentation,computer vision
"  When classifying point clouds, a large amount of time is devoted to the
process of engineering a reliable set of features which are then passed to a
classifier of choice. Generally, such features - usually derived from the
3D-covariance matrix - are computed using the surrounding neighborhood of
points. While these features capture local information, the process is usually
time-consuming, and requires the application at multiple scales combined with
contextual methods in order to adequately describe the diversity of objects
within a scene. In this paper we present a 1D-fully convolutional network that
consumes terrain-normalized points directly with the corresponding spectral
data,if available, to generate point-wise labeling while implicitly learning
contextual features in an end-to-end fashion. Our method uses only the
3D-coordinates and three corresponding spectral features for each point.
Spectral features may either be extracted from 2D-georeferenced images, as
shown here for Light Detection and Ranging (LiDAR) point clouds, or extracted
directly for passive-derived point clouds,i.e. from muliple-view imagery. We
train our network by splitting the data into square regions, and use a pooling
layer that respects the permutation-invariance of the input points. Evaluated
using the ISPRS 3D Semantic Labeling Contest, our method scored second place
with an overall accuracy of 81.6%. We ranked third place with a mean F1-score
of 63.32%, surpassing the F1-score of the method with highest accuracy by
1.69%. In addition to labeling 3D-point clouds, we also show that our method
can be easily extended to 2D-semantic segmentation tasks, with promising
initial results.
",Point Cloud Classification,3d point cloud classification
"  Relational databases are valuable resources for learning novel and
interesting relations and concepts. In order to constraint the search through
the large space of candidate definitions, users must tune the algorithm by
specifying a language bias. Unfortunately, specifying the language bias is done
via trial and error and is guided by the expert's intuitions. We propose
AutoBias, a system that leverages information in the schema and content of the
database to automatically induce the language bias used by popular relational
learning systems. We show that AutoBias delivers the same accuracy as using
manually-written language bias by imposing only a slight overhead on the
running time of the learning algorithm.
",Auto-Induction of Language Bias in Relational Learning,automatic generation of language bias for relational databases
"  Deep neural networks are widely used in various domains. However, the nature
of computations at each layer of the deep networks is far from being well
understood. Increasing the interpretability of deep neural networks is thus
important. Here, we construct a mean-field framework to understand how compact
representations are developed across layers, not only in deterministic deep
networks with random weights but also in generative deep networks where an
unsupervised learning is carried out. Our theory shows that the deep
computation implements a dimensionality reduction while maintaining a finite
level of weak correlations between neurons for possible feature extraction.
Mechanisms of dimensionality reduction and decorrelation are unified in the
same framework. This work may pave the way for understanding how a sensory
hierarchy works.
",Interpretable Deep Neural Networks,deep neural network interpretability
"  We introduce a novel approach to Maximum A Posteriori inference based on
discrete graphical models. By utilizing local Wasserstein distances for
coupling assignment measures across edges of the underlying graph, a given
discrete objective function is smoothly approximated and restricted to the
assignment manifold. A corresponding multiplicative update scheme combines in a
single process (i) geometric integration of the resulting Riemannian gradient
flow and (ii) rounding to integral solutions that represent valid labelings.
Throughout this process, local marginalization constraints known from the
established LP relaxation are satisfied, whereas the smooth geometric setting
results in rapidly converging iterations that can be carried out in parallel
for every edge.
",Graph-Based MAP Inference,maximum a posteriori inference based on discrete graphical models
"  Radiomics aims to extract and analyze large numbers of quantitative features
from medical images and is highly promising in staging, diagnosing, and
predicting outcomes of cancer treatments. Nevertheless, several challenges need
to be addressed to construct an optimal radiomics predictive model. First, the
predictive performance of the model may be reduced when features extracted from
an individual imaging modality are blindly combined into a single predictive
model. Second, because many different types of classifiers are available to
construct a predictive model, selecting an optimal classifier for a particular
application is still challenging. In this work, we developed multi-modality and
multi-classifier radiomics predictive models that address the aforementioned
issues in currently available models. Specifically, a new reliable classifier
fusion strategy was proposed to optimally combine output from different
modalities and classifiers. In this strategy, modality-specific classifiers
were first trained, and an analytic evidential reasoning (ER) rule was
developed to fuse the output score from each modality to construct an optimal
predictive model. One public data set and two clinical case studies were
performed to validate model performance. The experimental results indicated
that the proposed ER rule based radiomics models outperformed the traditional
models that rely on a single classifier or simply use combined features from
different modalities.
",Radiomics Model Optimization,medical imaging analytics
"  This paper addresses the optimal control problem known as the Linear
Quadratic Regulator in the case when the dynamics are unknown. We propose a
multi-stage procedure, called Coarse-ID control, that estimates a model from a
few experimental trials, estimates the error in that model with respect to the
truth, and then designs a controller using both the model and uncertainty
estimate. Our technique uses contemporary tools from random matrix theory to
bound the error in the estimation procedure. We also employ a recently
developed approach to control synthesis called System Level Synthesis that
enables robust control design by solving a convex optimization problem. We
provide end-to-end bounds on the relative error in control cost that are nearly
optimal in the number of parameters and that highlight salient properties of
the system to be controlled such as closed-loop sensitivity and optimal control
magnitude. We show experimentally that the Coarse-ID approach enables efficient
computation of a stabilizing controller in regimes where simple control schemes
that do not take the model uncertainty into account fail to stabilize the true
system.
",Optimal Control of Unknown Systems,control engineering
"  Low dimensional embeddings that capture the main variations of interest in
collections of data are important for many applications. One way to construct
these embeddings is to acquire estimates of similarity from the crowd. However,
similarity is a multi-dimensional concept that varies from individual to
individual. Existing models for learning embeddings from the crowd typically
make simplifying assumptions such as all individuals estimate similarity using
the same criteria, the list of criteria is known in advance, or that the crowd
workers are not influenced by the data that they see. To overcome these
limitations we introduce Context Embedding Networks (CENs). In addition to
learning interpretable embeddings from images, CENs also model worker biases
for different attributes along with the visual context i.e. the visual
attributes highlighted by a set of images. Experiments on two noisy crowd
annotated datasets show that modeling both worker bias and visual context
results in more interpretable embeddings compared to existing approaches.
",Crowdsourced Embedding Learning,learning embeddings from crowds with contextual information
"  IQ tests are an accepted method for assessing human intelligence. The tests
consist of several parts that must be solved under a time constraint. Of all
the tested abilities, pattern recognition has been found to have the highest
correlation with general intelligence. This is primarily because pattern
recognition is the ability to find order in a noisy environment, a necessary
skill for intelligent agents. In this paper, we propose a convolutional neural
network (CNN) model for solving geometric pattern recognition problems. The CNN
receives as input multiple ordered input images and outputs the next image
according to the pattern. Our CNN is able to solve problems involving rotation,
reflection, color, size and shape patterns and score within the top 5% of human
performance.
",Neural Networks for Pattern Recognition,pattern recognition
"  We present a deep neural network for a model-free prediction of a chaotic
dynamical system from noisy observations. The proposed deep learning model aims
to predict the conditional probability distribution of a state variable. The
Long Short-Term Memory network (LSTM) is employed to model the nonlinear
dynamics and a softmax layer is used to approximate a probability distribution.
The LSTM model is trained by minimizing a regularized cross-entropy function.
The LSTM model is validated against delay-time chaotic dynamical systems,
Mackey-Glass and Ikeda equations. It is shown that the present LSTM makes a
good prediction of the nonlinear dynamics by effectively filtering out the
noise. It is found that the prediction uncertainty of a multiple-step forecast
of the LSTM model is not a monotonic function of time; the predicted standard
deviation may increase or decrease dynamically in time.
",Deep Learning for Chaotic Systems Prediction,model-free prediction of chaotic dynamical systems
"  Traffic flow prediction is an important research issue to avoid traffic
congestion in transportation systems. Traffic congestion avoiding can be
achieved by knowing traffic flow and then conducting transportation planning.
Achieving traffic flow prediction is challenging as the prediction is affected
by many complex factors such as inter-region traffic, vehicles' relations, and
sudden events. However, as the mobile data of vehicles has been widely
collected by sensor-embedded devices in transportation systems, it is possible
to predict the traffic flow by analysing mobile data. This study proposes a
deep learning based prediction algorithm, DeepTFP, to collectively predict the
traffic flow on each and every traffic road of a city. This algorithm uses
three deep residual neural networks to model temporal closeness, period, and
trend properties of traffic flow. Each residual neural network consists of a
branch of residual convolutional units. DeepTFP aggregates the outputs of the
three residual neural networks to optimize the parameters of a time series
prediction model. Contrast experiments on mobile time series data from the
transportation system of England demonstrate that the proposed DeepTFP
outperforms the Long Short-Term Memory (LSTM) architecture based method in
prediction accuracy.
",Traffic Flow Prediction,traffic flow prediction
"  In this paper we propose a new Koopman operator approach to the decomposition
of nonlinear dynamical systems using Koopman Gramians. We introduce the notion
of an input-Koopman operator, and show how input-Koopman operators can be used
to cast a nonlinear system into the classical state-space form, and identify
conditions under which input and state observable functions are well separated.
We then extend an existing method of dynamic mode decomposition for learning
Koopman operators from data known as deep dynamic mode decomposition to systems
with controls or disturbances. We illustrate the accuracy of the method in
learning an input-state separable Koopman operator for an example system, even
when the underlying system exhibits mixed state-input terms. We next introduce
a nonlinear decomposition algorithm, based on Koopman Gramians, that maximizes
internal subsystem observability and disturbance rejection from unwanted noise
from other subsystems. We derive a relaxation based on Koopman Gramians and
multi-way partitioning for the resulting NP-hard decomposition problem. We
lastly illustrate the proposed algorithm with the swing dynamics for an IEEE
39-bus system.
",Koopman Operator-Based Decomposition for Nonlinear Dynamical Systems,nonlinear system decomposition
"  In this work, we propose a novel robot learning framework called Neural Task
Programming (NTP), which bridges the idea of few-shot learning from
demonstration and neural program induction. NTP takes as input a task
specification (e.g., video demonstration of a task) and recursively decomposes
it into finer sub-task specifications. These specifications are fed to a
hierarchical neural program, where bottom-level programs are callable
subroutines that interact with the environment. We validate our method in three
robot manipulation tasks. NTP achieves strong generalization across sequential
tasks that exhibit hierarchal and compositional structures. The experimental
results show that NTP learns to generalize well to- wards unseen tasks with
increasing lengths, variable topologies, and changing objectives.
",Robot Learning Frameworks,robot learning framework
"  Model pruning seeks to induce sparsity in a deep neural network's various
connection matrices, thereby reducing the number of nonzero-valued parameters
in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep
networks at the cost of only a marginal loss in accuracy and achieve a sizable
reduction in model size. This hints at the possibility that the baseline models
in these experiments are perhaps severely over-parameterized at the outset and
a viable alternative for model compression might be to simply reduce the number
of hidden units while maintaining the model's dense connection structure,
exposing a similar trade-off in model size and accuracy. We investigate these
two distinct paths for model compression within the context of energy-efficient
inference in resource-constrained environments and propose a new gradual
pruning technique that is simple and straightforward to apply across a variety
of models/datasets with minimal tuning and can be seamlessly incorporated
within the training process. We compare the accuracy of large, but pruned
models (large-sparse) and their smaller, but dense (small-dense) counterparts
with identical memory footprint. Across a broad range of neural network
architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find
large-sparse models to consistently outperform small-dense models and achieve
up to 10x reduction in number of non-zero parameters with minimal loss in
accuracy.
",Model Pruning Techniques,model pruning and compression
"  Deep learning methods are used on spectroscopic data to predict drug content
in tablets from near infrared (NIR) spectra. Using convolutional neural
networks (CNNs), features are ex- tracted from the spectroscopic data. Extended
multiplicative scatter correction (EMSC) and a novel spectral data augmentation
method are benchmarked as preprocessing steps. The learned models perform
better or on par with hypothetical optimal partial least squares (PLS) models
for all combinations of preprocessing. Data augmentation with subsequent EMSC
in combination gave the best results. The deep learning model CNNs also
outperform the PLS models in an extrapolation chal- lenge created using data
from a second instrument and from an analyte concentration not covered by the
training data. Qualitative investigations of the CNNs kernel activations show
their resemblance to wellknown data processing methods such as smoothing,
slope/derivative, thresholds and spectral region selection.
",Deep Learning for Spectroscopic Data Analysis,deep learning for drug content prediction in tablets using nir spectroscopy
"  Nowadays, the availability of large-scale data in disparate application
domains urges the deployment of sophisticated tools for extracting valuable
knowledge out of this huge bulk of information. In that vein, low-rank
representations (LRRs) which seek low-dimensional embeddings of data have
naturally appeared. In an effort to reduce computational complexity and improve
estimation performance, LRR has been viewed via a matrix factorization (MF)
perspective. Recently, low-rank MF (LRMF) approaches have been proposed for
tackling the inherent weakness of MF i.e., the unawareness of the dimension of
the low-dimensional space where data reside. Herein, inspired by the merits of
iterative reweighted schemes for rank minimization, we come up with a generic
low-rank promoting regularization function. Then, focusing on a specific
instance of it, we propose a regularizer that imposes column-sparsity jointly
on the two matrix factors that result from MF, thus promoting low-rankness on
the optimization problem. The problems of denoising, matrix completion and
non-negative matrix factorization (NMF) are redefined according to the new LRMF
formulation and solved via efficient Newton-type algorithms with proven
theoretical guarantees as to their convergence and rates of convergence to
stationary points. The effectiveness of the proposed algorithms is verified in
diverse simulated and real data experiments.
",Low-Rank Matrix Factorization,low-rank matrix factorization
"  Increasingly, Internet of Things (IoT) domains, such as sensor networks,
smart cities, and social networks, generate vast amounts of data. Such data are
not only unbounded and rapidly evolving. Rather, the content thereof
dynamically evolves over time, often in unforeseen ways. These variations are
due to so-called concept drifts, caused by changes in the underlying data
generation mechanisms. In a classification setting, concept drift causes the
previously learned models to become inaccurate, unsafe and even unusable.
Accordingly, concept drifts need to be detected, and handled, as soon as
possible. In medical applications and emergency response settings, for example,
change in behaviours should be detected in near real-time, to avoid potential
loss of life. To this end, we introduce the McDiarmid Drift Detection Method
(MDDM), which utilizes McDiarmid's inequality in order to detect concept drift.
The MDDM approach proceeds by sliding a window over prediction results, and
associate window entries with weights. Higher weights are assigned to the most
recent entries, in order to emphasize their importance. As instances are
processed, the detection algorithm compares a weighted mean of elements inside
the sliding window with the maximum weighted mean observed so far. A
significant difference between the two weighted means, upper-bounded by the
McDiarmid inequality, implies a concept drift. Our extensive experimentation
against synthetic and real-world data streams show that our novel method
outperforms the state-of-the-art. Specifically, MDDM yields shorter detection
delays as well as lower false negative rates, while maintaining high
classification accuracies.
",Concept Drift Detection in IoT Data Streams,concept drift detection in iot data streams
"  A Bernoulli Mixture Model (BMM) is a finite mixture of random binary vectors
with independent dimensions. The problem of clustering BMM data arises in a
variety of real-world applications, ranging from population genetics to
activity analysis in social networks. In this paper, we analyze the
clusterability of BMMs from a theoretical perspective, when the number of
clusters is unknown. In particular, we stipulate a set of conditions on the
sample complexity and dimension of the model in order to guarantee the Probably
Approximately Correct (PAC)-clusterability of a dataset. To the best of our
knowledge, these findings are the first non-asymptotic bounds on the sample
complexity of learning or clustering BMMs.
",Clustering Bernoulli Mixture Models,bernoulli mixture model clustering
"  A current challenge for data management systems is to support the
construction and maintenance of machine learning models over data that is
large, multi-dimensional, and evolving. While systems that could support these
tasks are emerging, the need to scale to distributed, streaming data requires
new models and algorithms. In this setting, as well as computational
scalability and model accuracy, we also need to minimize the amount of
communication between distributed processors, which is the chief component of
latency. We study Bayesian networks, the workhorse of graphical models, and
present a communication-efficient method for continuously learning and
maintaining a Bayesian network model over data that is arriving as a
distributed stream partitioned across multiple processors. We show a strategy
for maintaining model parameters that leads to an exponential reduction in
communication when compared with baseline approaches to maintain the exact MLE
(maximum likelihood estimation). Meanwhile, our strategy provides similar
prediction errors for the target distribution and for classification tasks.
",Distributed Bayesian Learning,distributed machine learning for scalable data management
"  Thompson Sampling algorithm is a well known Bayesian algorithm for solving
stochastic multi-armed bandit. At each time step the algorithm chooses each arm
with probability proportional to it being the current best arm. We modify the
strategy by introducing a paramter h which alters the importance of the
probability of an arm being the current best arm. We show that the optimality
of Thompson sampling is robust to this perturbation within a range of parameter
values for two arm bandits.
",Thompson Sampling with Perturbation Parameter,stochastic multi-armed bandit problem
"  Neural networks have been used prominently in several machine learning and
statistics applications. In general, the underlying optimization of neural
networks is non-convex which makes their performance analysis challenging. In
this paper, we take a novel approach to this problem by asking whether one can
constrain neural network weights to make its optimization landscape have good
theoretical properties while at the same time, be a good approximation for the
unconstrained one. For two-layer neural networks, we provide affirmative
answers to these questions by introducing Porcupine Neural Networks (PNNs)
whose weight vectors are constrained to lie over a finite set of lines. We show
that most local optima of PNN optimizations are global while we have a
characterization of regions where bad local optimizers may exist. Moreover, our
theoretical and empirical results suggest that an unconstrained neural network
can be approximated using a polynomially-large PNN.
",Neural Network Optimization,neural network optimization
"  Lifted Relational Neural Networks (LRNNs) describe relational domains using
weighted first-order rules which act as templates for constructing feed-forward
neural networks. While previous work has shown that using LRNNs can lead to
state-of-the-art results in various ILP tasks, these results depended on
hand-crafted rules. In this paper, we extend the framework of LRNNs with
structure learning, thus enabling a fully automated learning process. Similarly
to many ILP methods, our structure learning algorithm proceeds in an iterative
fashion by top-down searching through the hypothesis space of all possible Horn
clauses, considering the predicates that occur in the training examples as well
as invented soft concepts entailed by the best weighted rules found so far. In
the experiments, we demonstrate the ability to automatically induce useful
hierarchical soft concepts leading to deep LRNNs with a competitive predictive
power.
",Automated Structure Learning for Lifted Relational Neural Networks,neural networks for relational learning
"  Learning with recurrent neural networks (RNNs) on long sequences is a
notoriously difficult task. There are three major challenges: 1) complex
dependencies, 2) vanishing and exploding gradients, and 3) efficient
parallelization. In this paper, we introduce a simple yet effective RNN
connection structure, the DilatedRNN, which simultaneously tackles all of these
challenges. The proposed architecture is characterized by multi-resolution
dilated recurrent skip connections and can be combined flexibly with diverse
RNN cells. Moreover, the DilatedRNN reduces the number of parameters needed and
enhances training efficiency significantly, while matching state-of-the-art
performance (even with standard RNN cells) in tasks involving very long-term
dependencies. To provide a theory-based quantification of the architecture's
advantages, we introduce a memory capacity measure, the mean recurrent length,
which is more suitable for RNNs with long skip connections than existing
measures. We rigorously prove the advantages of the DilatedRNN over other
recurrent neural architectures. The code for our method is publicly available
at https://github.com/code-terminator/DilatedRNN
",DilatedRNN for Efficient Sequence Modeling,recurrent neural networks
"  The meteoric rise of deep learning models in computer vision research, having
achieved human-level accuracy in image recognition tasks is firm evidence of
the impact of representation learning of deep neural networks. In the chemistry
domain, recent advances have also led to the development of similar CNN models,
such as Chemception, that is trained to predict chemical properties using
images of molecular drawings. In this work, we investigate the effects of
systematically removing and adding localized domain-specific information to the
image channels of the training data. By augmenting images with only 3
additional basic information, and without introducing any architectural
changes, we demonstrate that an augmented Chemception (AugChemception)
outperforms the original model in the prediction of toxicity, activity, and
solvation free energy. Then, by altering the information content in the images,
and examining the resulting model's performance, we also identify two distinct
learning patterns in predicting toxicity/activity as compared to solvation free
energy. These patterns suggest that Chemception is learning about its tasks in
the manner that is consistent with established knowledge. Thus, our work
demonstrates that advanced chemical knowledge is not a pre-requisite for deep
learning models to accurately predict complex chemical properties.
",Deep Learning for Chemical Property Prediction,chemical property prediction using deep learning
"  We solve a system of ordinary differential equations with an unknown
functional form of a sink (reaction rate) term. We assume that the measurements
(time series) of state variables are partially available, and we use recurrent
neural network to ""learn"" the reaction rate from this data. This is achieved by
including a discretized ordinary differential equations as part of a recurrent
neural network training problem. We extend TensorFlow's recurrent neural
network architecture to create a simple but scalable and effective solver for
the unknown functions, and apply it to a fedbatch bioreactor simulation
problem. Use of techniques from recent deep learning literature enables
training of functions with behavior manifesting over thousands of time steps.
Our networks are structurally similar to recurrent neural networks, but
differences in design and function require modifications to the conventional
wisdom about training such networks.
",Neural Networks for ODE Solvers,neural networks for solving ordinary differential equations
"  Classification of sequence data is the topic of interest for dynamic Bayesian
models and Recurrent Neural Networks (RNNs). While the former can explicitly
model the temporal dependencies between class variables, the latter have a
capability of learning representations. Several attempts have been made to
improve performance by combining these two approaches or increasing the
processing capability of the hidden units in RNNs. This often results in
complex models with a large number of learning parameters. In this paper, a
compact model is proposed which offers both representation learning and
temporal inference of class variables by rolling Restricted Boltzmann Machines
(RBMs) and class variables over time. We address the key issue of
intractability in this variant of RBMs by optimising a conditional
distribution, instead of a joint distribution. Experiments reported in the
paper on melody modelling and optical character recognition show that the
proposed model can outperform the state-of-the-art. Also, the experimental
results on optical character recognition, part-of-speech tagging and text
chunking demonstrate that our model is comparable to recurrent neural networks
with complex memory gates while requiring far fewer parameters.
",Sequence Classification Models,sequence data classification
"  In this paper, we study two aspects of the variational autoencoder (VAE): the
prior distribution over the latent variables and its corresponding posterior.
First, we decompose the learning of VAEs into layerwise density estimation, and
argue that having a flexible prior is beneficial to both sample generation and
inference. Second, we analyze the family of inverse autoregressive flows
(inverse AF) and show that with further improvement, inverse AF could be used
as universal approximation to any complicated posterior. Our analysis results
in a unified approach to parameterizing a VAE, without the need to restrict
ourselves to use factorial Gaussians in the latent real space.
",Variational Autoencoder (VAE) Design Improvements,variational autoencoder (vae) topics
"  Recurrent neural networks have shown remarkable success in modeling
sequences. However low resource situations still adversely affect the
generalizability of these models. We introduce a new family of models, called
Lattice Recurrent Units (LRU), to address the challenge of learning deep
multi-layer recurrent models with limited resources. LRU models achieve this
goal by creating distinct (but coupled) flow of information inside the units: a
first flow along time dimension and a second flow along depth dimension. It
also offers a symmetry in how information can flow horizontally and vertically.
We analyze the effects of decoupling three different components of our LRU
model: Reset Gate, Update Gate and Projected State. We evaluate this family on
new LRU models on computational convergence rates and statistical efficiency.
Our experiments are performed on four publicly-available datasets, comparing
with Grid-LSTM and Recurrent Highway networks. Our results show that LRU has
better empirical computational convergence rates and statistical efficiency
values, along with learning more accurate language models.
",Deep Learning for Sequence Modeling,deep learning for low resource situations
"  The classification of time series data is a challenge common to all
data-driven fields. However, there is no agreement about which are the most
efficient techniques to group unlabeled time-ordered data. This is because a
successful classification of time series patterns depends on the goal and the
domain of interest, i.e. it is application-dependent.
  In this article, we study free-to-play game data. In this domain, clustering
similar time series information is increasingly important due to the large
amount of data collected by current mobile and web applications. We evaluate
which methods cluster accurately time series of mobile games, focusing on
player behavior data. We identify and validate several aspects of the
clustering: the similarity measures and the representation techniques to reduce
the high dimensionality of time series. As a robustness test, we compare
various temporal datasets of player activity from two free-to-play video-games.
  With these techniques we extract temporal patterns of player behavior
relevant for the evaluation of game events and game-business diagnosis. Our
experiments provide intuitive visualizations to validate the results of the
clustering and to determine the optimal number of clusters. Additionally, we
assess the common characteristics of the players belonging to the same group.
This study allows us to improve the understanding of player dynamics and churn
behavior.
",Time Series Clustering in Mobile Gaming,time series clustering for player behavior analysis in free-to-play games
"  Feature representations from pre-trained deep neural networks have been known
to exhibit excellent generalization and utility across a variety of related
tasks. Fine-tuning is by far the simplest and most widely used approach that
seeks to exploit and adapt these feature representations to novel tasks with
limited data. Despite the effectiveness of fine-tuning, itis often sub-optimal
and requires very careful optimization to prevent severe over-fitting to small
datasets. The problem of sub-optimality and over-fitting, is due in part to the
large number of parameters used in a typical deep convolutional neural network.
To address these problems, we propose a simple yet effective regularization
method for fine-tuning pre-trained deep networks for the task of k-shot
learning. To prevent overfitting, our key strategy is to cluster the model
parameters while ensuring intra-cluster similarity and inter-cluster diversity
of the parameters, effectively regularizing the dimensionality of the parameter
search space. In particular, we identify groups of neurons within each layer of
a deep network that shares similar activation patterns. When the network is to
be fine-tuned for a classification task using only k examples, we propagate a
single gradient to all of the neuron parameters that belong to the same group.
The grouping of neurons is non-trivial as neuron activations depend on the
distribution of the input data. To efficiently search for optimal groupings
conditioned on the input data, we propose a reinforcement learning search
strategy using recurrent networks to learn the optimal group assignments for
each network layer. Experimental results show that our method can be easily
applied to several popular convolutional neural networks and improve upon other
state-of-the-art fine-tuning based k-shot learning strategies by more than10%
",Regularization Methods for Fine-Tuning Pre-Trained Deep Networks,regularization methods for fine-tuning pre-trained deep neural networks
"  Recognizing objects in natural images is an intricate problem involving
multiple conflicting objectives. Deep convolutional neural networks, trained on
large datasets, achieve convincing results and are currently the
state-of-the-art approach for this task. However, the long time needed to train
such deep networks is a major drawback. We tackled this problem by reusing a
previously trained network. For this purpose, we first trained a deep
convolutional network on the ILSVRC2012 dataset. We then maintained the learned
convolution kernels and only retrained the classification part on different
datasets. Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100,
compared to the previous state-of-the-art result of 65.43 %. Furthermore, our
findings indicate that convolutional networks are able to learn generic feature
extractors that can be used for different tasks.
",Object Recognition using Pre-Trained Neural Networks,object recognition in natural images
"  The deep reinforcement learning community has made several independent
improvements to the DQN algorithm. However, it is unclear which of these
extensions are complementary and can be fruitfully combined. This paper
examines six extensions to the DQN algorithm and empirically studies their
combination. Our experiments show that the combination provides
state-of-the-art performance on the Atari 2600 benchmark, both in terms of data
efficiency and final performance. We also provide results from a detailed
ablation study that shows the contribution of each component to overall
performance.
",Deep Reinforcement Learning Algorithm Extensions,deep reinforcement learning
"  Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned
problem. We observe that the scaling-based weight space symmetry property in
rectified nonlinear network will cause this negative effect. Therefore, we
propose to constrain the incoming weights of each neuron to be unit-norm, which
is formulated as an optimization problem over Oblique manifold. A simple yet
efficient method referred to as projection based weight normalization (PBWN) is
also developed to solve this problem. PBWN executes standard gradient updates,
followed by projecting the updated weight back to Oblique manifold. This
proposed method has the property of regularization and collaborates well with
the commonly used batch normalization technique. We conduct comprehensive
experiments on several widely-used image datasets including CIFAR-10,
CIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art
convolutional neural networks, such as Inception, VGG and residual networks.
The results show that our method is able to improve the performance of DNNs
with different architectures consistently. We also apply our method to Ladder
network for semi-supervised learning on permutation invariant MNIST dataset,
and our method outperforms the state-of-the-art methods: we obtain test errors
as 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples,
respectively.
",Weight Normalization Techniques for Deep Neural Networks,deep neural network optimization
"  This work addresses the instability in asynchronous data parallel
optimization. It does so by introducing a novel distributed optimizer which is
able to efficiently optimize a centralized model under communication
constraints. The optimizer achieves this by pushing a normalized sequence of
first-order gradients to a parameter server. This implies that the magnitude of
a worker delta is smaller compared to an accumulated gradient, and provides a
better direction towards a minimum compared to first-order gradients, which in
turn also forces possible implicit momentum fluctuations to be more aligned
since we make the assumption that all workers contribute towards a single
minima. As a result, our approach mitigates the parameter staleness problem
more effectively since staleness in asynchrony induces (implicit) momentum, and
achieves a better convergence rate compared to other optimizers such as
asynchronous EASGD and DynSGD, which we show empirically.
",Distributed Optimization in Asynchronous Data Parallelism,asynchronous distributed optimization
"  Deep networks trained on demonstrations of human driving have learned to
follow roads and avoid obstacles. However, driving policies trained via
imitation learning cannot be controlled at test time. A vehicle trained
end-to-end to imitate an expert cannot be guided to take a specific turn at an
upcoming intersection. This limits the utility of such systems. We propose to
condition imitation learning on high-level command input. At test time, the
learned driving policy functions as a chauffeur that handles sensorimotor
coordination but continues to respond to navigational commands. We evaluate
different architectures for conditional imitation learning in vision-based
driving. We conduct experiments in realistic three-dimensional simulations of
urban driving and on a 1/5 scale robotic truck that is trained to drive in a
residential area. Both systems drive based on visual input yet remain
responsive to high-level navigational commands. The supplementary video can be
viewed at https://youtu.be/cFtnflNe5fM
",Conditional Imitation Learning for Autonomous Vehicles,conditional imitation learning in autonomous driving
"  We describe two recently proposed machine learning approaches for discovering
emerging trends in fatal accidental drug overdoses. The Gaussian Process Subset
Scan enables early detection of emerging patterns in spatio-temporal data,
accounting for both the non-iid nature of the data and the fact that detecting
subtle patterns requires integration of information across multiple spatial
areas and multiple time steps. We apply this approach to 17 years of
county-aggregated data for monthly opioid overdose deaths in the New York City
metropolitan area, showing clear advantages in the utility of discovered
patterns as compared to typical anomaly detection approaches.
  To detect and characterize emerging overdose patterns that differentially
affect a subpopulation of the data, including geographic, demographic, and
behavioral patterns (e.g., which combinations of drugs are involved), we apply
the Multidimensional Tensor Scan to 8 years of case-level overdose data from
Allegheny County, PA. We discover previously unidentified overdose patterns
which reveal unusual demographic clusters, show impacts of drug legislation,
and demonstrate potential for early detection and targeted intervention. These
approaches to early detection of overdose patterns can inform prevention and
response efforts, as well as understanding the effects of policy changes.
",Machine Learning for Drug Overdose Detection,machine learning for identifying emerging trends in fatal accidental drug overdoses
"  We present an approach for mobile robots to learn to navigate in dynamic
environments with pedestrians via raw depth inputs, in a socially compliant
manner. To achieve this, we adopt a generative adversarial imitation learning
(GAIL) strategy, which improves upon a pre-trained behavior cloning policy. Our
approach overcomes the disadvantages of previous methods, as they heavily
depend on the full knowledge of the location and velocity information of nearby
pedestrians, which not only requires specific sensors, but also the extraction
of such state information from raw sensory input could consume much computation
time. In this paper, our proposed GAIL-based model performs directly on raw
depth inputs and plans in real-time. Experiments show that our GAIL-based
approach greatly improves the safety and efficiency of the behavior of mobile
robots from pure behavior cloning. The real-world deployment also shows that
our method is capable of guiding autonomous vehicles to navigate in a socially
compliant manner directly through raw depth inputs. In addition, we release a
simulation plugin for modeling pedestrian behaviors based on the social force
model.
",Socially Compliant Robot Navigation,socially compliant mobile robot navigation
"  The increasing illegal parking has become more and more serious. Nowadays the
methods of detecting illegally parked vehicles are based on background
segmentation. However, this method is weakly robust and sensitive to
environment. Benefitting from deep learning, this paper proposes a novel
illegal vehicle parking detection system. Illegal vehicles captured by camera
are firstly located and classified by the famous Single Shot MultiBox Detector
(SSD) algorithm. To improve the performance, we propose to optimize SSD by
adjusting the aspect ratio of default box to accommodate with our dataset
better. After that, a tracking and analysis of movement is adopted to judge the
illegal vehicles in the region of interest (ROI). Experiments show that the
system can achieve a 99% accuracy and real-time (25FPS) detection with strong
robustness in complex environments.
",Illegal Vehicle Parking Detection,illegal vehicle parking detection using deep learning
"  A falling rule list is a probabilistic decision list for binary
classification, consisting of a series of if-then rules with antecedents in the
if clauses and probabilities of the desired outcome (""1"") in the then clauses.
Just as in a regular decision list, the order of rules in a falling rule list
is important -- each example is classified by the first rule whose antecedent
it satisfies. Unlike a regular decision list, a falling rule list requires the
probabilities of the desired outcome (""1"") to be monotonically decreasing down
the list. We propose an optimization approach to learning falling rule lists
and ""softly"" falling rule lists, along with Monte-Carlo search algorithms that
use bounds on the optimal solution to prune the search space.
",Falling Rule Lists Optimization,machine learning - classification algorithms
"  Under a Bayesian framework, we formulate the fully sequential sampling and
selection decision in statistical ranking and selection as a stochastic control
problem, and derive the associated Bellman equation. Using value function
approximation, we derive an approximately optimal allocation policy. We show
that this policy is not only computationally efficient but also possesses both
one-step-ahead and asymptotic optimality for independent normal sampling
distributions. Moreover, the proposed allocation policy is easily generalizable
in the approximate dynamic programming paradigm.
",Bayesian Ranking and Selection,stochastic control and optimization in statistical ranking and selection
"  Current topic models often suffer from discovering topics not matching human
intuition, unnatural switching of topics within documents and high
computational demands. We address these concerns by proposing a topic model and
an inference algorithm based on automatically identifying characteristic
keywords for topics. Keywords influence topic-assignments of nearby words. Our
algorithm learns (key)word-topic scores and it self-regulates the number of
topics. Inference is simple and easily parallelizable. Qualitative analysis
yields comparable results to state-of-the-art models (eg. LDA), but with
different strengths and weaknesses. Quantitative analysis using 9 datasets
shows gains in terms of classification accuracy, PMI score, computational
performance and consistency of topic assignments within documents, while most
often using less topics.
",Topic Modeling Optimization,improving topic modeling
"  A key task in Bayesian statistics is sampling from distributions that are
only specified up to a partition function (i.e., constant of proportionality).
However, without any assumptions, sampling (even approximately) can be #P-hard,
and few works have provided ""beyond worst-case"" guarantees for such settings.
  For log-concave distributions, classical results going back to Bakry and
\'Emery (1985) show that natural continuous-time Markov chains called Langevin
diffusions mix in polynomial time. The most salient feature of log-concavity
violated in practice is uni-modality: commonly, the distributions we wish to
sample from are multi-modal. In the presence of multiple deep and
well-separated modes, Langevin diffusion suffers from torpid mixing.
  We address this problem by combining Langevin diffusion with simulated
tempering. The result is a Markov chain that mixes more rapidly by
transitioning between different temperatures of the distribution. We analyze
this Markov chain for the canonical multi-modal distribution: a mixture of
gaussians (of equal variance). The algorithm based on our Markov chain provably
samples from distributions that are close to mixtures of gaussians, given
access to the gradient of the log-pdf. For the analysis, we use a spectral
decomposition theorem for graphs (Gharan and Trevisan, 2014) and a Markov chain
decomposition technique (Madras and Randall, 2002).
",Sampling from Multi-modal Distributions,efficient sampling from multi-modal distributions
"  We present a new clustering algorithm that is based on searching for natural
gaps in the components of the lowest energy eigenvectors of the Laplacian of a
graph. In comparing the performance of the proposed method with a set of other
popular methods (KMEANS, spectral-KMEANS, and an agglomerative method) in the
context of the Lancichinetti-Fortunato-Radicchi (LFR) Benchmark for undirected
weighted overlapping networks, we find that the new method outperforms the
other spectral methods considered in certain parameter regimes. Finally, in an
application to climate data involving one of the most important modes of
interannual climate variability, the El Nino Southern Oscillation phenomenon,
we demonstrate the ability of the new algorithm to readily identify different
flavors of the phenomenon.
",Graph-based Clustering Algorithms,clustering algorithms for network analysis
"  Proteins are the main workhorses of biological functions in a cell, a tissue,
or an organism. Identification and quantification of proteins in a given
sample, e.g. a cell type under normal/disease conditions, are fundamental tasks
for the understanding of human health and disease. In this paper, we present
DeepNovo, a deep learning-based tool to address the problem of protein
identification from tandem mass spectrometry data. The idea was first proposed
in the context of de novo peptide sequencing [1] in which convolutional neural
networks and recurrent neural networks were applied to predict the amino acid
sequence of a peptide from its spectrum, a similar task to generating a caption
from an image. We further develop DeepNovo to perform sequence database search,
the main technique for peptide identification that greatly benefits from
numerous existing protein databases. We combine two modules de novo sequencing
and database search into a single deep learning framework for peptide
identification, and integrate de Bruijn graph assembly technique to offer a
complete solution to reconstruct protein sequences from tandem mass
spectrometry data. This paper describes a comprehensive protocol of DeepNovo
for protein identification, including training neural network models, dynamic
programming search, database querying, estimation of false discovery rate, and
de Bruijn graph assembly. Training and testing data, model implementations, and
comprehensive tutorials in form of IPython notebooks are available in our
GitHub repository (https://github.com/nh2tran/DeepNovo).
",Protein Identification Using Deep Learning,protein identification from mass spectrometry data
"  We propose a novel Bayesian approach to modelling nonlinear alignments of
time series based on latent shared information. We apply the method to the
real-world problem of finding common structure in the sensor data of wind
turbines introduced by the underlying latent and turbulent wind field. The
proposed model allows for both arbitrary alignments of the inputs and
non-parametric output warpings to transform the observations. This gives rise
to multiple deep Gaussian process models connected via latent generating
processes. We present an efficient variational approximation based on nested
variational compression and show how the model can be used to extract shared
information between dependent time series, recovering an interpretable
functional decomposition of the learning problem. We show results for an
artificial data set and real-world data of two wind turbines.
",Time Series Alignment Modelling,nonlinear time series alignment and shared information modeling
"  We consider the problem of classifying business process instances based on
structural features derived from event logs. The main motivation is to provide
machine learning based techniques with quick response times for interactive
computer assisted root cause analysis. In particular, we create structural
features from process mining such as activity and transition occurrence counts,
and ordering of activities to be evaluated as potential features for
classification. We show that adding such structural features increases the
amount of information thus potentially increasing classification accuracy.
However, there is an inherent trade-off as using too many features leads to too
long run-times for machine learning classification models. One way to improve
the machine learning algorithms' run-time is to only select a small number of
features by a feature selection algorithm. However, the run-time required by
the feature selection algorithm must also be taken into account. Also, the
classification accuracy should not suffer too much from the feature selection.
The main contributions of this paper are as follows: First, we propose and
compare six different feature selection algorithms by means of an experimental
setup comparing their classification accuracy and achievable response times.
Second, we discuss the potential use of feature selection results for computer
assisted root cause analysis as well as the properties of different types of
structural features in the context of feature selection.
",Feature Selection for Business Process Classification,process mining and feature selection
"  We have witnessed the discovery of many techniques for network representation
learning in recent years, ranging from encoding the context in random walks to
embedding the lower order connections, to finding latent space representations
with auto-encoders. However, existing techniques are looking mostly into the
local structures in a network, while higher-level properties such as global
community structures are often neglected. We propose a novel network
representations learning model framework called RUM (network Representation
learning throUgh Multi-level structural information preservation). In RUM, we
incorporate three essential aspects of a node that capture a network's
characteristics in multiple levels: a node's affiliated local triads, its
neighborhood relationships, and its global community affiliations. Therefore
the framework explicitly and comprehensively preserves the structural
information of a network, extending the encoding process both to the local end
of the structural information spectrum and to the global end. The framework is
also flexible enough to take various community discovery algorithms as its
preprocessor. Empirical results show that the representations learned by RUM
have demonstrated substantial performance advantages in real-life tasks.
",Network Representation Learning,network representation learning
"  This paper aims to develop a new and robust approach to feature
representation. Motivated by the success of Auto-Encoders, we first theoretical
summarize the general properties of all algorithms that are based on
traditional Auto-Encoders: 1) The reconstruction error of the input can not be
lower than a lower bound, which can be viewed as a guiding principle for
reconstructing the input. Additionally, when the input is corrupted with
noises, the reconstruction error of the corrupted input also can not be lower
than a lower bound. 2) The reconstruction of a hidden representation achieving
its ideal situation is the necessary condition for the reconstruction of the
input to reach the ideal state. 3) Minimizing the Frobenius norm of the
Jacobian matrix of the hidden representation has a deficiency and may result in
a much worse local optimum value. We believe that minimizing the reconstruction
error of the hidden representation is more robust than minimizing the Frobenius
norm of the Jacobian matrix of the hidden representation. Based on the above
analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs),
which uses corruption and reconstruction on both the input and the hidden
representation. We demonstrate that the proposed model is highly flexible and
extensible and has a potentially better capability to learn invariant and
robust feature representations. We also show that our model is more robust than
Denoising Auto-Encoders (DAEs) for dealing with noises or inessential features.
Furthermore, we detail how to train DDAEs with two different pre-training
methods by optimizing the objective function in a combined and separate manner,
respectively. Comparative experiments illustrate that the proposed model is
significantly better for representation learning than the state-of-the-art
models.
",Autoencoder-based Feature Representation,auto-encoder based feature representation
"  In this paper, we propose an information-theoretic exploration strategy for
stochastic, discrete multi-armed bandits that achieves optimal regret. Our
strategy is based on the value of information criterion. This criterion
measures the trade-off between policy information and obtainable rewards. High
amounts of policy information are associated with exploration-dominant searches
of the space and yield high rewards. Low amounts of policy information favor
the exploitation of existing knowledge. Information, in this criterion, is
quantified by a parameter that can be varied during search. We demonstrate that
a simulated-annealing-like update of this parameter, with a sufficiently fast
cooling schedule, leads to an optimal regret that is logarithmic with respect
to the number of episodes.
",Multi-Armed Bandits Exploration Strategy,multi-armed bandit problem
"  This paper presents a deep learning framework that is capable of solving
partially observable locomotion tasks based on our novel interpretation of
Recurrent Deterministic Policy Gradient (RDPG). We study on bias of sampled
error measure and its variance induced by the partial observability of
environment and subtrajectory sampling, respectively. Three major improvements
are introduced in our RDPG based learning framework: tail-step bootstrap of
interpolated temporal difference, initialisation of hidden state using past
trajectory scanning, and injection of external experiences learned by other
agents. The proposed learning framework was implemented to solve the
Bipedal-Walker challenge in OpenAI's gym simulation environment where only
partial state information is available. Our simulation study shows that the
autonomous behaviors generated by the RDPG agent are highly adaptive to a
variety of obstacles and enables the agent to effectively traverse rugged
terrains for long distance with higher success rate than leading contenders.
",Partially Observable Reinforcement Learning,reinforcement learning for partially observable locomotion tasks
"  The lack of interpretability often makes black-box models difficult to be
applied to many practical domains. For this reason, the current work, from the
black-box model input port, proposes to incorporate data-based prior
information into the black-box soft-margin SVM model to enhance its
interpretability. The concept and incorporation mechanism of data-based prior
information are successively developed, based on which the interpretable or
partly interpretable SVM optimization model is designed and then solved through
handily rewriting the optimization problem as a nonlinear quadratic programming
problem. An algorithm for mining data-based linear prior information from data
set is also proposed, which generates a linear expression with respect to two
appropriate inputs identified from all inputs of system. At last, the proposed
interpretability enhancement strategy is applied to eight benchmark examples
for effectiveness exhibition.
",Interpretable Machine Learning Models,improving interpretability of black-box models
"  Since the invention of word2vec, the skip-gram model has significantly
advanced the research of network embedding, such as the recent emergence of the
DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of
the aforementioned models with negative sampling can be unified into the matrix
factorization framework with closed forms. Our analysis and proofs reveal that:
(1) DeepWalk empirically produces a low-rank transformation of a network's
normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk
when the size of vertices' context is set to one; (3) As an extension of LINE,
PTE can be viewed as the joint factorization of multiple networks' Laplacians;
(4) node2vec is factorizing a matrix related to the stationary distribution and
transition probability tensor of a 2nd-order random walk. We further provide
the theoretical connections between skip-gram based network embedding
algorithms and the theory of graph Laplacian. Finally, we present the NetMF
method as well as its approximation algorithm for computing network embedding.
Our method offers significant improvements over DeepWalk and LINE for
conventional network mining tasks. This work lays the theoretical foundation
for skip-gram based network embedding methods, leading to a better
understanding of latent network representation learning.
",Unifying Network Embedding Algorithms,network embedding
"  Trajectory optimization and posture generation are hard problems in robot
locomotion, which can be non-convex and have multiple local optima. Progress on
these problems is further hindered by a lack of open benchmarks, since
comparisons of different solutions are difficult to make. In this paper we
introduce a new benchmark for trajectory optimization and posture generation of
legged robots, using a pre-defined scenario, robot and constraints, as well as
evaluation criteria. We evaluate state-of-the-art trajectory optimization
algorithms based on sequential quadratic programming (SQP) on the benchmark, as
well as new stochastic and incremental optimization methods borrowed from the
large-scale machine learning literature. Interestingly we show that some of
these stochastic and incremental methods, which are based on stochastic
gradient descent (SGD), achieve higher success rates than SQP on tough
initializations. Inspired by this observation we also propose a new incremental
variant of SQP which updates only a random subset of the costs and constraints
at each iteration. The algorithm is the best performing in both success rate
and convergence speed, improving over SQP by up to 30% in both criteria. The
benchmark's resources and a solution evaluation script are made openly
available.
",Trajectory Optimization in Legged Robots,robot trajectory optimization
"  Many real-world networks are complex dynamical systems, where both local
(e.g., changing node attributes) and global (e.g., changing network topology)
processes unfold over time. Local dynamics may provoke global changes in the
network, and the ability to detect such effects could have profound
implications for a number of real-world problems. Most existing techniques
focus individually on either local or global aspects of the problem or treat
the two in isolation from each other. In this paper we propose a novel network
model that simultaneously accounts for both local and global dynamics. To the
best of our knowledge, this is the first attempt at modeling and detecting
local and global change points on dynamic networks via a unified generative
framework. Our model is built upon the popular mixed membership stochastic
blockmodels (MMSB) with sparse co-evolving patterns. We derive an efficient
stochastic gradient Langevin dynamics (SGLD) sampler for our proposed model,
which allows it to scale to potentially very large networks. Finally, we
validate our model on both synthetic and real-world data and demonstrate its
superiority over several baselines.
",Dynamic Network Modeling,modeling local and global dynamics in complex networks
"  We propose Embedding Propagation (EP), an unsupervised learning framework for
graph-structured data. EP learns vector representations of graphs by passing
two types of messages between neighboring nodes. Forward messages consist of
label representations such as representations of words and other attributes
associated with the nodes. Backward messages consist of gradients that result
from aggregating the label representations and applying a reconstruction loss.
Node representations are finally computed from the representation of their
labels. With significantly fewer parameters and hyperparameters an instance of
EP is competitive with and often outperforms state of the art unsupervised and
semi-supervised learning methods on a range of benchmark data sets.
",Graph Embedding,graph representation learning
"  Trained recurrent networks are powerful tools for modeling dynamic neural
computations. We present a target-based method for modifying the full
connectivity matrix of a recurrent network to train it to perform tasks
involving temporally complex input/output transformations. The method
introduces a second network during training to provide suitable ""target""
dynamics useful for performing the task. Because it exploits the full recurrent
connectivity, the method produces networks that perform tasks with fewer
neurons and greater noise robustness than traditional least-squares (FORCE)
approaches. In addition, we show how introducing additional input signals into
the target-generating network, which act as task hints, greatly extends the
range of tasks that can be learned and provides control over the complexity and
nature of the dynamics of the trained, task-performing network.
",Recurrent Network Training,recurrent neural network training method
"  We study the problem of formal verification of Binarized Neural Networks
(BNN), which have recently been proposed as a energy-efficient alternative to
traditional learning networks. The verification of BNNs, using the reduction to
hardware verification, can be even more scalable by factoring computations
among neurons within the same layer. By proving the NP-hardness of finding
optimal factoring as well as the hardness of PTAS approximability, we design
polynomial-time search heuristics to generate factoring solutions. The overall
framework allows applying verification techniques to moderately-sized BNNs for
embedded devices with thousands of neurons and inputs.
",Formal Verification of Binarized Neural Networks,formal verification of binarized neural networks
"  The rapid emergence of high-dimensional data in various areas has brought new
challenges to current ensemble clustering research. To deal with the curse of
dimensionality, recently considerable efforts in ensemble clustering have been
made by means of different subspace-based techniques. However, besides the
emphasis on subspaces, rather limited attention has been paid to the potential
diversity in similarity/dissimilarity metrics. It remains a surprisingly open
problem in ensemble clustering how to create and aggregate a large population
of diversified metrics, and furthermore, how to jointly investigate the
multi-level diversity in the large populations of metrics, subspaces, and
clusters in a unified framework. To tackle this problem, this paper proposes a
novel multidiversified ensemble clustering approach. In particular, we create a
large number of diversified metrics by randomizing a scaled exponential
similarity kernel, which are then coupled with random subspaces to form a large
set of metric-subspace pairs. Based on the similarity matrices derived from
these metric-subspace pairs, an ensemble of diversified base clusterings can
thereby be constructed. Further, an entropy-based criterion is utilized to
explore the cluster-wise diversity in ensembles, based on which three specific
ensemble clustering algorithms are presented by incorporating three types of
consensus functions. Extensive experiments are conducted on 30 high-dimensional
datasets, including 18 cancer gene expression datasets and 12 image/speech
datasets, which demonstrate the superiority of our algorithms over the
state-of-the-art. The source code is available at
https://github.com/huangdonghere/MDEC.
",Ensemble Clustering with Diversified Metrics,multidiversified ensemble clustering for high-dimensional data
"  Random Projection is a foundational research topic that connects a bunch of
machine learning algorithms under a similar mathematical basis. It is used to
reduce the dimensionality of the dataset by projecting the data points
efficiently to a smaller dimensions while preserving the original relative
distance between the data points. In this paper, we are intended to explain
random projection method, by explaining its mathematical background and
foundation, the applications that are currently adopting it, and an overview on
its current research perspective.
",Dimensionality Reduction Techniques,machine learning algorithms
"  Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.
",Fairness in Machine Learning Predictions,fairness in machine learning
"  With the advent of Big Data, nowadays in many applications databases
containing large quantities of similar time series are available. Forecasting
time series in these domains with traditional univariate forecasting procedures
leaves great potentials for producing accurate forecasts untapped. Recurrent
neural networks (RNNs), and in particular Long Short-Term Memory (LSTM)
networks, have proven recently that they are able to outperform
state-of-the-art univariate time series forecasting methods in this context
when trained across all available time series. However, if the time series
database is heterogeneous, accuracy may degenerate, so that on the way towards
fully automatic forecasting methods in this space, a notion of similarity
between the time series needs to be built into the methods. To this end, we
present a prediction model that can be used with different types of RNN models
on subgroups of similar time series, which are identified by time series
clustering techniques. We assess our proposed methodology using LSTM networks,
a widely popular RNN variant. Our method achieves competitive results on
benchmarking datasets under competition evaluation procedures. In particular,
in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM
model and outperforms all other methods on the CIF2016 forecasting competition
dataset.
",Time Series Forecasting with RNNs,time series forecasting
"  In this paper we propose a function space approach to Representation Learning
and the analysis of the representation layers in deep learning architectures.
We show how to compute a weak-type Besov smoothness index that quantifies the
geometry of the clustering in the feature space. This approach was already
applied successfully to improve the performance of machine learning algorithms
such as the Random Forest and tree-based Gradient Boosting. Our experiments
demonstrate that in well-known and well-performing trained networks, the Besov
smoothness of the training set, measured in the corresponding hidden layer
feature map representation, increases from layer to layer. We also contribute
to the understanding of generalization by showing how the Besov smoothness of
the representations, decreases as we add more mis-labeling to the training
data. We hope this approach will contribute to the de-mystification of some
aspects of deep learning.
",Besov Smoothness in Deep Learning,representation learning in deep learning architectures
"  We present the checkpoint ensembles method that can learn ensemble models on
a single training process. Although checkpoint ensembles can be applied to any
parametric iterative learning technique, here we focus on neural networks.
Neural networks' composable and simple neurons make it possible to capture many
individual and interaction effects among features. However, small sample sizes
and sampling noise may result in patterns in the training data that are not
representative of the true relationship between the features and the outcome.
As a solution, regularization during training is often used (e.g. dropout).
However, regularization is no panacea -- it does not perfectly address
overfitting. Even with methods like dropout, two methodologies are commonly
used in practice. First is to utilize a validation set independent to the
training set as a way to decide when to stop training. Second is to use
ensemble methods to further reduce overfitting and take advantage of local
optima (i.e. averaging over the predictions of several models). In this paper,
we explore checkpoint ensembles -- a simple technique that combines these two
ideas in one training process. Checkpoint ensembles improve performance by
averaging the predictions from ""checkpoints"" of the best models within single
training process. We use three real-world data sets -- text, image, and
electronic health record data -- using three prediction models: a vanilla
neural network, a convolutional neural network, and a long short term memory
network to show that checkpoint ensembles outperform existing methods: a method
that selects a model by minimum validation score, and two methods that average
models by weights. Our results also show that checkpoint ensembles capture a
portion of the performance gains that traditional ensembles provide.
",Ensemble Methods in Neural Networks,checkpoint ensembles for neural network training
"  Many applications infer the structure of a probabilistic graphical model from
data to elucidate the relationships between variables. But how can we train
graphical models on a massive data set? In this paper, we show how to construct
coresets -compressed data sets which can be used as proxy for the original data
and have provably bounded worst case error- for Gaussian dependency networks
(DNs), i.e., cyclic directed graphical models over Gaussians, where the parents
of each variable are its Markov blanket. Specifically, we prove that Gaussian
DNs admit coresets of size independent of the size of the data set.
Unfortunately, this does not extend to DNs over members of the exponential
family in general. As we will prove, Poisson DNs do not admit small coresets.
Despite this worst-case result, we will provide an argument why our coreset
construction for DNs can still work well in practice on count data. To
corroborate our theoretical results, we empirically evaluated the resulting
Core DNs on real data sets. The results
",Graphical Model Learning from Large Datasets,coreset construction for gaussian dependency networks
"  While all kinds of mixed data -from personal data, over panel and scientific
data, to public and commercial data- are collected and stored, building
probabilistic graphical models for these hybrid domains becomes more difficult.
Users spend significant amounts of time in identifying the parametric form of
the random variables (Gaussian, Poisson, Logit, etc.) involved and learning the
mixed models. To make this difficult task easier, we propose the first
trainable probabilistic deep architecture for hybrid domains that features
tractable queries. It is based on Sum-Product Networks (SPNs) with piecewise
polynomial leave distributions together with novel nonparametric decomposition
and conditioning steps using the Hirschfeld-Gebelein-R\'enyi Maximum
Correlation Coefficient. This relieves the user from deciding a-priori the
parametric form of the random variables but is still expressive enough to
effectively approximate any continuous distribution and permits efficient
learning and inference. Our empirical evidence shows that the architecture,
called Mixed SPNs, can indeed capture complex distributions across a wide range
of hybrid domains.
",Probabilistic Deep Architecture for Hybrid Data Modeling,mixed data modeling with deep architectures
"  Massive Open Online Courses (MOOCs) are attracting the attention of people
all over the world. Regardless the platform, numbers of registrants for online
courses are impressive but in the same time, completion rates are
disappointing. Understanding the mechanisms of dropping out based on the
learner profile arises as a crucial task in MOOCs, since it will allow
intervening at the right moment in order to assist the learner in completing
the course. In this paper, the dropout behaviour of learners in a MOOC is
thoroughly studied by first extracting features that describe the behavior of
learners within the course and then by comparing three classifiers (Logistic
Regression, Random Forest and AdaBoost) in two tasks: predicting which users
will have dropped out by a certain week and predicting which users will drop
out on a specific week. The former has showed to be considerably easier, with
all three classifiers performing equally well. However, the accuracy for the
second task is lower, and Logistic Regression tends to perform slightly better
than the other two algorithms. We found that features that reflect an active
attitude of the user towards the MOOC, such as submitting their assignment,
posting on the Forum and filling their Profile, are strong indicators of
persistence.
",Predicting MOOC Dropout Rates,mooc dropout prediction
"  Deep neural networks have been remarkable successful in various AI tasks but
often cast high computation and energy cost for energy-constrained applications
such as mobile sensing. We address this problem by proposing a novel framework
that optimizes the prediction accuracy and energy cost simultaneously, thus
enabling effective cost-accuracy trade-off at test time. In our framework, each
data instance is pushed into a cascade of deep neural networks with increasing
sizes, and a selection module is used to sequentially determine when a
sufficiently accurate classifier can be used for this data instance. The
cascade of neural networks and the selection module are jointly trained in an
end-to-end fashion by the REINFORCE algorithm to optimize a trade-off between
the computational cost and the predictive accuracy. Our method is able to
simultaneously improve the accuracy and efficiency by learning to assign easy
instances to fast yet sufficiently accurate classifiers to save computation and
energy cost, while assigning harder instances to deeper and more powerful
classifiers to ensure satisfiable accuracy. With extensive experiments on
several image classification datasets using cascaded ResNet classifiers, we
demonstrate that our method outperforms the standard well-trained ResNets in
accuracy but only requires less than 20% and 50% FLOPs cost on the CIFAR-10/100
datasets and 66% on the ImageNet dataset, respectively.
",Energy-Efficient Deep Learning,efficient deep learning for energy-constrained applications
"  Monotonic policy improvement and off-policy learning are two main desirable
properties for reinforcement learning algorithms. In this paper, by lower
bounding the performance difference of two policies, we show that the monotonic
policy improvement is guaranteed from on- and off-policy mixture samples. An
optimization procedure which applies the proposed bound can be regarded as an
off-policy natural policy gradient method. In order to support the theoretical
result, we provide a trust region policy optimization method using experience
replay as a naive application of our bound, and evaluate its performance in two
classical benchmark problems.
",Monotonic Policy Improvement in Reinforcement Learning,reinforcement learning
"  In several domains obtaining class annotations is expensive while at the same
time unlabelled data are abundant. While most semi-supervised approaches
enforce restrictive assumptions on the data distribution, recent work has
managed to learn semi-supervised models in a non-restrictive regime. However,
so far such approaches have only been proposed for linear models. In this work,
we introduce semi-supervised parameter learning for Sum-Product Networks
(SPNs). SPNs are deep probabilistic models admitting inference in linear time
in number of network edges. Our approach has several advantages, as it (1)
allows generative and discriminative semi-supervised learning, (2) guarantees
that adding unlabelled data can increase, but not degrade, the performance
(safe), and (3) is computationally efficient and does not enforce restrictive
assumptions on the data distribution. We show on a variety of data sets that
safe semi-supervised learning with SPNs is competitive compared to
state-of-the-art and can lead to a better generative and discriminative
objective value than a purely supervised approach.
",Semi-supervised Learning with Sum-Product Networks,semi-supervised learning
"  Domain shift refers to the well known problem that a model trained in one
source domain performs poorly when applied to a target domain with different
statistics. {Domain Generalization} (DG) techniques attempt to alleviate this
issue by producing models which by design generalize well to novel testing
domains. We propose a novel {meta-learning} method for domain generalization.
Rather than designing a specific model that is robust to domain shift as in
most previous DG work, we propose a model agnostic training procedure for DG.
Our algorithm simulates train/test domain shift during training by synthesizing
virtual testing domains within each mini-batch. The meta-optimization objective
requires that steps to improve training domain performance should also improve
testing domain performance. This meta-learning procedure trains models with
good generalization ability to novel domains. We evaluate our method and
achieve state of the art results on a recent cross-domain image classification
benchmark, as well demonstrating its potential on two classic reinforcement
learning tasks.
",Domain Generalization Techniques,domain generalization
"  Dropout is a simple yet effective algorithm for regularizing neural networks
by randomly dropping out units through Bernoulli multiplicative noise, and for
some restricted problem classes, such as linear or logistic regression, several
theoretical studies have demonstrated the equivalence between dropout and a
fully deterministic optimization problem with data-dependent Tikhonov
regularization. This work presents a theoretical analysis of dropout for matrix
factorization, where Bernoulli random variables are used to drop a factor,
thereby attempting to control the size of the factorization. While recent work
has demonstrated the empirical effectiveness of dropout for matrix
factorization, a theoretical understanding of the regularization properties of
dropout in this context remains elusive. This work demonstrates the equivalence
between dropout and a fully deterministic model for matrix factorization in
which the factors are regularized by the sum of the product of the norms of the
columns. While the resulting regularizer is closely related to a variational
form of the nuclear norm, suggesting that dropout may limit the size of the
factorization, we show that it is possible to trivially lower the objective
value by doubling the size of the factorization. We show that this problem is
caused by the use of a fixed dropout rate, which motivates the use of a rate
that increases with the size of the factorization. Synthetic experiments
validate our theoretical findings.
",Dropout in Matrix Factorization,theoretical analysis of dropout in matrix factorization
"  The robustness of complex networks under targeted attacks is deeply connected
to the resilience of complex systems, i.e., the ability to make appropriate
responses to the attacks. In this article, we investigated the state-of-the-art
targeted node attack algorithms and demonstrate that they become very
inefficient when the cost of the attack is taken into consideration. In this
paper, we made explicit assumption that the cost of removing a node is
proportional to the number of adjacent links that are removed, i.e., higher
degree nodes have higher cost. Finally, for the case when it is possible to
attack links, we propose a simple and efficient edge removal strategy named
Hierarchical Power Iterative Normalized cut (HPI-Ncut).The results on real and
artificial networks show that the HPI-Ncut algorithm outperforms all the node
removal and link removal attack algorithms when the cost of the attack is taken
into consideration. In addition, we show that on sparse networks, the
complexity of this hierarchical power iteration edge removal algorithm is only
$O(n\log^{2+\epsilon}(n))$.
",Network Robustness under Targeted Attacks,robustness of complex networks under targeted attacks
"  In this paper, we study the online learning algorithm without explicit
regularization terms. This algorithm is essentially a stochastic gradient
descent scheme in a reproducing kernel Hilbert space (RKHS). The polynomially
decaying step size in each iteration can play a role of regularization to
ensure the generalization ability of online learning algorithm. We develop a
novel capacity dependent analysis on the performance of the last iterate of
online learning algorithm. The contribution of this paper is two-fold. First,
our nice analysis can lead to the convergence rate in the standard mean square
distance which is the best so far. Second, we establish, for the first time,
the strong convergence of the last iterate with polynomially decaying step
sizes in the RKHS norm. We demonstrate that the theoretical analysis
established in this paper fully exploits the fine structure of the underlying
RKHS, and thus can lead to sharp error estimates of online learning algorithm.
",Online Learning in Reproducing Kernel Hilbert Space,online learning algorithm in reproducing kernel hilbert space
"  How can we find patterns and anomalies in a tensor, or multi-dimensional
array, in an efficient and directly interpretable way? How can we do this in an
online environment, where a new tensor arrives each time step? Finding patterns
and anomalies in a tensor is a crucial problem with many applications,
including building safety monitoring, patient health monitoring, cyber
security, terrorist detection, and fake user detection in social networks.
Standard PARAFAC and Tucker decomposition results are not directly
interpretable. Although a few sampling-based methods have previously been
proposed towards better interpretability, they need to be made faster, more
memory efficient, and more accurate.
  In this paper, we propose CTD, a fast, accurate, and directly interpretable
tensor decomposition method based on sampling. CTD-S, the static version of
CTD, provably guarantees a high accuracy that is 17 ~ 83x more accurate than
that of the state-of-the-art method. Also, CTD-S is made 5 ~ 86x faster, and 7
~ 12x more memory-efficient than the state-of-the-art method by removing
redundancy. CTD-D, the dynamic version of CTD, is the first interpretable
dynamic tensor decomposition method ever proposed. Also, it is made 2 ~ 3x
faster than already fast CTD-S by exploiting factors at previous time step and
by reordering operations. With CTD, we demonstrate how the results can be
effectively interpreted in the online distributed denial of service (DDoS)
attack detection.
",Tensor Anomaly Detection,tensor decomposition for anomaly detection
"  XGBoost is often presented as the algorithm that wins every ML competition.
Surprisingly, this is true even though predictions are piecewise constant. This
might be justified in high dimensional input spaces, but when the number of
features is low, a piecewise linear model is likely to perform better. XGBoost
was extended into LinXGBoost that stores at each leaf a linear model. This
extension, equivalent to piecewise regularized least-squares, is particularly
attractive for regression of functions that exhibits jumps or discontinuities.
Those functions are notoriously hard to regress. Our extension is compared to
the vanilla XGBoost and Random Forest in experiments on both synthetic and
real-world data sets.
",Enhancing XGBoost with Linear Models,xgboost and its extension to piecewise linear models
"  Ability to continuously learn and adapt from limited experience in
nonstationary environments is an important milestone on the path towards
general intelligence. In this paper, we cast the problem of continuous
adaptation into the learning-to-learn framework. We develop a simple
gradient-based meta-learning algorithm suitable for adaptation in dynamically
changing and adversarial scenarios. Additionally, we design a new multi-agent
competitive environment, RoboSumo, and define iterated adaptation games for
testing various aspects of continuous adaptation strategies. We demonstrate
that meta-learning enables significantly more efficient adaptation than
reactive baselines in the few-shot regime. Our experiments with a population of
agents that learn and compete suggest that meta-learners are the fittest.
",Meta-Learning for Continuous Adaptation,meta-learning for continuous adaptation in nonstationary environments
"  We perform an average case analysis of the generalization dynamics of large
neural networks trained using gradient descent. We study the
practically-relevant ""high-dimensional"" regime where the number of free
parameters in the network is on the order of or even larger than the number of
examples in the dataset. Using random matrix theory and exact solutions in
linear models, we derive the generalization error and training error dynamics
of learning and analyze how they depend on the dimensionality of data and
signal to noise ratio of the learning problem. We find that the dynamics of
gradient descent learning naturally protect against overtraining and
overfitting in large networks. Overtraining is worst at intermediate network
sizes, when the effective number of free parameters equals the number of
samples, and thus can be reduced by making a network smaller or larger.
Additionally, in the high-dimensional regime, low generalization error requires
starting with small initial weights. We then turn to non-linear neural
networks, and show that making networks very large does not harm their
generalization performance. On the contrary, it can in fact reduce
overtraining, even without early stopping or regularization of any sort. We
identify two novel phenomena underlying this behavior in overcomplete models:
first, there is a frozen subspace of the weights in which no learning occurs
under gradient descent; and second, the statistical properties of the
high-dimensional regime yield better-conditioned input correlations which
protect against overtraining. We demonstrate that naive application of
worst-case theories such as Rademacher complexity are inaccurate in predicting
the generalization performance of deep neural networks, and derive an
alternative bound which incorporates the frozen subspace and conditioning
effects and qualitatively matches the behavior observed in simulation.
",Generalization Dynamics of Large Neural Networks,deep learning
"  In this work we introduce the concept of an Underestimate Sequence (UES),
which is motivated by Nesterov's estimate sequence. Our definition of a UES
utilizes three sequences, one of which is a lower bound (or under-estimator) of
the objective function. The question of how to construct an appropriate
sequence of lower bounds is addressed, and we present lower bounds for strongly
convex smooth functions and for strongly convex composite functions, which
adhere to the UES framework. Further, we propose several first order methods
for minimizing strongly convex functions in both the smooth and composite
cases. The algorithms, based on efficiently updating lower bounds on the
objective functions, have natural stopping conditions that provide the user
with a certificate of optimality. Convergence of all algorithms is guaranteed
through the UES framework, and we show that all presented algorithms converge
linearly, with the accelerated variants enjoying the optimal linear rate of
convergence.
",Optimality Certificates for Convex Optimization,optimization methods for strongly convex functions
"  Deep neural networks have enabled progress in a wide variety of applications.
Growing the size of the neural network typically results in improved accuracy.
As model sizes grow, the memory and compute requirements for training these
models also increases. We introduce a technique to train deep neural networks
using half precision floating point numbers. In our technique, weights,
activations and gradients are stored in IEEE half-precision format.
Half-precision floating numbers have limited numerical range compared to
single-precision numbers. We propose two techniques to handle this loss of
information. Firstly, we recommend maintaining a single-precision copy of the
weights that accumulates the gradients after each optimizer step. This
single-precision copy is rounded to half-precision format during training.
Secondly, we propose scaling the loss appropriately to handle the loss of
information with half-precision gradients. We demonstrate that this approach
works for a wide variety of models including convolution neural networks,
recurrent neural networks and generative adversarial networks. This technique
works for large scale models with more than 100 million parameters trained on
large datasets. Using this approach, we can reduce the memory consumption of
deep learning models by nearly 2x. In future processors, we can also expect a
significant computation speedup using half-precision hardware units.
",Efficient Training of Deep Neural Networks with Low-Precision Data Types,deep learning
"  Steering a car through traffic is a complex task that is difficult to cast
into algorithms. Therefore, researchers turn to training artificial neural
networks from front-facing camera data stream along with the associated
steering angles. Nevertheless, most existing solutions consider only the visual
camera frames as input, thus ignoring the temporal relationship between frames.
In this work, we propose a Convolutional Long Short-Term Memory Recurrent
Neural Network (C-LSTM), that is end-to-end trainable, to learn both visual and
dynamic temporal dependencies of driving. Additionally, We introduce posing the
steering angle regression problem as classification while imposing a spatial
relationship between the output layer neurons. Such method is based on learning
a sinusoidal function that encodes steering angles. To train and validate our
proposed methods, we used the publicly available Comma.ai dataset. Our solution
improved steering root mean square error by 35% over recent methods, and led to
a more stable steering by 87%.
",Deep Learning for Autonomous Vehicles,lane following and steering angle regression using deep learning
"  Given a sample of bids from independent auctions, this paper examines the
question of inference on auction fundamentals (e.g. valuation distributions,
welfare measures) under weak assumptions on information structure. The question
is important as it allows us to learn about the valuation distribution in a
robust way, i.e., without assuming that a particular information structure
holds across observations. We leverage the recent contributions of
\cite{Bergemann2013} in the robust mechanism design literature that exploit the
link between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in
incomplete information games to construct an econometrics framework for
learning about auction fundamentals using observed data on bids. We showcase
our construction of identified sets in private value and common value auctions.
Our approach for constructing these sets inherits the computational simplicity
of solving for correlated equilibria: checking whether a particular valuation
distribution belongs to the identified set is as simple as determining whether
a {\it linear} program is feasible. A similar linear program can be used to
construct the identified set on various welfare measures and counterfactual
objects. For inference and to summarize statistical uncertainty, we propose
novel finite sample methods using tail inequalities that are used to construct
confidence regions on sets. We also highlight methods based on Bayesian
bootstrap and subsampling. A set of Monte Carlo experiments show adequate
finite sample properties of our inference procedures. We illustrate our methods
using data from OCS auctions.
",Auction Fundamentals Inference,auction theory
"  Scientists often seek simplified representations of complex systems to
facilitate prediction and understanding. If the factors comprising a
representation allow us to make accurate predictions about our system, but
obscuring any subset of the factors destroys our ability to make predictions,
we say that the representation exhibits informational synergy. We argue that
synergy is an undesirable feature in learned representations and that
explicitly minimizing synergy can help disentangle the true factors of
variation underlying data. We explore different ways of quantifying synergy,
deriving new closed-form expressions in some cases, and then show how to modify
learning to produce representations that are minimally synergistic. We
introduce a benchmark task to disentangle separate characters from images of
words. We demonstrate that Minimally Synergistic (MinSyn) representations
correctly disentangle characters while methods relying on statistical
independence fail.
",Informational Synergy in Representation Learning,minimizing synergy in machine learning representations
"  Knowledge transfer between tasks can improve the performance of learned
models, but requires an accurate estimate of the inter-task relationships to
identify the relevant knowledge to transfer. These inter-task relationships are
typically estimated based on training data for each task, which is inefficient
in lifelong learning settings where the goal is to learn each consecutive task
rapidly from as little data as possible. To reduce this burden, we develop a
lifelong learning method based on coupled dictionary learning that utilizes
high-level task descriptions to model the inter-task relationships. We show
that using task descriptors improves the performance of the learned task
policies, providing both theoretical justification for the benefit and
empirical demonstration of the improvement across a variety of learning
problems. Given only the descriptor for a new task, the lifelong learner is
also able to accurately predict a model for the new task through zero-shot
learning using the coupled dictionary, eliminating the need to gather training
data before addressing the task.
",Lifelong Learning with Task Descriptors,lifelong learning with task descriptions
"  We provide a complete picture of asymptotically minimax estimation of
$L_r$-norms (for any $r\ge 1$) of the mean in Gaussian white noise model over
Nikolskii-Besov spaces. In this regard, we complement the work of Lepski,
Nemirovski and Spokoiny (1999), who considered the cases of $r=1$ (with
poly-logarithmic gap between upper and lower bounds) and $r$ even (with
asymptotically sharp upper and lower bounds) over H\""{o}lder spaces. We
additionally consider the case of asymptotically adaptive minimax estimation
and demonstrate a difference between even and non-even $r$ in terms of an
investigator's ability to produce asymptotically adaptive minimax estimators
without paying a penalty.
",Minimax Estimation in Gaussian White Noise Model,asymptotically minimax estimation of l-r norms in gaussian white noise model
"  Real world applications often naturally decompose into several sub-tasks. In
many settings (e.g., robotics) demonstrations provide a natural way to specify
the sub-tasks. However, most methods for learning from demonstrations either do
not provide guarantees that the artifacts learned for the sub-tasks can be
safely recombined or limit the types of composition available. Motivated by
this deficit, we consider the problem of inferring Boolean non-Markovian
rewards (also known as logical trace properties or specifications) from
demonstrations provided by an agent operating in an uncertain, stochastic
environment. Crucially, specifications admit well-defined composition rules
that are typically easy to interpret. In this paper, we formulate the
specification inference task as a maximum a posteriori (MAP) probability
inference problem, apply the principle of maximum entropy to derive an analytic
demonstration likelihood model and give an efficient approach to search for the
most likely specification in a large candidate pool of specifications. In our
experiments, we demonstrate how learning specifications can help avoid common
problems that often arise due to ad-hoc reward composition.
",Learning from Demonstrations,learning from demonstrations for multi-task reward inference
"  We present PRM-RL, a hierarchical method for long-range navigation task
completion that combines sampling based path planning with reinforcement
learning (RL). The RL agents learn short-range, point-to-point navigation
policies that capture robot dynamics and task constraints without knowledge of
the large-scale topology. Next, the sampling-based planners provide roadmaps
which connect robot configurations that can be successfully navigated by the RL
agent. The same RL agents are used to control the robot under the direction of
the planning, enabling long-range navigation. We use the Probabilistic Roadmaps
(PRMs) for the sampling-based planner. The RL agents are constructed using
feature-based and deep neural net policies in continuous state and action
spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation
tasks with non-trivial robot dynamics: end-to-end differential drive indoor
navigation in office environments, and aerial cargo delivery in urban
environments with load displacement constraints. Our results show improvement
in task completion over both RL agents on their own and traditional
sampling-based planners. In the indoor navigation task, PRM-RL successfully
completes up to 215 m long trajectories under noisy sensor conditions, and the
aerial cargo delivery completes flights over 1000 m without violating the task
constraints in an environment 63 million times larger than used in training.
",Hierarchical Navigation using Reinforcement Learning,long-range navigation using hierarchical path planning and reinforcement learning
"  A recently proposed learning algorithm for massive network-structured data
sets (big data over networks) is the network Lasso (nLasso), which extends the
well- known Lasso estimator from sparse models to network-structured datasets.
Efficient implementations of the nLasso have been presented using modern convex
optimization methods. In this paper, we provide sufficient conditions on the
network structure and available label information such that nLasso accurately
learns a vector-valued graph signal (representing label information) from the
information provided by the labels of a few data points.
",Network Lasso Algorithm for Graph Signal Learning,machine learning for network-structured data
"  For many algorithms, parameter tuning remains a challenging and critical
task, which becomes tedious and infeasible in a multi-parameter setting.
Multi-penalty regularization, successfully used for solving undetermined sparse
regression of problems of unmixing type where signal and noise are additively
mixed, is one of such examples. In this paper, we propose a novel algorithmic
framework for an adaptive parameter choice in multi-penalty regularization with
a focus on the correct support recovery. Building upon the theory of
regularization paths and algorithms for single-penalty functionals, we extend
these ideas to a multi-penalty framework by providing an efficient procedure
for the construction of regions containing structurally similar solutions,
i.e., solutions with the same sparsity and sign pattern, over the whole range
of parameters. Combining this with a model selection criterion, we can choose
regularization parameters in a data-adaptive manner. Another advantage of our
algorithm is that it provides an overview on the solution stability over the
whole range of parameters. This can be further exploited to obtain additional
insights into the problem of interest. We provide a numerical analysis of our
method and compare it to the state-of-the-art single-penalty algorithms for
compressed sensing problems in order to demonstrate the robustness and power of
the proposed algorithm.
",Multi-Parameter Regularization,multi-penalty regularization for parameter tuning in multi-parameter setting
"  Topological Data Analysis is a recent and fast growing field providing a set
of new topological and geometric tools to infer relevant features for possibly
complex data. This paper is a brief introduction, through a few selected
topics, to basic fundamental and practical aspects of \tda\ for non experts.
",Introduction to Topological Data Analysis,topological data analysis
"  We consider multi-agent stochastic optimization problems over reproducing
kernel Hilbert spaces (RKHS). In this setting, a network of interconnected
agents aims to learn decision functions, i.e., nonlinear statistical models,
that are optimal in terms of a global convex functional that aggregates data
across the network, with only access to locally and sequentially observed
samples. We propose solving this problem by allowing each agent to learn a
local regression function while enforcing consensus constraints. We use a
penalized variant of functional stochastic gradient descent operating
simultaneously with low-dimensional subspace projections. These subspaces are
constructed greedily by applying orthogonal matching pursuit to the sequence of
kernel dictionaries and weights. By tuning the projection-induced bias, we
propose an algorithm that allows for each individual agent to learn, based upon
its locally observed data stream and message passing with its neighbors only, a
regression function that is close to the globally optimal regression function.
That is, we establish that with constant step-size selections agents' functions
converge to a neighborhood of the globally optimal one while satisfying the
consensus constraints as the penalty parameter is increased. Moreover, the
complexity of the learned regression functions is guaranteed to remain finite.
On both multi-class kernel logistic regression and multi-class kernel support
vector classification with data generated from class-dependent Gaussian mixture
models, we observe stable function estimation and state of the art performance
for distributed online multi-class classification. Experiments on the Brodatz
textures further substantiate the empirical validity of this approach.
",Distributed Online Learning,distributed machine learning
"  Comparing with traditional learning criteria, such as mean square error
(MSE), the minimum error entropy (MEE) criterion is superior in nonlinear and
non-Gaussian signal processing and machine learning. The argument of the
logarithm in Renyis entropy estimator, called information potential (IP), is a
popular MEE cost in information theoretic learning (ITL). The computational
complexity of IP is however quadratic in terms of sample number due to double
summation. This creates computational bottlenecks especially for large-scale
datasets. To address this problem, in this work we propose an efficient
quantization approach to reduce the computational burden of IP, which decreases
the complexity from O(N*N) to O (MN) with M << N. The new learning criterion is
called the quantized MEE (QMEE). Some basic properties of QMEE are presented.
Illustrative examples are provided to verify the excellent performance of QMEE.
",Efficient Computation of Information Potential in Machine Learning,efficient quantization of minimum error entropy criterion for nonlinear and non-gaussian signal processing
"  I present a web service for querying an embedding of entities in the Wikidata
knowledge graph. The embedding is trained on the Wikidata dump using Gensim's
Word2Vec implementation and a simple graph walk. A REST API is implemented.
Together with the Wikidata API the web service exposes a multilingual resource
for over 600'000 Wikidata items and properties.
",Wikidata Embedding Service,knowledge graph querying
"  One of the most basic skills a robot should possess is predicting the effect
of physical interactions with objects in the environment. This enables optimal
action selection to reach a certain goal state. Traditionally, dynamics are
approximated by physics-based analytical models. These models rely on specific
state representations that may be hard to obtain from raw sensory data,
especially if no knowledge of the object shape is assumed. More recently, we
have seen learning approaches that can predict the effect of complex physical
interactions directly from sensory input. It is however an open question how
far these models generalize beyond their training data. In this work, we
investigate the advantages and limitations of neural network based learning
approaches for predicting the effects of actions based on sensory input and
show how analytical and learned models can be combined to leverage the best of
both worlds. As physical interaction task, we use planar pushing, for which
there exists a well-known analytical model and a large real-world dataset. We
propose to use a convolutional neural network to convert raw depth images or
organized point clouds into a suitable representation for the analytical model
and compare this approach to using neural networks for both, perception and
prediction. A systematic evaluation of the proposed approach on a very large
real-world dataset shows two main advantages of the hybrid architecture.
Compared to a pure neural network, it significantly (i) reduces required
training data and (ii) improves generalization to novel physical interaction.
",Predicting Effects of Robot-Object Interactions,robotics and machine learning
"  We investigate recurrent neural network architectures for event-sequence
processing. Event sequences, characterized by discrete observations stamped
with continuous-valued times of occurrence, are challenging due to the
potentially wide dynamic range of relevant time scales as well as interactions
between time scales. We describe four forms of inductive bias that should
benefit architectures for event sequences: temporal locality, position and
scale homogeneity, and scale interdependence. We extend the popular gated
recurrent unit (GRU) architecture to incorporate these biases via intrinsic
temporal dynamics, obtaining a continuous-time GRU. The CT-GRU arises by
interpreting the gates of a GRU as selecting a time scale of memory, and the
CT-GRU generalizes the GRU by incorporating multiple time scales of memory and
performing context-dependent selection of time scales for information storage
and retrieval. Event time-stamps drive decay dynamics of the CT-GRU, whereas
they serve as generic additional inputs to the GRU. Despite the very different
manner in which the two models consider time, their performance on eleven data
sets we examined is essentially identical. Our surprising results point both to
the robustness of GRU and LSTM architectures for handling continuous time, and
to the potency of incorporating continuous dynamics into neural architectures.
",Recurrent Neural Networks for Event Sequence Processing,recurrent neural networks for event sequence processing
"  Cars can nowadays record several thousands of signals through the CAN bus
technology and potentially provide real-time information on the car, the driver
and the surrounding environment. This paper proposes a new method for the
analysis and classification of driver behavior using a selected subset of CAN
bus signals, specifically gas pedal position, brake pedal pressure, steering
wheel angle, steering wheel momentum, velocity, RPM, frontal and lateral
acceleration. Data has been collected in a completely uncontrolled experiment,
where 64 people drove 10 cars for or a total of over 2000 driving trips without
any type of pre-determined driving instruction on a wide variety of road
scenarios. We propose an unsupervised learning technique that clusters drivers
in different groups, and offers a validation method to test the robustness of
clustering in a wide range of experimental settings. The minimal amount of data
needed to preserve robust driver clustering is also computed. The presented
study provides a new methodology for near-real-time classification of driver
behavior in uncontrolled environments.
",Driver Behavior Analysis,driver behavior analysis using unsupervised learning
"  We present Synkhronos, an extension to Theano for multi-GPU computations
leveraging data parallelism. Our framework provides automated execution and
synchronization across devices, allowing users to continue to write serial
programs without risk of race conditions. The NVIDIA Collective Communication
Library is used for high-bandwidth inter-GPU communication. Further
enhancements to the Theano function interface include input slicing (with
aggregation) and input indexing, which perform common data-parallel computation
patterns efficiently. One example use case is synchronous SGD, which has
recently been shown to scale well for a growing set of deep learning problems.
When training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA
DGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in
isolation. Yet Synkhronos remains general to any data-parallel computation
programmable in Theano. By implementing parallelism at the level of individual
Theano functions, our framework uniquely addresses a niche between manual
multi-device programming and prescribed multi-GPU training routines.
",Multi-GPU Computing Frameworks,multi-gpu computation in theano
"  We prove near-tight concentration of measure for polynomial functions of the
Ising model under high temperature. For any degree $d$, we show that a
degree-$d$ polynomial of a $n$-spin Ising model exhibits exponential tails that
scale as $\exp(-r^{2/d})$ at radius $r=\tilde{\Omega}_d(n^{d/2})$. Our
concentration radius is optimal up to logarithmic factors for constant $d$,
improving known results by polynomial factors in the number of spins. We
demonstrate the efficacy of polynomial functions as statistics for testing the
strength of interactions in social networks in both synthetic and real world
data.
",Concentration of Measure in Ising Models,concentration of measure in ising model
"  A widely studied non-deterministic polynomial time (NP) hard problem lies in
finding a route between the two nodes of a graph. Often meta-heuristics
algorithms such as $A^{*}$ are employed on graphs with a large number of nodes.
Here, we propose a deep recurrent neural network architecture based on the
Sequence-2-Sequence (Seq2Seq) model, widely used, for instance in text
translation. Particularly, we illustrate that utilising a context vector that
has been learned from two different recurrent networks enables increased
accuracies in learning the shortest route of a graph. Additionally, we show
that one can boost the performance of the Seq2Seq network by smoothing the loss
function using a homotopy continuation of the decoder's loss function.
",Graph Route Optimization with Deep Learning,graph routing optimization
"  Learning a regression function using censored or interval-valued output data
is an important problem in fields such as genomics and medicine. The goal is to
learn a real-valued prediction function, and the training output labels
indicate an interval of possible values. Whereas most existing algorithms for
this task are linear models, in this paper we investigate learning nonlinear
tree models. We propose to learn a tree by minimizing a margin-based
discriminative objective function, and we provide a dynamic programming
algorithm for computing the optimal solution in log-linear time. We show
empirically that this algorithm achieves state-of-the-art speed and prediction
accuracy in a benchmark of several data sets.
",Nonlinear Regression with Interval-Valued Data,nonlinear regression modeling for censored or interval-valued output data
"  Linear least-squares regression with a ""design"" matrix A approximates a given
matrix B via minimization of the spectral- or Frobenius-norm discrepancy
||AX-B|| over every conformingly sized matrix X. Another popular approximation
is low-rank approximation via principal component analysis (PCA) -- which is
essentially singular value decomposition (SVD) -- or interpolative
decomposition (ID). Classically, PCA/SVD and ID operate solely with the matrix
B being approximated, not supervised by any auxiliary matrix A. However, linear
least-squares regression models can inform the ID, yielding regression-aware
ID. As a bonus, this provides an interpretation as regression-aware PCA for a
kind of canonical correlation analysis between A and B. The regression-aware
decompositions effectively enable supervision to inform classical
dimensionality reduction, which classically has been totally unsupervised. The
regression-aware decompositions reveal the structure inherent in B that is
relevant to regression against A.
",Regression-Aware Dimensionality Reduction,regression-aware dimensionality reduction
"  We analyze the local convergence of proximal splitting algorithms to solve
optimization problems that are convex besides a rank constraint. For this, we
show conditions under which the proximal operator of a function involving the
rank constraint is locally identical to the proximal operator of its convex
envelope, hence implying local convergence. The conditions imply that the
non-convex algorithms locally converge to a solution whenever a convex
relaxation involving the convex envelope can be expected to solve the
non-convex problem.
",Proximal Splitting Algorithms for Non-Convex Optimization,local convergence of non-convex optimization algorithms
"  We study the construction of coresets for kernel density estimates. That is
we show how to approximate the kernel density estimate described by a large
point set with another kernel density estimate with a much smaller point set.
For characteristic kernels (including Gaussian and Laplace kernels), our
approximation preserves the $L_\infty$ error between kernel density estimates
within error $\epsilon$, with coreset size $2/\epsilon^2$, but no other aspects
of the data, including the dimension, the diameter of the point set, or the
bandwidth of the kernel common to other approximations. When the dimension is
unrestricted, we show this bound is tight for these kernels as well as a much
broader set.
  This work provides a careful analysis of the iterative Frank-Wolfe algorithm
adapted to this context, an algorithm called \emph{kernel herding}. This
analysis unites a broad line of work that spans statistics, machine learning,
and geometry.
  When the dimension $d$ is constant, we demonstrate much tighter bounds on the
size of the coreset specifically for Gaussian kernels, showing that it is
bounded by the size of the coreset for axis-aligned rectangles. Currently the
best known constructive bound is $O(\frac{1}{\epsilon} \log^d
\frac{1}{\epsilon})$, and non-constructively, this can be improved by
$\sqrt{\log \frac{1}{\epsilon}}$. This improves the best constant dimension
bounds polynomially for $d \geq 3$.
",Coresets for Kernel Density Estimates,core-set approximation for kernel density estimates
"  Conventional seismic techniques for detecting the subsurface geologic
features are challenged by limited data coverage, computational inefficiency,
and subjective human factors. We developed a novel data-driven geological
feature detection approach based on pre-stack seismic measurements. Our
detection method employs an efficient and accurate machine-learning detection
approach to extract useful subsurface geologic features automatically.
Specifically, our method is based on kernel ridge regression model. The
conventional kernel ridge regression can be computationally prohibited because
of the large volume of seismic measurements. We employ a data reduction
technique in combination with the conventional kernel ridge regression method
to improve the computational efficiency and reduce memory usage. In particular,
we utilize a randomized numerical linear algebra technique, named Nystr\""om
method, to effectively reduce the dimensionality of the feature space without
compromising the information content required for accurate detection. We
provide thorough computational cost analysis to show efficiency of our new
geological feature detection methods. We further validate the performance of
our new subsurface geologic feature detection method using synthetic surface
seismic data for 2D acoustic and elastic velocity models. Our numerical
examples demonstrate that our new detection method significantly improves the
computational efficiency while maintaining comparable accuracy. Interestingly,
we show that our method yields a speed-up ratio on the order of $\sim10^2$ to
$\sim 10^3$ in a multi-core computational environment.
",Machine Learning in Seismic Feature Detection,geologic feature detection
"  Spectral decomposition of the Koopman operator is attracting attention as a
tool for the analysis of nonlinear dynamical systems. Dynamic mode
decomposition is a popular numerical algorithm for Koopman spectral analysis;
however, we often need to prepare nonlinear observables manually according to
the underlying dynamics, which is not always possible since we may not have any
a priori knowledge about them. In this paper, we propose a fully data-driven
method for Koopman spectral analysis based on the principle of learning Koopman
invariant subspaces from observed data. To this end, we propose minimization of
the residual sum of squares of linear least-squares regression to estimate a
set of functions that transforms data into a form in which the linear
regression fits well. We introduce an implementation with neural networks and
evaluate performance empirically using nonlinear dynamical systems and
applications.
",Data-Driven Koopman Spectral Analysis,koopman spectral analysis
"  In building intelligent transportation systems such as taxi or rideshare
services, accurate prediction of travel time and distance is crucial for
customer experience and resource management. Using the NYC taxi dataset, which
contains taxi trips data collected from GPS-enabled taxis [23], this paper
investigates the use of deep neural networks to jointly predict taxi trip time
and distance. We propose a model, called ST-NN (Spatio-Temporal Neural
Network), which first predicts the travel distance between an origin and a
destination GPS coordinate, then combines this prediction with the time of day
to predict the travel time. The beauty of ST-NN is that it uses only the raw
trips data without requiring further feature engineering and provides a joint
estimate of travel time and distance. We compare the performance of ST-NN to
that of state-of-the-art travel time estimation methods, and we observe that
the proposed approach generalizes better than state-of-the-art methods. We show
that ST-NN approach significantly reduces the mean absolute error for both
predicted travel time and distance, about 17% for travel time prediction. We
also observe that the proposed approach is more robust to outliers present in
the dataset by testing the performance of ST-NN on the datasets with and
without outliers.
",Predicting Travel Time and Distance in Taxi Services,deep learning for intelligent transportation systems
"  The project aims to research on combining deep learning specifically
Long-Short Memory (LSTM) and basic statistics in multiple multistep time series
prediction. LSTM can dive into all the pages and learn the general trends of
variation in a large scope, while the well selected medians for each page can
keep the special seasonality of different pages so that the future trend will
not fluctuate too much from the reality. A recent Kaggle competition on 145K
Web Traffic Time Series Forecasting [1] is used to thoroughly illustrate and
test this idea.
",Multistep Time Series Prediction using LSTM and Statistics,time series prediction using lstm and statistics
"  In practical analysis, domain knowledge about analysis target has often been
accumulated, although, typically, such knowledge has been discarded in the
statistical analysis stage, and the statistical tool has been applied as a
black box. In this paper, we introduce sign constraints that are a handy and
simple representation for non-experts in generic learning problems. We have
developed two new optimization algorithms for the sign-constrained regularized
loss minimization, called the sign-constrained Pegasos (SC-Pega) and the
sign-constrained SDCA (SC-SDCA), by simply inserting the sign correction step
into the original Pegasos and SDCA, respectively. We present theoretical
analyses that guarantee that insertion of the sign correction step does not
degrade the convergence rate for both algorithms. Two applications, where the
sign-constrained learning is effective, are presented. The one is exploitation
of prior information about correlation between explanatory variables and a
target variable. The other is introduction of the sign-constrained to
SVM-Pairwise method. Experimental results demonstrate significant improvement
of generalization performance by introducing sign constraints in both
applications.
",Sign-Constrained Learning,machine learning
"  Machine learning systems are increasingly used to make decisions about
people's lives, such as whether to give someone a loan or whether to interview
someone for a job. This has led to considerable interest in making such machine
learning systems fair. One approach is to transform the input data used by the
algorithm. This can be achieved by passing each input data point through a
representation function prior to its use in training or testing. Techniques for
learning such representation functions from data have been successful
empirically, but typically lack theoretical fairness guarantees. We show that
it is possible to prove that a representation function is fair according to
common measures of both group and individual fairness, as well as useful with
respect to a target task. These provable properties can be used in a governance
model involving a data producer, a data user and a data regulator, where there
is a separation of concerns between fairness and target task utility to ensure
transparency and prevent perverse incentives. We formally define the 'cost of
mistrust' of using this model compared to the setting where there is a single
trusted party, and provide bounds on this cost in particular cases. We present
a practical approach to learning fair representation functions and apply it to
financial and criminal justice datasets. We evaluate the fairness and utility
of these representation functions using measures motivated by our theoretical
results.
",Fairness in Machine Learning,machine learning for fairness
"  We present a novel tractable generative model that extends Sum-Product
Networks (SPNs) and significantly boosts their power. We call it
Sum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate
conditional distributions into the model by direct computation using quotient
nodes, e.g. $P(A|B) = \frac{P(A,B)}{P(B)}$. We provide sufficient conditions
for the tractability of SPQNs that generalize and relax the decomposable and
complete tractability conditions of SPNs. These relaxed conditions give rise to
an exponential boost to the expressive efficiency of our model, i.e. we prove
that there are distributions which SPQNs can compute efficiently but require
SPNs to be of exponential size. Thus, we narrow the gap in expressivity between
tractable graphical models and other Neural Network-based generative models.
",Sum-Product-Quotient Networks (SPQNs),sum-product-quotient networks for tractable generative modeling
"  In this paper, a new adaptive multi-batch experience replay scheme is
proposed for proximal policy optimization (PPO) for continuous action control.
On the contrary to original PPO, the proposed scheme uses the batch samples of
past policies as well as the current policy for the update for the next policy,
where the number of the used past batches is adaptively determined based on the
oldness of the past batches measured by the average importance sampling (IS)
weight. The new algorithm constructed by combining PPO with the proposed
multi-batch experience replay scheme maintains the advantages of original PPO
such as random mini-batch sampling and small bias due to low IS weights by
storing the pre-computed advantages and values and adaptively determining the
mini-batch size. Numerical results show that the proposed method significantly
increases the speed and stability of convergence on various continuous control
tasks compared to original PPO.
",Adaptive Experience Replay for Proximal Policy Optimization,reinforcement learning
"  In this paper, a new approach for classification of target task using limited
labeled target data as well as enormous unlabeled source data is proposed which
is called self-taught learning. The target and source data can be drawn from
different distributions. In the previous approaches, covariate shift assumption
is considered where the marginal distributions p(x) change over domains and the
conditional distributions p(y|x) remain the same. In our approach, we propose a
new objective function which simultaneously learns a common space T(.) where
the conditional distributions over domains p(T(x)|y) remain the same and learns
robust SVM classifiers for target task using both source and target data in the
new representation. Hence, in the proposed objective function, the hidden label
of the source data is also incorporated. We applied the proposed approach on
Caltech-256, MSRC+LMO datasets and compared the performance of our algorithm to
the available competing methods. Our method has a superior performance to the
successful existing algorithms.
",Self-Taught Learning for Domain Adaptation,self-taught learning for classification
"  The presence of noisy instances in mobile phone data is a fundamental issue
for classifying user phone call behavior (i.e., accept, reject, missed and
outgoing), with many potential negative consequences. The classification
accuracy may decrease and the complexity of the classifiers may increase due to
the number of redundant training samples. To detect such noisy instances from a
training dataset, researchers use naive Bayes classifier (NBC) as it identifies
misclassified instances by taking into account independence assumption and
conditional probabilities of the attributes. However, some of these
misclassified instances might indicate usages behavioral patterns of individual
mobile phone users. Existing naive Bayes classifier based noise detection
techniques have not considered this issue and, thus, are lacking in
classification accuracy. In this paper, we propose an improved noise detection
technique based on naive Bayes classifier for effectively classifying users'
phone call behaviors. In order to improve the classification accuracy, we
effectively identify noisy instances from the training dataset by analyzing the
behavioral patterns of individuals. We dynamically determine a noise threshold
according to individual's unique behavioral patterns by using both the naive
Bayes classifier and Laplace estimator. We use this noise threshold to identify
noisy instances. To measure the effectiveness of our technique in classifying
user phone call behavior, we employ the most popular classification algorithm
(e.g., decision tree). Experimental results on the real phone call log dataset
show that our proposed technique more accurately identifies the noisy instances
from the training datasets that leads to better classification accuracy.
",Noisy Instance Detection in Mobile Phone Data,noise detection in mobile phone call behavior classification
"  Multilayer (or deep) networks are powerful probabilistic models based on
multiple stages of a linear transform followed by a non-linear (possibly
random) function. In general, the linear transforms are defined by matrices and
the non-linear functions are defined by information channels. These models have
gained great popularity due to their ability to characterize complex
probabilistic relationships arising in a wide variety of inference problems.
The contribution of this paper is a new method for analyzing the fundamental
limits of statistical inference in settings where the model is known. The
validity of our method can be established in a number of settings and is
conjectured to hold more generally. A key assumption made throughout is that
the matrices are drawn randomly from orthogonally invariant distributions.
  Our method yields explicit formulas for 1) the mutual information; 2) the
minimum mean-squared error (MMSE); 3) the existence and locations of certain
phase-transitions with respect to the problem parameters; and 4) the stationary
points for the state evolution of approximate message passing algorithms. When
applied to the special case of models with multivariate Gaussian channels our
method is rigorous and has close connections to free probability theory for
random matrices. When applied to the general case of non-Gaussian channels, our
method provides a simple alternative to the replica method from statistical
physics. A key observation is that the combined effects of the individual
components in the model (namely the matrices and the channels) are additive
when viewed in a certain transform domain.
",Deep Network Analysis,machine learning/deep learning
"  The Epicurean Philosophy is commonly thought as simplistic and hedonistic.
Here I discuss how this is a misconception and explore its link to
Reinforcement Learning. Based on the letters of Epicurus, I construct an
objective function for hedonism which turns out to be equivalent of the
Reinforcement Learning objective function when omitting the discount factor. I
then discuss how Plato and Aristotle 's views that can be also loosely linked
to Reinforcement Learning, as well as their weaknesses in relationship to it.
Finally, I emphasise the close affinity of the Epicurean views and the Bellman
equation.
",Epicurean Philosophy and Reinforcement Learning,philosophy and reinforcement learning
"  The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is
the main computational bottleneck in spectral clustering. In this work, we
introduce a highly-scalable, spectrum-preserving graph sparsification algorithm
that enables to build ultra-sparse NN (u-NN) graphs with guaranteed
preservation of the original graph spectrums, such as the first few
eigenvectors of the original graph Laplacian. Our approach can immediately lead
to scalable spectral clustering of large data networks without sacrificing
solution quality. The proposed method starts from constructing low-stretch
spanning trees (LSSTs) from the original graphs, which is followed by
iteratively recovering small portions of ""spectrally critical"" off-tree edges
to the LSSTs by leveraging a spectral off-tree embedding scheme. To determine
the suitable amount of off-tree edges to be recovered to the LSSTs, an
eigenvalue stability checking scheme is proposed, which enables to robustly
preserve the first few Laplacian eigenvectors within the sparsified graph.
Additionally, an incremental graph densification scheme is proposed for
identifying extra edges that have been missing in the original NN graphs but
can still play important roles in spectral clustering tasks. Our experimental
results for a variety of well-known data sets show that the proposed method can
dramatically reduce the complexity of NN graphs, leading to significant
speedups in spectral clustering.
",Scalable Spectral Clustering via Graph Sparsification,spectral clustering
"  Imitation learning is a powerful paradigm for robot skill acquisition.
However, obtaining demonstrations suitable for learning a policy that maps from
raw pixels to actions can be challenging. In this paper we describe how
consumer-grade Virtual Reality headsets and hand tracking hardware can be used
to naturally teleoperate robots to perform complex tasks. We also describe how
imitation learning can learn deep neural network policies (mapping from pixels
to actions) that can acquire the demonstrated skills. Our experiments showcase
the effectiveness of our approach for learning visuomotor skills.
",Virtual Reality-based Imitation Learning for Robotics,robot learning and teleoperation
"  Robots that navigate through human crowds need to be able to plan safe,
efficient, and human predictable trajectories. This is a particularly
challenging problem as it requires the robot to predict future human
trajectories within a crowd where everyone implicitly cooperates with each
other to avoid collisions. Previous approaches to human trajectory prediction
have modeled the interactions between humans as a function of proximity.
However, that is not necessarily true as some people in our immediate vicinity
moving in the same direction might not be as important as other people that are
further away, but that might collide with us in the future. In this work, we
propose Social Attention, a novel trajectory prediction model that captures the
relative importance of each person when navigating in the crowd, irrespective
of their proximity. We demonstrate the performance of our method against a
state-of-the-art approach on two publicly available crowd datasets and analyze
the trained attention model to gain a better understanding of which surrounding
agents humans attend to, when navigating in a crowd.
",Human Trajectory Prediction in Crowded Environments,human trajectory prediction in crowds
"  With the advent of automated machine learning, automated hyperparameter
optimization methods are by now routinely used in data mining. However, this
progress is not yet matched by equal progress on automatic analyses that yield
information beyond performance-optimizing hyperparameter settings. In this
work, we aim to answer the following two questions: Given an algorithm, what
are generally its most important hyperparameters, and what are typically good
values for these? We present methodology and a framework to answer these
questions based on meta-learning across many datasets. We apply this
methodology using the experimental meta-data available on OpenML to determine
the most important hyperparameters of support vector machines, random forests
and Adaboost, and to infer priors for all their hyperparameters. The results,
obtained fully automatically, provide a quantitative basis to focus efforts in
both manual algorithm design and in automated hyperparameter optimization. The
conducted experiments confirm that the hyperparameters selected by the proposed
method are indeed the most important ones and that the obtained priors also
lead to statistically significant improvements in hyperparameter optimization.
",Hyperparameter Analysis,automated hyperparameter optimization
"  Ever growing volume and velocity of data coupled with decreasing attention
span of end users underscore the critical need for real-time analytics. In this
regard, anomaly detection plays a key role as an application as well as a means
to verify data fidelity. Although the subject of anomaly detection has been
researched for over 100 years in a multitude of disciplines such as, but not
limited to, astronomy, statistics, manufacturing, econometrics, marketing, most
of the existing techniques cannot be used as is on real-time data streams.
Further, the lack of characterization of performance -- both with respect to
real-timeliness and accuracy -- on production data sets makes model selection
very challenging. To this end, we present an in-depth analysis, geared towards
real-time streaming data, of anomaly detection techniques. Given the
requirements with respect to real-timeliness and accuracy, the analysis
presented in this paper should serve as a guide for selection of the ""best""
anomaly detection technique. To the best of our knowledge, this is the first
characterization of anomaly detection techniques proposed in very diverse set
of fields, using production data sets corresponding to a wide set of
application domains.
",Anomaly Detection in Real-Time Data Streams,real-time anomaly detection techniques
"  We study Bayesian hypernetworks: a framework for approximate Bayesian
inference in neural networks. A Bayesian hypernetwork $\h$ is a neural network
which learns to transform a simple noise distribution, $p(\vec\epsilon) =
\N(\vec 0,\mat I)$, to a distribution $q(\pp) := q(h(\vec\epsilon))$ over the
parameters $\pp$ of another neural network (the ""primary network"")\@. We train
$q$ with variational inference, using an invertible $\h$ to enable efficient
estimation of the variational lower bound on the posterior $p(\pp | \D)$ via
sampling. In contrast to most methods for Bayesian deep learning, Bayesian
hypernets can represent a complex multimodal approximate posterior with
correlations between parameters, while enabling cheap iid sampling of~$q(\pp)$.
In practice, Bayesian hypernets can provide a better defense against
adversarial examples than dropout, and also exhibit competitive performance on
a suite of tasks which evaluate model uncertainty, including regularization,
active learning, and anomaly detection.
",Bayesian Hypernetworks,bayesian hypernetworks for approximate bayesian inference in neural networks
"  Given two data matrices $X$ and $Y$, sparse canonical correlation analysis
(SCCA) is to seek two sparse canonical vectors $u$ and $v$ to maximize the
correlation between $Xu$ and $Yv$. However, classical and sparse CCA models
consider the contribution of all the samples of data matrices and thus cannot
identify an underlying specific subset of samples. To this end, we propose a
novel sparse weighted canonical correlation analysis (SWCCA), where weights are
used for regularizing different samples. We solve the $L_0$-regularized SWCCA
($L_0$-SWCCA) using an alternating iterative algorithm. We apply $L_0$-SWCCA to
synthetic data and real-world data to demonstrate its effectiveness and
superiority compared to related methods. Lastly, we consider also SWCCA with
different penalties like LASSO (Least absolute shrinkage and selection
operator) and Group LASSO, and extend it for integrating more than three data
matrices.
",Sparse Weighted Canonical Correlation Analysis,sparse canonical correlation analysis
"  Deep neural networks are widely used for classification. These deep models
often suffer from a lack of interpretability -- they are particularly difficult
to understand because of their non-linear nature. As a result, neural networks
are often treated as ""black box"" models, and in the past, have been trained
purely to optimize the accuracy of predictions. In this work, we create a novel
network architecture for deep learning that naturally explains its own
reasoning for each prediction. This architecture contains an autoencoder and a
special prototype layer, where each unit of that layer stores a weight vector
that resembles an encoded training input. The encoder of the autoencoder allows
us to do comparisons within the latent space, while the decoder allows us to
visualize the learned prototypes. The training objective has four terms: an
accuracy term, a term that encourages every prototype to be similar to at least
one encoded input, a term that encourages every encoded input to be close to at
least one prototype, and a term that encourages faithful reconstruction by the
autoencoder. The distances computed in the prototype layer are used as part of
the classification process. Since the prototypes are learned during training,
the learned network naturally comes with explanations for each prediction, and
the explanations are loyal to what the network actually computes.
",Interpretable Deep Learning Models,deep learning for explainable ai
"  Deep directed generative models have attracted much attention recently due to
their generative modeling nature and powerful data representation ability. In
this paper, we review different structures of deep directed generative models
and the learning and inference algorithms associated with the structures. We
focus on a specific structure that consists of layers of Bayesian Networks due
to the property of capturing inherent and rich dependencies among latent
variables. The major difficulty of learning and inference with deep directed
models with many latent variables is the intractable inference due to the
dependencies among the latent variables and the exponential number of latent
variable configurations. Current solutions use variational methods often
through an auxiliary network to approximate the posterior probability
inference. In contrast, inference can also be performed directly without using
any auxiliary network to maximally preserve the dependencies among the latent
variables. Specifically, by exploiting the sparse representation with the
latent space, max-max instead of max-sum operation can be used to overcome the
exponential number of latent configurations. Furthermore, the max-max operation
and augmented coordinate ascent are applied to both supervised and unsupervised
learning as well as to various inference. Quantitative evaluations on benchmark
datasets of different models are given for both data representation and feature
learning tasks.
",Deep Directed Generative Models for Capturing Latent Variable Dependencies,deep directed generative models
"  With the recent renaissance of deep convolution neural networks, encouraging
breakthroughs have been achieved on the supervised recognition tasks, where
each class has sufficient training data and fully annotated training data.
However, to scale the recognition to a large number of classes with few or now
training samples for each class remains an unsolved problem. One approach to
scaling up the recognition is to develop models capable of recognizing unseen
categories without any training instances, or zero-shot recognition/ learning.
This article provides a comprehensive review of existing zero-shot recognition
techniques covering various aspects ranging from representations of models, and
from datasets and evaluation settings. We also overview related recognition
tasks including one-shot and open set recognition which can be used as natural
extensions of zero-shot recognition when limited number of class samples become
available or when zero-shot recognition is implemented in a real-world setting.
Importantly, we highlight the limitations of existing approaches and point out
future research directions in this existing new research area.
",Zero-Shot Learning,zero-shot recognition techniques
"  This study investigates the performance of two open source intrusion
detection systems (IDSs) namely Snort and Suricata for accurately detecting the
malicious traffic on computer networks. Snort and Suricata were installed on
two different but identical computers and the performance was evaluated at 10
Gbps network speed. It was noted that Suricata could process a higher speed of
network traffic than Snort with lower packet drop rate but it consumed higher
computational resources. Snort had higher detection accuracy and was thus
selected for further experiments. It was observed that the Snort triggered a
high rate of false positive alarms. To solve this problem a Snort adaptive
plug-in was developed. To select the best performing algorithm for Snort
adaptive plug-in, an empirical study was carried out with different learning
algorithms and Support Vector Machine (SVM) was selected. A hybrid version of
SVM and Fuzzy logic produced a better detection accuracy. But the best result
was achieved using an optimised SVM with firefly algorithm with FPR (false
positive rate) as 8.6% and FNR (false negative rate) as 2.2%, which is a good
result. The novelty of this work is the performance comparison of two IDSs at
10 Gbps and the application of hybrid and optimised machine learning algorithms
to Snort.
",Intrusion Detection Systems Performance Evaluation,intrusion detection systems performance evaluation
"  In this paper, we study the Nystr{\""o}m type subsampling for large scale
kernel methods to reduce the computational complexities of big data. We discuss
the multi-penalty regularization scheme based on Nystr{\""o}m type subsampling
which is motivated from well-studied manifold regularization schemes. We
develop a theoretical analysis of multi-penalty least-square regularization
scheme under the general source condition in vector-valued function setting,
therefore the results can also be applied to multi-task learning problems. We
achieve the optimal minimax convergence rates of multi-penalty regularization
using the concept of effective dimension for the appropriate subsampling size.
We discuss an aggregation approach based on linear function strategy to combine
various Nystr{\""o}m approximants. Finally, we demonstrate the performance of
multi-penalty regularization based on Nystr{\""o}m type subsampling on
Caltech-101 data set for multi-class image classification and NSL-KDD benchmark
data set for intrusion detection problem.
",Subsampling for Large-Scale Kernel Methods,nyström type subsampling for large scale kernel methods
"  Neural networks with random hidden nodes have gained increasing interest from
researchers and practical applications. This is due to their unique features
such as very fast training and universal approximation property. In these
networks the weights and biases of hidden nodes determining the nonlinear
feature mapping are set randomly and are not learned. Appropriate selection of
the intervals from which weights and biases are selected is extremely
important. This topic has not yet been sufficiently explored in the literature.
In this work a method of generating random weights and biases is proposed. This
method generates the parameters of the hidden nodes in such a way that
nonlinear fragments of the activation functions are located in the input space
regions with data and can be used to construct the surface approximating a
nonlinear target function. The weights and biases are dependent on the input
data range and activation function type. The proposed methods allows us to
control the generalization degree of the model. These all lead to improvement
in approximation performance of the network. Several experiments show very
promising results.
",Random Weight and Bias Selection in Neural Networks,random weight initialization for neural networks
"  In human-in-the-loop machine learning, the user provides information beyond
that in the training data. Many algorithms and user interfaces have been
designed to optimize and facilitate this human--machine interaction; however,
fewer studies have addressed the potential defects the designs can cause.
Effective interaction often requires exposing the user to the training data or
its statistics. The design of the system is then critical, as this can lead to
double use of data and overfitting, if the user reinforces noisy patterns in
the data. We propose a user modelling methodology, by assuming simple rational
behaviour, to correct the problem. We show, in a user study with 48
participants, that the method improves predictive performance in a sparse
linear regression sentiment analysis task, where graded user knowledge on
feature relevance is elicited. We believe that the key idea of inferring user
knowledge with probabilistic user models has general applicability in guarding
against overfitting and improving interactive machine learning.
",Human-in-the-Loop Machine Learning Defects,human-in-the-loop machine learning
"  It is a usual practice to ignore any structural information underlying
classes in multi-class classification. In this paper, we propose a graph
convolutional network (GCN) augmented neural network classifier to exploit a
known, underlying graph structure of labels. The proposed approach resembles an
(approximate) inference procedure in, for instance, a conditional random field
(CRF). We evaluate the proposed approach on document classification and object
recognition and report both accuracies and graph-theoretic metrics that
correspond to the consistency of the model's prediction. The experiment results
reveal that the proposed model outperforms a baseline method which ignores the
graph structures of a label space in terms of graph-theoretic metrics.
",Graph-Based Multi-Class Classification,graph-based multi-class classification
"  Algorithmic decision making process now affects many aspects of our lives.
Standard tools for machine learning, such as classification and regression, are
subject to the bias in data, and thus direct application of such off-the-shelf
tools could lead to a specific group being unfairly discriminated. Removing
sensitive attributes of data does not solve this problem because a
\textit{disparate impact} can arise when non-sensitive attributes and sensitive
attributes are correlated. Here, we study a fair machine learning algorithm
that avoids such a disparate impact when making a decision. Inspired by the
two-stage least squares method that is widely used in the field of economics,
we propose a two-stage algorithm that removes bias in the training data. The
proposed algorithm is conceptually simple. Unlike most of existing fair
algorithms that are designed for classification tasks, the proposed method is
able to (i) deal with regression tasks, (ii) combine explanatory attributes to
remove reverse discrimination, and (iii) deal with numerical sensitive
attributes. The performance and fairness of the proposed algorithm are
evaluated in simulations with synthetic and real-world datasets.
",Fairness in Machine Learning,fair machine learning
"  Context is an essential capability for robots that are to be as adaptive as
possible in challenging environments. Although there are many context modeling
efforts, they assume a fixed structure and number of contexts. In this paper,
we propose an incremental deep model that extends Restricted Boltzmann
Machines. Our model gets one scene at a time, and gradually extends the
contextual model when necessary, either by adding a new context or a new
context layer to form a hierarchy. We show on a scene classification benchmark
that our method converges to a good estimate of the contexts of the scenes, and
performs better or on-par on several tasks compared to other incremental models
or non-incremental models.
",Incremental Context Modeling for Robotics,incremental context modeling in robotics
"  There have been several attempts at modeling context in robots. However,
either these attempts assume a fixed number of contexts or use a rule-based
approach to determine when to increment the number of contexts. In this paper,
we pose the task of when to increment as a learning problem, which we solve
using a Recurrent Neural Network. We show that the network successfully (with
98\% testing accuracy) learns to predict when to increment, and demonstrate, in
a scene modeling problem (where the correct number of contexts is not known),
that the robot increments the number of contexts in an expected manner (i.e.,
the entropy of the system is reduced). We also present how the incremental
model can be used for various scene reasoning tasks.
",Context Modeling in Robotics,context modeling in robots
"  The conditional mutual information I(X;Y|Z) measures the average information
that X and Y contain about each other given Z. This is an important primitive
in many learning problems including conditional independence testing, graphical
model inference, causal strength estimation and time-series problems. In
several applications, it is desirable to have a functional purely of the
conditional distribution p_{Y|X,Z} rather than of the joint distribution
p_{X,Y,Z}. We define the potential conditional mutual information as the
conditional mutual information calculated with a modified joint distribution
p_{Y|X,Z} q_{X,Z}, where q_{X,Z} is a potential distribution, fixed airport. We
develop K nearest neighbor based estimators for this functional, employing
importance sampling, and a coupling trick, and prove the finite k consistency
of such an estimator. We demonstrate that the estimator has excellent practical
performance and show an application in dynamical system inference.
",Conditional Mutual Information Estimation,conditional mutual information estimation
"  The automation of posterior inference in Bayesian data analysis has enabled
experts and nonexperts alike to use more sophisticated models, engage in faster
exploratory modeling and analysis, and ensure experimental reproducibility.
However, standard automated posterior inference algorithms are not tractable at
the scale of massive modern datasets, and modifications to make them so are
typically model-specific, require expert tuning, and can break theoretical
guarantees on inferential quality. Building on the Bayesian coresets framework,
this work instead takes advantage of data redundancy to shrink the dataset
itself as a preprocessing step, providing fully-automated, scalable Bayesian
inference with theoretical guarantees. We begin with an intuitive reformulation
of Bayesian coreset construction as sparse vector sum approximation, and
demonstrate that its automation and performance-based shortcomings arise from
the use of the supremum norm. To address these shortcomings we develop Hilbert
coresets, i.e., Bayesian coresets constructed under a norm induced by an
inner-product on the log-likelihood function space. We propose two Hilbert
coreset construction algorithms---one based on importance sampling, and one
based on the Frank-Wolfe algorithm---along with theoretical guarantees on
approximation quality as a function of coreset size. Since the exact
computation of the proposed inner-products is model-specific, we automate the
construction with a random finite-dimensional projection of the log-likelihood
functions. The resulting automated coreset construction algorithm is simple to
implement, and experiments on a variety of models with real and synthetic
datasets show that it provides high-quality posterior approximations and a
significant reduction in the computational cost of inference.
",Scalable Bayesian Inference,scalable bayesian inference
"  We propose a probabilistic model for interpreting gene expression levels that
are observed through single-cell RNA sequencing. In the model, each cell has a
low-dimensional latent representation. Additional latent variables account for
technical effects that may erroneously set some observations of gene expression
levels to zero. Conditional distributions are specified by neural networks,
giving the proposed model enough flexibility to fit the data well. We use
variational inference and stochastic optimization to approximate the posterior
distribution. The inference procedure scales to over one million cells, whereas
competing algorithms do not. Even for smaller datasets, for several tasks, the
proposed procedure outperforms state-of-the-art methods like ZIFA and
ZINB-WaVE. We also extend our framework to take into account batch effects and
other confounding factors and propose a natural Bayesian hypothesis framework
for differential expression that outperforms tradition DESeq2.
",Single-Cell RNA Sequencing Analysis,single-cell rna sequencing analysis
"  Recent work on imitation learning has generated policies that reproduce
expert behavior from multi-modal data. However, past approaches have focused
only on recreating a small number of distinct, expert maneuvers, or have relied
on supervised learning techniques that produce unstable policies. This work
extends InfoGAIL, an algorithm for multi-modal imitation learning, to reproduce
behavior over an extended period of time. Our approach involves reformulating
the typical imitation learning setting to include ""burn-in demonstrations"" upon
which policies are conditioned at test time. We demonstrate that our approach
outperforms standard InfoGAIL in maximizing the mutual information between
predicted and unseen style labels in road scene simulations, and we show that
our method leads to policies that imitate expert autonomous driving systems
over long time horizons.
",Imitation Learning for Autonomous Driving,imitation learning for autonomous driving
"  Data discretization is an important step in the process of machine learning,
since it is easier for classifiers to deal with discrete attributes rather than
continuous attributes. Over the years, several methods of performing
discretization such as Boolean Reasoning, Equal Frequency Binning, Entropy have
been proposed, explored, and implemented. In this article, a simple supervised
discretization approach is introduced. The prime goal of MIL is to maximize
classification accuracy of classifier, minimizing loss of information while
discretization of continuous attributes. The performance of the suggested
approach is compared with the supervised discretization algorithm Minimum
Information Loss (MIL), using the state-of-the-art rule inductive algorithms-
J48 (Java implementation of C4.5 classifier). The presented approach is,
indeed, the modified version of MIL. The empirical results show that the
modified approach performs better in several cases in comparison to the
original MIL algorithm and Minimum Description Length Principle (MDLP) .
",Data Discretization in Machine Learning,machine learning discretization techniques
"  Regularization for matrix factorization (MF) and approximation problems has
been carried out in many different ways. Due to its popularity in deep
learning, dropout has been applied also for this class of problems. Despite its
solid empirical performance, the theoretical properties of dropout as a
regularizer remain quite elusive for this class of problems. In this paper, we
present a theoretical analysis of dropout for MF, where Bernoulli random
variables are used to drop columns of the factors. We demonstrate the
equivalence between dropout and a fully deterministic model for MF in which the
factors are regularized by the sum of the product of squared Euclidean norms of
the columns. Additionally, we inspect the case of a variable sized
factorization and we prove that dropout achieves the global minimum of a convex
approximation problem with (squared) nuclear norm regularization. As a result,
we conclude that dropout can be used as a low-rank regularizer with data
dependent singular-value thresholding.
",Dropout Regularization in Matrix Factorization,regularization techniques for matrix factorization
"  It is known that the inconsistent distribution and representation of
different modalities, such as image and text, cause the heterogeneity gap that
makes it challenging to correlate such heterogeneous data. Generative
adversarial networks (GANs) have shown its strong ability of modeling data
distribution and learning discriminative representation, existing GANs-based
works mainly focus on generative problem to generate new data. We have
different goal, aim to correlate heterogeneous data, by utilizing the power of
GANs to model cross-modal joint distribution. Thus, we propose Cross-modal GANs
to learn discriminative common representation for bridging heterogeneity gap.
The main contributions are: (1) Cross-modal GANs architecture is proposed to
model joint distribution over data of different modalities. The inter-modality
and intra-modality correlation can be explored simultaneously in generative and
discriminative models. Both of them beat each other to promote cross-modal
correlation learning. (2) Cross-modal convolutional autoencoders with
weight-sharing constraint are proposed to form generative model. They can not
only exploit cross-modal correlation for learning common representation, but
also preserve reconstruction information for capturing semantic consistency
within each modality. (3) Cross-modal adversarial mechanism is proposed, which
utilizes two kinds of discriminative models to simultaneously conduct
intra-modality and inter-modality discrimination. They can mutually boost to
make common representation more discriminative by adversarial training process.
To the best of our knowledge, our proposed CM-GANs approach is the first to
utilize GANs to perform cross-modal common representation learning. Experiments
are conducted to verify the performance of our proposed approach on cross-modal
retrieval paradigm, compared with 10 methods on 3 cross-modal datasets.
",Cross-modal Representation Learning,cross-modal representation learning with generative adversarial networks (gans)
"  Given $n$ vectors $\mathbf{x}_i\in \mathbb{R}^d$, we want to fit a linear
regression model for noisy labels $y_i\in\mathbb{R}$. The ridge estimator is a
classical solution to this problem. However, when labels are expensive, we are
forced to select only a small subset of vectors $\mathbf{x}_i$ for which we
obtain the labels $y_i$. We propose a new procedure for selecting the subset of
vectors, such that the ridge estimator obtained from that subset offers strong
statistical guarantees in terms of the mean squared prediction error over the
entire dataset of $n$ labeled vectors. The number of labels needed is
proportional to the statistical dimension of the problem which is often much
smaller than $d$. Our method is an extension of a joint subsampling procedure
called volume sampling. A second major contribution is that we speed up volume
sampling so that it is essentially as efficient as leverage scores, which is
the main i.i.d. subsampling procedure for this task. Finally, we show
theoretically and experimentally that volume sampling has a clear advantage
over any i.i.d. sampling when labels are expensive.
",Active Learning for Linear Regression,machine learning
"  Parametric embedding methods such as parametric t-SNE (pt-SNE) have been
widely adopted for data visualization and out-of-sample data embedding without
further computationally expensive optimization or approximation. However, the
performance of pt-SNE is highly sensitive to the hyper-parameter batch size due
to conflicting optimization goals, and often produces dramatically different
embeddings with different choices of user-defined perplexities. To effectively
solve these issues, we present parametric t-distributed stochastic
exemplar-centered embedding methods. Our strategy learns embedding parameters
by comparing given data only with precomputed exemplars, resulting in a cost
function with linear computational and memory complexity, which is further
reduced by noise contrastive samples. Moreover, we propose a shallow embedding
network with high-order feature interactions for data visualization, which is
much easier to tune but produces comparable performance in contrast to a deep
neural network employed by pt-SNE. We empirically demonstrate, using several
benchmark datasets, that our proposed methods significantly outperform pt-SNE
in terms of robustness, visual effects, and quantitative evaluations.
",Data Visualization via Parametric Embedding Methods,parametric t-distributed stochastic exemplar-centered embedding methods
"  Predicting fine-grained interests of users with temporal behavior is
important to personalization and information filtering applications. However,
existing interest prediction methods are incapable of capturing the subtle
degreed user interests towards particular items, and the internal time-varying
drifting attention of individuals is not studied yet. Moreover, the prediction
process can also be affected by inter-personal influence, known as behavioral
mutual infectivity. Inspired by point process in modeling temporal point
process, in this paper we present a deep prediction method based on two
recurrent neural networks (RNNs) to jointly model each user's continuous
browsing history and asynchronous event sequences in the context of inter-user
behavioral mutual infectivity. Our model is able to predict the fine-grained
interest from a user regarding a particular item and corresponding timestamps
when an occurrence of event takes place. The proposed approach is more flexible
to capture the dynamic characteristic of event sequences by using the temporal
point process to model event data and timely update its intensity function by
RNNs. Furthermore, to improve the interpretability of the model, the attention
mechanism is introduced to emphasize both intra-personal and inter-personal
behavior influence over time. Experiments on real datasets demonstrate that our
model outperforms the state-of-the-art methods in fine-grained user interest
prediction.
",User Interest Prediction with Temporal Behavior Modeling,fine-grained user interest prediction in personalization applications
"  Overfitting is one of the most critical challenges in deep neural networks,
and there are various types of regularization methods to improve generalization
performance. Injecting noises to hidden units during training, e.g., dropout,
is known as a successful regularizer, but it is still not clear enough why such
training techniques work well in practice and how we can maximize their benefit
in the presence of two conflicting objectives---optimizing to true data
distribution and preventing overfitting by regularization. This paper addresses
the above issues by 1) interpreting that the conventional training methods with
regularization by noise injection optimize the lower bound of the true
objective and 2) proposing a technique to achieve a tighter lower bound using
multiple noise samples per training example in a stochastic gradient descent
iteration. We demonstrate the effectiveness of our idea in several computer
vision applications.
",Regularization in Deep Neural Networks,regularization techniques in deep neural networks
"  Social network analysis provides meaningful information about behavior of
network members that can be used for diverse applications such as
classification, link prediction. However, network analysis is computationally
expensive because of feature learning for different applications. In recent
years, many researches have focused on feature learning methods in social
networks. Network embedding represents the network in a lower dimensional
representation space with the same properties which presents a compressed
representation of the network. In this paper, we introduce a novel algorithm
named ""CARE"" for network embedding that can be used for different types of
networks including weighted, directed and complex. Current methods try to
preserve local neighborhood information of nodes, whereas the proposed method
utilizes local neighborhood and community information of network nodes to cover
both local and global structure of social networks. CARE builds customized
paths, which are consisted of local and global structure of network nodes, as a
basis for network embedding and uses the Skip-gram model to learn
representation vector of nodes. Subsequently, stochastic gradient descent is
applied to optimize our objective function and learn the final representation
of nodes. Our method can be scalable when new nodes are appended to network
without information loss. Parallelize generation of customized random walks is
also used for speeding up CARE. We evaluate the performance of CARE on multi
label classification and link prediction tasks. Experimental results on various
networks indicate that the proposed method outperforms others in both Micro and
Macro-f1 measures for different size of training data.
",Network Embedding,network embedding
"  We prove that $\tilde{\Theta}(k d^2 / \varepsilon^2)$ samples are necessary
and sufficient for learning a mixture of $k$ Gaussians in $\mathbb{R}^d$, up to
error $\varepsilon$ in total variation distance. This improves both the known
upper bounds and lower bounds for this problem. For mixtures of axis-aligned
Gaussians, we show that $\tilde{O}(k d / \varepsilon^2)$ samples suffice,
matching a known lower bound. Moreover, these results hold in the
agnostic-learning/robust-estimation setting as well, where the target
distribution is only approximately a mixture of Gaussians.
  The upper bound is shown using a novel technique for distribution learning
based on a notion of `compression.' Any class of distributions that allows such
a compression scheme can also be learned with few samples. Moreover, if a class
of distributions has such a compression scheme, then so do the classes of
products and mixtures of those distributions. The core of our main result is
showing that the class of Gaussians in $\mathbb{R}^d$ admits a small-sized
compression scheme.
",Sample Complexity of Gaussian Mixture Learning,machine learning
"  Both resources in the natural environment and concepts in a semantic space
are distributed ""patchily"", with large gaps in between the patches. To describe
people's internal and external foraging behavior, various random walk models
have been proposed. In particular, internal foraging has been modeled as
sampling: in order to gather relevant information for making a decision, people
draw samples from a mental representation using random-walk algorithms such as
Markov chain Monte Carlo (MCMC). However, two common empirical observations
argue against simple sampling algorithms such as MCMC. First, the spatial
structure is often best described by a L\'evy flight distribution: the
probability of the distance between two successive locations follows a
power-law on the distances. Second, the temporal structure of the sampling that
humans and other animals produce have long-range, slowly decaying serial
correlations characterized as $1/f$-like fluctuations. We propose that mental
sampling is not done by simple MCMC, but is instead adapted to multimodal
representations and is implemented by Metropolis-coupled Markov chain Monte
Carlo (MC$^3$), one of the first algorithms developed for sampling from
multimodal distributions. MC$^3$ involves running multiple Markov chains in
parallel but with target distributions of different temperatures, and it swaps
the states of the chains whenever a better location is found. Heated chains
more readily traverse valleys in the probability landscape to propose moves to
far-away peaks, while the colder chains make the local steps that explore the
current peak or patch. We show that MC$^3$ generates distances between
successive samples that follow a L\'evy flight distribution and $1/f$-like
serial correlations, providing a single mechanistic account of these two
puzzling empirical phenomena.
",Human Mental Sampling Mechanisms,random walk models for internal and external foraging behavior
"  We study learning algorithms that are restricted to using a small amount of
information from their input sample. We introduce a category of learning
algorithms we term $d$-bit information learners, which are algorithms whose
output conveys at most $d$ bits of information of their input. A central theme
in this work is that such algorithms generalize.
  We focus on the learning capacity of these algorithms, and prove sample
complexity bounds with tight dependencies on the confidence and error
parameters. We also observe connections with well studied notions such as
sample compression schemes, Occam's razor, PAC-Bayes and differential privacy.
  We discuss an approach that allows us to prove upper bounds on the amount of
information that algorithms reveal about their inputs, and also provide a lower
bound by showing a simple concept class for which every (possibly randomized)
empirical risk minimizer must reveal a lot of information. On the other hand,
we show that in the distribution-dependent setting every VC class has empirical
risk minimizers that do not reveal a lot of information.
",Information-Theoretic Limits in Machine Learning,learning with limited information
"  Many machine learning problems can be formulated as consensus optimization
problems which can be solved efficiently via a cooperative multi-agent system.
However, the agents in the system can be unreliable due to a variety of
reasons: noise, faults and attacks. Providing erroneous updates leads the
optimization process in a wrong direction, and degrades the performance of
distributed machine learning algorithms. This paper considers the problem of
decentralized learning using ADMM in the presence of unreliable agents. First,
we rigorously analyze the effect of erroneous updates (in ADMM learning
iterations) on the convergence behavior of multi-agent system. We show that the
algorithm linearly converges to a neighborhood of the optimal solution under
certain conditions and characterize the neighborhood size analytically. Next,
we provide guidelines for network design to achieve a faster convergence. We
also provide conditions on the erroneous updates for exact convergence to the
optimal solution. Finally, to mitigate the influence of unreliable agents, we
propose \textsf{ROAD}, a robust variant of ADMM, and show its resilience to
unreliable agents with an exact convergence to the optimum.
",Robust Decentralized Learning with Unreliable Agents,decentralized machine learning in the presence of unreliable agents
"  In order to autonomously learn wide repertoires of complex skills, robots
must be able to learn from their own autonomously collected data, without human
supervision. One learning signal that is always available for autonomously
collected data is prediction: if a robot can learn to predict the future, it
can use this predictive model to take actions to produce desired outcomes, such
as moving an object to a particular location. However, in complex open-world
scenarios, designing a representation for prediction is difficult. In this
work, we instead aim to enable self-supervised robotic learning through direct
video prediction: instead of attempting to design a good representation, we
directly predict what the robot will see next, and then use this model to
achieve desired goals. A key challenge in video prediction for robotic
manipulation is handling complex spatial arrangements such as occlusions. To
that end, we introduce a video prediction model that can keep track of objects
through occlusion by incorporating temporal skip-connections. Together with a
novel planning criterion and action space formulation, we demonstrate that this
model substantially outperforms prior work on video prediction-based control.
Our results show manipulation of objects not seen during training, handling
multiple objects, and pushing objects around obstructions. These results
represent a significant advance in the range and complexity of skills that can
be performed entirely with self-supervised robotic learning.
",Self-Supervised Robotic Learning for Object Manipulation,autonomous robot learning through video prediction
"  In this work, we propose an infinite restricted Boltzmann machine~(RBM),
whose maximum likelihood estimation~(MLE) corresponds to a constrained convex
optimization. We consider the Frank-Wolfe algorithm to solve the program, which
provides a sparse solution that can be interpreted as inserting a hidden unit
at each iteration, so that the optimization process takes the form of a
sequence of finite models of increasing complexity. As a side benefit, this can
be used to easily and efficiently identify an appropriate number of hidden
units during the optimization. The resulting model can also be used as an
initialization for typical state-of-the-art RBM training algorithms such as
contrastive divergence, leading to models with consistently higher test
likelihood than random initialization.
",Restricted Boltzmann Machine Optimization,machine learning
"  Detect facial keypoints is a critical element in face recognition. However,
there is difficulty to catch keypoints on the face due to complex influences
from original images, and there is no guidance to suitable algorithms. In this
paper, we study different algorithms that can be applied to locate keyponits.
Specifically: our framework (1)prepare the data for further investigation
(2)Using PCA and LBP to process the data (3) Apply different algorithms to
analysis data, including linear regression models, tree based model, neural
network and convolutional neural network, etc. Finally we will give our
conclusion and further research topic. A comprehensive set of experiments on
dataset demonstrates the effectiveness of our framework.
",Facial Keypoint Detection Algorithms,facial keypoint detection
"  Convolutional neural networks (CNNs) are widely used in many image
recognition tasks due to their extraordinary performance. However, training a
good CNN model can still be a challenging task. In a training process, a CNN
model typically learns a large number of parameters over time, which usually
results in different performance. Often, it is difficult to explore the
relationships between the learned parameters and the model performance due to a
large number of parameters and different random initializations. In this paper,
we present a visual analytics approach to compare two different snapshots of a
trained CNN model taken after different numbers of epochs, so as to provide
some insight into the design or the training of a better CNN model. Our system
compares snapshots by exploring the differences in operation parameters and the
corresponding blob data at different levels. A case study has been conducted to
demonstrate the effectiveness of our system.
",Visualizing CNN Model Training,visual analytics for convolutional neural networks
"  In this paper, we propose a generative model which learns the relationship
between language and human action in order to generate a human action sequence
given a sentence describing human behavior. The proposed generative model is a
generative adversarial network (GAN), which is based on the sequence to
sequence (SEQ2SEQ) model. Using the proposed generative network, we can
synthesize various actions for a robot or a virtual agent using a text encoder
recurrent neural network (RNN) and an action decoder RNN. The proposed
generative network is trained from 29,770 pairs of actions and sentence
annotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video
dataset. We demonstrate that the network can generate human-like actions which
can be transferred to a Baxter robot, such that the robot performs an action
based on a provided sentence. Results show that the proposed generative network
correctly models the relationship between language and action and can generate
a diverse set of actions from the same sentence.
",Language-to-Action Generation,natural language processing (nlp)
"  Nonconvex optimization problems arise in different research fields and arouse
lots of attention in signal processing, statistics and machine learning. In
this work, we explore the accelerated proximal gradient method and some of its
variants which have been shown to converge under nonconvex context recently. We
show that a novel variant proposed here, which exploits adaptive momentum and
block coordinate update with specific update rules, further improves the
performance of a broad class of nonconvex problems. In applications to sparse
linear regression with regularizations like Lasso, grouped Lasso, capped
$\ell_1$ and SCAP, the proposed scheme enjoys provable local linear
convergence, with experimental justification.
",Accelerated Proximal Gradient Methods for Nonconvex Optimization,non-convex optimization
"  Recent advances in weakly supervised classification allow us to train a
classifier only from positive and unlabeled (PU) data. However, existing PU
classification methods typically require an accurate estimate of the
class-prior probability, which is a critical bottleneck particularly for
high-dimensional data. This problem has been commonly addressed by applying
principal component analysis in advance, but such unsupervised dimension
reduction can collapse underlying class structure. In this paper, we propose a
novel representation learning method from PU data based on the
information-maximization principle. Our method does not require class-prior
estimation and thus can be used as a preprocessing method for PU
classification. Through experiments, we demonstrate that our method combined
with deep neural networks highly improves the accuracy of PU class-prior
estimation, leading to state-of-the-art PU classification performance.
",Weakly Supervised Classification,weakly supervised classification
"  Embed-to-control (E2C) is a model for solving high-dimensional optimal
control problems by combining variational auto-encoders with locally-optimal
controllers. However, the E2C model suffers from two major drawbacks: 1) its
objective function does not correspond to the likelihood of the data sequence
and 2) the variational encoder used for embedding typically has large
variational approximation error, especially when there is noise in the system
dynamics. In this paper, we present a new model for learning robust
locally-linear controllable embedding (RCE). Our model directly estimates the
predictive conditional density of the future observation given the current one,
while introducing the bottleneck between the current and future observations.
Although the bottleneck provides a natural embedding candidate for control, our
RCE model introduces additional specific structures in the generative graphical
model so that the model dynamics can be robustly linearized. We also propose a
principled variational approximation of the embedding posterior that takes the
future observation into account, and thus, makes the variational approximation
more robust against the noise. Experimental results show that RCE outperforms
the E2C model, and does so significantly when the underlying dynamics is noisy.
",Robust Locally-Linear Controllable Embeddings for Optimal Control,robust locally-linear controllable embedding for optimal control
"  In this study, we systematically investigate the impact of class imbalance on
classification performance of convolutional neural networks (CNNs) and compare
frequently used methods to address the issue. Class imbalance is a common
problem that has been comprehensively studied in classical machine learning,
yet very limited systematic research is available in the context of deep
learning. In our study, we use three benchmark datasets of increasing
complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of
imbalance on classification and perform an extensive comparison of several
methods to address the issue: oversampling, undersampling, two-phase training,
and thresholding that compensates for prior class probabilities. Our main
evaluation metric is area under the receiver operating characteristic curve
(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is
associated with notable difficulties in the context of imbalanced data. Based
on results from our experiments we conclude that (i) the effect of class
imbalance on classification performance is detrimental; (ii) the method of
addressing class imbalance that emerged as dominant in almost all analyzed
scenarios was oversampling; (iii) oversampling should be applied to the level
that completely eliminates the imbalance, whereas the optimal undersampling
ratio depends on the extent of imbalance; (iv) as opposed to some classical
machine learning models, oversampling does not cause overfitting of CNNs; (v)
thresholding should be applied to compensate for prior class probabilities when
overall number of properly classified cases is of interest.
",Class Imbalance in Convolutional Neural Networks,class imbalance in convolutional neural networks
"  We analyze the dynamics of an online algorithm for independent component
analysis in the high-dimensional scaling limit. As the ambient dimension tends
to infinity, and with proper time scaling, we show that the time-varying joint
empirical measure of the target feature vector and the estimates provided by
the algorithm will converge weakly to a deterministic measured-valued process
that can be characterized as the unique solution of a nonlinear PDE. Numerical
solutions of this PDE, which involves two spatial variables and one time
variable, can be efficiently obtained. These solutions provide detailed
information about the performance of the ICA algorithm, as many practical
performance metrics are functionals of the joint empirical measures. Numerical
simulations show that our asymptotic analysis is accurate even for moderate
dimensions. In addition to providing a tool for understanding the performance
of the algorithm, our PDE analysis also provides useful insight. In particular,
in the high-dimensional limit, the original coupled dynamics associated with
the algorithm will be asymptotically ""decoupled"", with each coordinate
independently solving a 1-D effective minimization problem via stochastic
gradient descent. Exploiting this insight to design new algorithms for
achieving optimal trade-offs between computational and statistical efficiency
may prove an interesting line of future research.
",Independent Component Analysis Dynamics,independent component analysis in high-dimensional spaces
"  Policy evaluation or value function or Q-function approximation is a key
procedure in reinforcement learning (RL). It is a necessary component of policy
iteration and can be used for variance reduction in policy gradient methods.
Therefore its quality has a significant impact on most RL algorithms. Motivated
by manifold regularized learning, we propose a novel kernelized policy
evaluation method that takes advantage of the intrinsic geometry of the state
space learned from data, in order to achieve better sample efficiency and
higher accuracy in Q-function approximation. Applying the proposed method in
the Least-Squares Policy Iteration (LSPI) framework, we observe superior
performance compared to widely used parametric basis functions on two standard
benchmarks in terms of policy quality.
",Kernelized Policy Evaluation in Reinforcement Learning,reinforcement learning
"  ""How much energy is consumed for an inference made by a convolutional neural
network (CNN)?"" With the increased popularity of CNNs deployed on the
wide-spectrum of platforms (from mobile devices to workstations), the answer to
this question has drawn significant attention. From lengthening battery life of
mobile devices to reducing the energy bill of a datacenter, it is important to
understand the energy efficiency of CNNs during serving for making an
inference, before actually training the model. In this work, we propose
NeuralPower: a layer-wise predictive framework based on sparse polynomial
regression, for predicting the serving energy consumption of a CNN deployed on
any GPU platform. Given the architecture of a CNN, NeuralPower provides an
accurate prediction and breakdown for power and runtime across all layers in
the whole network, helping machine learners quickly identify the power,
runtime, or energy bottlenecks. We also propose the ""energy-precision ratio""
(EPR) metric to guide machine learners in selecting an energy-efficient CNN
architecture that better trades off the energy consumption and prediction
accuracy. The experimental results show that the prediction accuracy of the
proposed NeuralPower outperforms the best published model to date, yielding an
improvement in accuracy of up to 68.5%. We also assess the accuracy of
predictions at the network level, by predicting the runtime, power, and energy
of state-of-the-art CNN architectures, achieving an average accuracy of 88.24%
in runtime, 88.34% in power, and 97.21% in energy. We comprehensively
corroborate the effectiveness of NeuralPower as a powerful framework for
machine learners by testing it on different GPU platforms and Deep Learning
software tools.
",Energy Efficiency of CNNs,energy efficiency of convolutional neural networks
"  We propose a general framework for interactively learning models, such as
(binary or non-binary) classifiers, orderings/rankings of items, or clusterings
of data points. Our framework is based on a generalization of Angluin's
equivalence query model and Littlestone's online learning model: in each
iteration, the algorithm proposes a model, and the user either accepts it or
reveals a specific mistake in the proposal. The feedback is correct only with
probability $p > 1/2$ (and adversarially incorrect with probability $1 - p$),
i.e., the algorithm must be able to learn in the presence of arbitrary noise.
The algorithm's goal is to learn the ground truth model using few iterations.
  Our general framework is based on a graph representation of the models and
user feedback. To be able to learn efficiently, it is sufficient that there be
a graph $G$ whose nodes are the models and (weighted) edges capture the user
feedback, with the property that if $s, s^*$ are the proposed and target
models, respectively, then any (correct) user feedback $s'$ must lie on a
shortest $s$-$s^*$ path in $G$. Under this one assumption, there is a natural
algorithm reminiscent of the Multiplicative Weights Update algorithm, which
will efficiently learn $s^*$ even in the presence of noise in the user's
feedback.
  From this general result, we rederive with barely any extra effort classic
results on learning of classifiers and a recent result on interactive
clustering; in addition, we easily obtain new interactive learning algorithms
for ordering/ranking.
",Interactive Learning Under Noisy Feedback,machine learning
"  This paper provides theoretical insights into why and how deep learning can
generalize well, despite its large capacity, complexity, possible algorithmic
instability, nonrobustness, and sharp minima, responding to an open question in
the literature. We also discuss approaches to provide non-vacuous
generalization guarantees for deep learning. Based on theoretical observations,
we propose new open problems and discuss the limitations of our results.
",Deep Learning Generalization Guarantees,deep learning generalization
"  To effectively control complex dynamical systems, accurate nonlinear models
are typically needed. However, these models are not always known. In this
paper, we present a data-driven approach based on Gaussian processes that
learns models of quadrotors operating in partially unknown environments. What
makes this challenging is that if the learning process is not carefully
controlled, the system will go unstable, i.e., the quadcopter will crash. To
this end, barrier certificates are employed for safe learning. The barrier
certificates establish a non-conservative forward invariant safe region, in
which high probability safety guarantees are provided based on the statistics
of the Gaussian Process. A learning controller is designed to efficiently
explore those uncertain states and expand the barrier certified safe region
based on an adaptive sampling scheme. In addition, a recursive Gaussian Process
prediction method is developed to learn the complex quadrotor dynamics in
real-time. Simulation results are provided to demonstrate the effectiveness of
the proposed approach.
",Safe Learning of Quadrotor Dynamics,data-driven control of quadrotors in partially unknown environments
"  Excellent ranking power along with well calibrated probability estimates are
needed in many classification tasks. In this paper, we introduce a technique,
Calibrated Boosting-Forest that captures both. This novel technique is an
ensemble of gradient boosting machines that can support both continuous and
binary labels. While offering superior ranking power over any individual
regression or classification model, Calibrated Boosting-Forest is able to
preserve well calibrated posterior probabilities. Along with these benefits, we
provide an alternative to the tedious step of tuning gradient boosting
machines. We demonstrate that tuning Calibrated Boosting-Forest can be reduced
to a simple hyper-parameter selection. We further establish that increasing
this hyper-parameter improves the ranking performance under a diminishing
return. We examine the effectiveness of Calibrated Boosting-Forest on
ligand-based virtual screening where both continuous and binary labels are
available and compare the performance of Calibrated Boosting-Forest with
logistic regression, gradient boosting machine and deep learning. Calibrated
Boosting-Forest achieved an approximately 48% improvement compared to a
state-of-art deep learning model. Moreover, it achieved around 95% improvement
on probability quality measurement compared to the best individual gradient
boosting machine. Calibrated Boosting-Forest offers a benchmark demonstration
that in the field of ligand-based virtual screening, deep learning is not the
universally dominant machine learning model and good calibrated probabilities
can better facilitate virtual screening process.
",Ensemble Methods for Calibrated Classification,machine learning
"  In this work, we show the intrinsic relations between optimal transportation
and convex geometry, especially the variational approach to solve Alexandrov
problem: constructing a convex polytope with prescribed face normals and
volumes. This leads to a geometric interpretation to generative models, and
leads to a novel framework for generative models. By using the optimal
transportation view of GAN model, we show that the discriminator computes the
Kantorovich potential, the generator calculates the transportation map. For a
large class of transportation costs, the Kantorovich potential can give the
optimal transportation map by a close-form formula. Therefore, it is sufficient
to solely optimize the discriminator. This shows the adversarial competition
can be avoided, and the computational architecture can be simplified.
Preliminary experimental results show the geometric method outperforms WGAN for
approximating probability measures with multiple clusters in low dimensional
space.
",Optimal Transportation in Generative Models,convex geometry and generative models
"  A successful grasp requires careful balancing of the contact forces. Deducing
whether a particular grasp will be successful from indirect measurements, such
as vision, is therefore quite challenging, and direct sensing of contacts
through touch sensing provides an appealing avenue toward more successful and
consistent robotic grasping. However, in order to fully evaluate the value of
touch sensing for grasp outcome prediction, we must understand how touch
sensing can influence outcome prediction accuracy when combined with other
modalities. Doing so using conventional model-based techniques is exceptionally
difficult. In this work, we investigate the question of whether touch sensing
aids in predicting grasp outcomes within a multimodal sensing framework that
combines vision and touch. To that end, we collected more than 9,000 grasping
trials using a two-finger gripper equipped with GelSight high-resolution
tactile sensors on each finger, and evaluated visuo-tactile deep neural network
models to directly predict grasp outcomes from either modality individually,
and from both modalities together. Our experimental results indicate that
incorporating tactile readings substantially improve grasping performance.
",Multimodal Sensing for Robotic Grasping,robotics: grasp outcome prediction with multimodal sensing
"  The success of deep convolutional neural network (CNN) in computer vision
especially image classification problems requests a new information theory for
function of image, instead of image itself. In this article, after establishing
a deep mathematical connection between image classification problem and quantum
spin model, we propose to use entanglement entropy, a generalization of
classical Boltzmann-Shannon entropy, as a powerful tool to characterize the
information needed for representation of general function of image. We prove
that there is a sub-volume-law bound for entanglement entropy of target
functions of reasonable image classification problems. Therefore target
functions of image classification only occupy a small subspace of the whole
Hilbert space. As a result, a neural network with polynomial number of
parameters is efficient for representation of such target functions of image.
The concept of entanglement entropy can also be useful to characterize the
expressive power of different neural networks. For example, we show that to
maintain the same expressive power, number of channels $D$ in a convolutional
neural network should scale with the number of convolution layers $n_c$ as
$D\sim D_0^{\frac{1}{n_c}}$. Therefore, deeper CNN with large $n_c$ is more
efficient than shallow ones.
",Entanglement Entropy in Image Classification,quantum information theory for deep learning
"  Matrix factorization techniques have been widely used as a method for
collaborative filtering for recommender systems. In recent times, different
variants of deep learning algorithms have been explored in this setting to
improve the task of making a personalized recommendation with user-item
interaction data. The idea that the mapping between the latent user or item
factors and the original features is highly nonlinear suggest that classical
matrix factorization techniques are no longer sufficient. In this paper, we
propose a multilayer nonlinear semi-nonnegative matrix factorization method,
with the motivation that user-item interactions can be modeled more accurately
using a linear combination of non-linear item features. Firstly, we learn
latent factors for representations of users and items from the designed
multilayer nonlinear Semi-NMF approach using explicit ratings. Secondly, the
architecture built is compared with deep-learning algorithms like Restricted
Boltzmann Machine and state-of-the-art Deep Matrix factorization techniques. By
using both supervised rate prediction task and unsupervised clustering in
latent item space, we demonstrate that our proposed approach achieves better
generalization ability in prediction as well as comparable representation
ability as deep matrix factorization in the clustering task.
",Deep Learning in Recommender Systems,deep learning for recommender systems
"  Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.
",Scalable Graph Learning,scalable graph learning
"  Person re-identification (Re-ID) usually suffers from noisy samples with
background clutter and mutual occlusion, which makes it extremely difficult to
distinguish different individuals across the disjoint camera views. In this
paper, we propose a novel deep self-paced learning (DSPL) algorithm to
alleviate this problem, in which we apply a self-paced constraint and symmetric
regularization to help the relative distance metric training the deep neural
network, so as to learn the stable and discriminative features for person
Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive
the adaptive weights to samples based on both the training loss and model age.
As a result, the high-confidence fidelity samples will be emphasized and the
low-confidence noisy samples will be suppressed at early stage of the whole
training process. Such a learning regime is naturally implemented under a
self-paced learning (SPL) framework, in which samples weights are adaptively
updated based on both model age and sample loss using an alternative
optimization method. Secondly, we introduce a symmetric regularizer term to
revise the asymmetric gradient back-propagation derived by the relative
distance metric, so as to simultaneously minimize the intra-class distance and
maximize the inter-class distance in each triplet unit. Finally, we build a
part-based deep neural network, in which the features of different body parts
are first discriminately learned in the lower convolutional layers and then
fused in the higher fully connected layers. Experiments on several benchmark
datasets have demonstrated the superior performance of our method as compared
with the state-of-the-art approaches.
",Person Re-identification with Noisy Samples,person re-identification
"  In this paper, we investigate the effectiveness of deep learning techniques
for lung nodule classification in computed tomography scans. Using less than
10,000 training examples, our deep networks perform two times better than a
standard radiology software. Visualization of the networks' neurons reveals
semantically meaningful features that are consistent with the clinical
knowledge and radiologists' perception. Our paper also proposes a novel
framework for rapidly adapting deep networks to the radiologists' feedback, or
change in the data due to the shift in sensor's resolution or patient
population. The classification accuracy of our approach remains above 80% while
popular deep networks' accuracy is around chance. Finally, we provide in-depth
analysis of our framework by asking a radiologist to examine important
networks' features and perform blind re-labeling of networks' mistakes.
",Deep Learning in Lung Nodule Classification,medical imaging
"  We consider a repeated newsvendor problem where the inventory manager has no
prior information about the demand, and can access only censored/sales data. In
analogy to multi-armed bandit problems, the manager needs to simultaneously
""explore"" and ""exploit"" with her inventory decisions, in order to minimize the
cumulative cost. We make no probabilistic assumptions---importantly,
independence or time stationarity---regarding the mechanism that creates the
demand sequence. Our goal is to shed light on the hardness of the problem, and
to develop policies that perform well with respect to the regret criterion,
that is, the difference between the cumulative cost of a policy and that of the
best fixed action/static inventory decision in hindsight, uniformly over all
feasible demand sequences. We show that a simple randomized policy, termed the
Exponentially Weighted Forecaster, combined with a carefully designed cost
estimator, achieves optimal scaling of the expected regret (up to logarithmic
factors) with respect to all three key primitives: the number of time periods,
the number of inventory decisions available, and the demand support. Through
this result, we derive an important insight: the benefit from ""information
stalking"" as well as the cost of censoring are both negligible in this dynamic
learning problem, at least with respect to the regret criterion. Furthermore,
we modify the proposed policy in order to perform well in terms of the tracking
regret, that is, using as benchmark the best sequence of inventory decisions
that switches a limited number of times. Numerical experiments suggest that the
proposed approach outperforms existing ones (that are tailored to, or
facilitated by, time stationarity) on nonstationary demand models. Finally, we
extend the proposed approach and its analysis to a ""combinatorial"" version of
the repeated newsvendor problem.
",Inventory Management with Censored Demand Data,repeated newsvendor problem
"  This paper takes a step towards temporal reasoning in a dynamically changing
video, not in the pixel space that constitutes its frames, but in a latent
space that describes the non-linear dynamics of the objects in its world. We
introduce the Kalman variational auto-encoder, a framework for unsupervised
learning of sequential data that disentangles two latent representations: an
object's representation, coming from a recognition model, and a latent state
describing its dynamics. As a result, the evolution of the world can be
imagined and missing data imputed, both without the need to generate high
dimensional frames at each time step. The model is trained end-to-end on videos
of a variety of simulated physical systems, and outperforms competing methods
in generative and missing data imputation tasks.
",Temporal Reasoning in Video Analysis,temporal reasoning in dynamic video analysis
"  Recent research implies that training and inference of deep neural networks
(DNN) can be computed with low precision numerical representations of the
training/test data, weights and gradients without a general loss in accuracy.
The benefit of such compact representations is twofold: they allow a
significant reduction of the communication bottleneck in distributed DNN
training and faster neural network implementations on hardware accelerators
like FPGAs. Several quantization methods have been proposed to map the original
32-bit floating point problem to low-bit representations. While most related
publications validate the proposed approach on a single DNN topology, it
appears to be evident, that the optimal choice of the quantization method and
number of coding bits is topology dependent. To this end, there is no general
theory available, which would allow users to derive the optimal quantization
during the design of a DNN topology. In this paper, we present a quantization
tool box for the TensorFlow framework. TensorQuant allows a transparent
quantization simulation of existing DNN topologies during training and
inference. TensorQuant supports generic quantization methods and allows
experimental evaluation of the impact of the quantization on single layers as
well as on the full topology. In a first series of experiments with
TensorQuant, we show an analysis of fix-point quantizations of popular CNN
topologies.
",Deep Neural Network Quantization,quantization methods for deep neural networks
"  In this work we consider the task of constructing prediction intervals in an
inductive batch setting. We present a discriminative learning framework which
optimizes the expected error rate under a budget constraint on the interval
sizes. Most current methods for constructing prediction intervals offer
guarantees for a single new test point. Applying these methods to multiple test
points can result in a high computational overhead and degraded statistical
guarantees. By focusing on expected errors, our method allows for variability
in the per-example conditional error rates. As we demonstrate both analytically
and empirically, this flexibility can increase the overall accuracy, or
alternatively, reduce the average interval size.
  While the problem we consider is of a regressive flavor, the loss we use is
combinatorial. This allows us to provide PAC-style, finite-sample guarantees.
Computationally, we show that our original objective is NP-hard, and suggest a
tractable convex surrogate. We conclude with a series of experimental
evaluations.
",Prediction Interval Construction,machine learning: predictive interval construction
"  Classifiers and rating scores are prone to implicitly codifying biases, which
may be present in the training data, against protected classes (i.e., age,
gender, or race). So it is important to understand how to design classifiers
and scores that prevent discrimination in predictions. This paper develops
computationally tractable algorithms for designing accurate but fair support
vector machines (SVM's). Our approach imposes a constraint on the covariance
matrices conditioned on each protected class, which leads to a nonconvex
quadratic constraint in the SVM formulation. We develop iterative algorithms to
compute fair linear and kernel SVM's, which solve a sequence of relaxations
constructed using a spectral decomposition of the nonconvex constraint. Its
effectiveness in achieving high prediction accuracy while ensuring fairness is
shown through numerical experiments on several data sets.
",Fairness in Machine Learning,fair machine learning
"  The choice of activation functions in deep networks has a significant effect
on the training dynamics and task performance. Currently, the most successful
and widely-used activation function is the Rectified Linear Unit (ReLU).
Although various hand-designed alternatives to ReLU have been proposed, none
have managed to replace it due to inconsistent gains. In this work, we propose
to leverage automatic search techniques to discover new activation functions.
Using a combination of exhaustive and reinforcement learning-based search, we
discover multiple novel activation functions. We verify the effectiveness of
the searches by conducting an empirical evaluation with the best discovered
activation function. Our experiments show that the best discovered activation
function, $f(x) = x \cdot \text{sigmoid}(\beta x)$, which we name Swish, tends
to work better than ReLU on deeper models across a number of challenging
datasets. For example, simply replacing ReLUs with Swish units improves top-1
classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for
Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it
easy for practitioners to replace ReLUs with Swish units in any neural network.
",Activation Function Optimization,novel activation functions
"  We develop a method for policy architecture search and adaptation via
gradient-free optimization which can learn to perform autonomous driving tasks.
By learning from both demonstration and environmental reward we develop a model
that can learn with relatively few early catastrophic failures. We first learn
an architecture of appropriate complexity to perceive aspects of world state
relevant to the expert demonstration, and then mitigate the effect of
domain-shift during deployment by adapting a policy demonstrated in a source
domain to rewards obtained in a target environment. We show that our approach
allows safer learning than baseline methods, offering a reduced cumulative
crash metric over the agent's lifetime as it learns to drive in a realistic
simulated environment.
",Autonomous Driving Policy Optimization,autonomous vehicle learning
"  Image classification is the task of assigning to an input image a label from
a fixed set of categories. One of its most important applicative fields is that
of robotics, in particular the needing of a robot to be aware of what's around
and the consequent exploitation of that information as a benefit for its tasks.
In this work we consider the problem of a robot that enters a new environment
and wants to understand visual data coming from its camera, so to extract
knowledge from them. As main novelty we want to overcome the needing of a
physical robot, as it could be expensive and unhandy, so to hopefully enhance,
speed up and ease the research in this field. That's why we propose to develop
an application for a mobile platform that wraps several deep visual recognition
tasks. First we deal with a simple Image classification, testing a model
obtained from an AlexNet trained on the ILSVRC 2012 dataset. Several photo
settings are considered to better understand which factors affect most the
quality of classification. For the same purpose we are interested to integrate
the classification task with an extra module dealing with segmentation of the
object inside the image. In particular we propose a technique for extracting
the object shape and moving out all the background, so to focus the
classification only on the region occupied by the object. Another significant
task that is included is that of object discovery. Its purpose is to simulate
the situation in which the robot needs a certain object to complete one of its
activities. It starts searching for what it needs by looking around and trying
to understand the location of the object by scanning the surrounding
environment. Finally we provide a tool for dealing with the creation of
customized task-specific databases, meant to better suit to one's needing in a
particular vision task.
",Visual Object Recognition for Robotics,image classification and object recognition for robotics
"  Recent advances in policy gradient methods and deep learning have
demonstrated their applicability for complex reinforcement learning problems.
However, the variance of the performance gradient estimates obtained from the
simulation is often excessive, leading to poor sample efficiency. In this
paper, we apply the stochastic variance reduced gradient descent (SVRG) to
model-free policy gradient to significantly improve the sample-efficiency. The
SVRG estimation is incorporated into a trust-region Newton conjugate gradient
framework for the policy optimization. On several Mujoco tasks, our method
achieves significantly better performance compared to the state-of-the-art
model-free policy gradient methods in robotic continuous control such as trust
region policy optimization (TRPO)
",Variance Reduction in Policy Gradient Methods,reinforcement learning
"  Deep neural networks are vulnerable to adversarial examples, which poses
security concerns on these algorithms due to the potentially severe
consequences. Adversarial attacks serve as an important surrogate to evaluate
the robustness of deep learning models before they are deployed. However, most
of existing adversarial attacks can only fool a black-box model with a low
success rate. To address this issue, we propose a broad class of momentum-based
iterative algorithms to boost adversarial attacks. By integrating the momentum
term into the iterative process for attacks, our methods can stabilize update
directions and escape from poor local maxima during the iterations, resulting
in more transferable adversarial examples. To further improve the success rates
for black-box attacks, we apply momentum iterative algorithms to an ensemble of
models, and show that the adversarially trained models with a strong defense
ability are also vulnerable to our black-box attacks. We hope that the proposed
methods will serve as a benchmark for evaluating the robustness of various deep
models and defense methods. With this method, we won the first places in NIPS
2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack
competitions.
",Adversarial Attacks on Deep Learning Models,adversarial attacks on deep neural networks
"  We study parameter estimation in Nonlinear Factor Analysis (NFA) where the
generative model is parameterized by a deep neural network. Recent work has
focused on learning such models using inference (or recognition) networks; we
identify a crucial problem when modeling large, sparse, high-dimensional
datasets -- underfitting. We study the extent of underfitting, highlighting
that its severity increases with the sparsity of the data. We propose methods
to tackle it via iterative optimization inspired by stochastic variational
inference \citep{hoffman2013stochastic} and improvements in the sparse data
representation used for inference. The proposed techniques drastically improve
the ability of these powerful models to fit sparse data, achieving
state-of-the-art results on a benchmark text-count dataset and excellent
results on the task of top-N recommendation.
",Underfitting in Nonlinear Factor Analysis,nonlinear factor analysis and underfitting in deep neural networks
"  We propose a framework to understand the unprecedented performance and
robustness of deep neural networks using field theory. Correlations between the
weights within the same layer can be described by symmetries in that layer, and
networks generalize better if such symmetries are broken to reduce the
redundancies of the weights. Using a two parameter field theory, we find that
the network can break such symmetries itself towards the end of training in a
process commonly known in physics as spontaneous symmetry breaking. This
corresponds to a network generalizing itself without any user input layers to
break the symmetry, but by communication with adjacent layers. In the layer
decoupling limit applicable to residual networks (He et al., 2015), we show
that the remnant symmetries that survive the non-linear layers are
spontaneously broken. The Lagrangian for the non-linear and weight layers
together has striking similarities with the one in quantum field theory of a
scalar. Using results from quantum field theory we show that our framework is
able to explain many experimentally observed phenomena,such as training on
random labels with zero error (Zhang et al., 2017), the information bottleneck,
the phase transition out of it and gradient variance explosion (Shwartz-Ziv &
Tishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.
",Deep Learning Symmetry Breaking,applications of field theory in deep learning
"  Consider the problem of approximating the optimal policy of a Markov decision
process (MDP) by sampling state transitions. In contrast to existing
reinforcement learning methods that are based on successive approximations to
the nonlinear Bellman equation, we propose a Primal-Dual $\pi$ Learning method
in light of the linear duality between the value and policy. The $\pi$ learning
method is model-free and makes primal-dual updates to the policy and value
vectors as new data are revealed. For infinite-horizon undiscounted Markov
decision process with finite state space $S$ and finite action space $A$, the
$\pi$ learning method finds an $\epsilon$-optimal policy using the following
number of sample transitions $$ \tilde{O}( \frac{(\tau\cdot t^*_{mix})^2 |S|
|A| }{\epsilon^2} ),$$ where $t^*_{mix}$ is an upper bound of mixing times
across all policies and $\tau$ is a parameter characterizing the range of
stationary distributions across policies. The $\pi$ learning method also
applies to the computational problem of MDP where the transition probabilities
and rewards are explicitly given as the input. In the case where each state
transition can be sampled in $\tilde{O}(1)$ time, the $\pi$ learning method
gives a sublinear-time algorithm for solving the averaged-reward MDP.
",Markov Decision Process Approximation,reinforcement learning
"  In order for robots to perform mission-critical tasks, it is essential that
they are able to quickly adapt to changes in their environment as well as to
injuries and or other bodily changes. Deep reinforcement learning has been
shown to be successful in training robot control policies for operation in
complex environments. However, existing methods typically employ only a single
policy. This can limit the adaptability since a large environmental
modification might require a completely different behavior compared to the
learning environment. To solve this problem, we propose Map-based Multi-Policy
Reinforcement Learning (MMPRL), which aims to search and store multiple
policies that encode different behavioral features while maximizing the
expected reward in advance of the environment change. Thanks to these policies,
which are stored into a multi-dimensional discrete map according to its
behavioral feature, adaptation can be performed within reasonable time without
retraining the robot. An appropriate pre-trained policy from the map can be
recalled using Bayesian optimization. Our experiments show that MMPRL enables
robots to quickly adapt to large changes without requiring any prior knowledge
on the type of injuries that could occur. A highlight of the learned behaviors
can be found here: https://youtu.be/QwInbilXNOE .
",Robot Adaptability,adaptive robotics
"  We propose two deep neural network architectures for classification of
arbitrary-length electrocardiogram (ECG) recordings and evaluate them on the
atrial fibrillation (AF) classification data set provided by the PhysioNet/CinC
Challenge 2017. The first architecture is a deep convolutional neural network
(CNN) with averaging-based feature aggregation across time. The second
architecture combines convolutional layers for feature extraction with
long-short term memory (LSTM) layers for temporal aggregation of features. As a
key ingredient of our training procedure we introduce a simple data
augmentation scheme for ECG data and demonstrate its effectiveness in the AF
classification task at hand. The second architecture was found to outperform
the first one, obtaining an $F_1$ score of $82.1$% on the hidden challenge
testing set.
",Deep Learning for ECG Analysis,deep learning for ecg signal processing
"  Forecasting thermal load is a key component for the majority of optimization
solutions for controlling district heating and cooling systems. Recent studies
have analysed the results of a number of data-driven methods applied to thermal
load forecasting, this paper presents the results of combining a collection of
these individual methods in an expert system. The expert system will combine
multiple thermal load forecasts in a way that it always tracks the best expert
in the system. This solution is tested and validated using a thermal load
dataset of 27 months obtained from 10 residential buildings located in Rottne,
Sweden together with outdoor temperature information received from a weather
forecast service. The expert system is composed of the following data-driven
methods: linear regression, extremely randomized trees regression, feed-forward
neural network and support vector machine. The results of the proposed solution
are compared with the results of the individual methods.
",Thermal Load Forecasting,thermal load forecasting using expert system
"  Towards the vision of translating code that implements an algorithm from one
programming language into another, this paper proposes an approach for
automated program classification using bilateral tree-based convolutional
neural networks (BiTBCNNs). It is layered on top of two tree-based
convolutional neural networks (TBCNNs), each of which recognizes the algorithm
of code written in an individual programming language. The combination layer of
the networks recognizes the similarities and differences among code in
different programming languages. The BiTBCNNs are trained using the source code
in different languages but known to implement the same algorithms and/or
functionalities. For a preliminary evaluation, we use 3591 Java and 3534 C++
code snippets from 6 algorithms we crawled systematically from GitHub. We
obtained over 90% accuracy in the cross-language binary classification task to
tell whether any given two code snippets implement the same algorithm. Also,
for the algorithm classification task, i.e., to predict which one of the six
algorithm labels is implemented by an arbitrary C++ code snippet, we achieved
over 80% precision.
",Program Translation,automated program classification
"  Black-box risk scoring models permeate our lives, yet are typically
proprietary or opaque. We propose Distill-and-Compare, a model distillation and
comparison approach to audit such models. To gain insight into black-box
models, we treat them as teachers, training transparent student models to mimic
the risk scores assigned by black-box models. We compare the student model
trained with distillation to a second un-distilled transparent model trained on
ground-truth outcomes, and use differences between the two models to gain
insight into the black-box model. Our approach can be applied in a realistic
setting, without probing the black-box model API. We demonstrate the approach
on four public data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending
Club. We also propose a statistical test to determine if a data set is missing
key features used to train the black-box model. Our test finds that the
ProPublica data is likely missing key feature(s) used in COMPAS.
",Audit of Black-Box Risk Scoring Models,model interpretability
"  The correlation length-scale next to the noise variance are the most used
hyperparameters for the Gaussian processes. Typically, stationary covariance
functions are used, which are only dependent on the distances between input
points and thus invariant to the translations in the input space. The
optimization of the hyperparameters is commonly done by maximizing the log
marginal likelihood. This works quite well, if the distances are uniform
distributed. In the case of a locally adapted or even sparse input space, the
prediction of a test point can be worse dependent of its position. A possible
solution to this, is the usage of a non-stationary covariance function, where
the hyperparameters are calculated by a deep neural network. So that the
correlation length scales and possibly the noise variance are dependent on the
test point. Furthermore, different types of covariance functions are trained
simultaneously, so that the Gaussian process prediction is an additive overlay
of different covariance matrices. The right covariance functions combination
and its hyperparameters are learned by the deep neural network. Additional, the
Gaussian process will be able to be trained by batches or online and so it can
handle arbitrarily large data sets. We call this framework Deep Gaussian
Covariance Network (DGCP). There are also further extensions to this framework
possible, for example sequentially dependent problems like time series or the
local mixture of experts. The basic framework and some extension possibilities
will be presented in this work. Moreover, a comparison to some recent state of
the art surrogate model methods will be performed, also for a time dependent
problem.
",Deep Gaussian Process Modeling,machine learning for gaussian processes
"  Hyperparameter optimization aims to find the optimal hyperparameter
configuration of a machine learning model, which provides the best performance
on a validation dataset. Manual search usually leads to get stuck in a local
hyperparameter configuration, and heavily depends on human intuition and
experience. A simple alternative of manual search is random/grid search on a
space of hyperparameters, which still undergoes extensive evaluations of
validation errors in order to find its best configuration. Bayesian
optimization that is a global optimization method for black-box functions is
now popular for hyperparameter optimization, since it greatly reduces the
number of validation error evaluations required, compared to random/grid
search. Bayesian optimization generally finds the best hyperparameter
configuration from random initialization without any prior knowledge. This
motivates us to let Bayesian optimization start from the configurations that
were successful on similar datasets, which are able to remarkably minimize the
number of evaluations. In this paper, we propose deep metric learning to learn
meta-features over datasets such that the similarity over them is effectively
measured by Euclidean distance between their associated meta-features. To this
end, we introduce a Siamese network composed of deep feature and meta-feature
extractors, where deep feature extractor provides a semantic representation of
each instance in a dataset and meta-feature extractor aggregates a set of deep
features to encode a single representation over a dataset. Then, our learned
meta-features are used to select a few datasets similar to the new dataset, so
that hyperparameters in similar datasets are adopted as initializations to
warm-start Bayesian hyperparameter optimization.
",Hyperparameter Optimization using Meta-Learning,bayesian optimization for hyperparameter tuning
"  We consider the homogeneous and the non-homogeneous convex relaxations for
combinatorial penalty functions defined on support sets. Our study identifies
key differences in the tightness of the resulting relaxations through the
notion of the lower combinatorial envelope of a set-function along with new
necessary conditions for support identification. We then propose a general
adaptive estimator for convex monotone regularizers, and derive new sufficient
conditions for support recovery in the asymptotic setting.
",Combinatorial Penalty Functions Relaxation,convex relaxations for combinatorial optimization
"  Entropic regularization is quickly emerging as a new standard in optimal
transport (OT). It enables to cast the OT computation as a differentiable and
unconstrained convex optimization problem, which can be efficiently solved
using the Sinkhorn algorithm. However, entropy keeps the transportation plan
strictly positive and therefore completely dense, unlike unregularized OT. This
lack of sparsity can be problematic in applications where the transportation
plan itself is of interest. In this paper, we explore regularizing the primal
and dual OT formulations with a strongly convex term, which corresponds to
relaxing the dual and primal constraints with smooth approximations. We show
how to incorporate squared $2$-norm and group lasso regularizations within that
framework, leading to sparse and group-sparse transportation plans. On the
theoretical side, we bound the approximation error introduced by regularizing
the primal and dual formulations. Our results suggest that, for the regularized
primal, the approximation error can often be smaller with squared $2$-norm than
with entropic regularization. We showcase our proposed framework on the task of
color transfer.
",Regularization in Optimal Transport,entropic regularization for optimal transport
"  With tens of thousands of electrocardiogram (ECG) records processed by mobile
cardiac event recorders every day, heart rhythm classification algorithms are
an important tool for the continuous monitoring of patients at risk. We utilise
an annotated dataset of 12,186 single-lead ECG recordings to build a diverse
ensemble of recurrent neural networks (RNNs) that is able to distinguish
between normal sinus rhythms, atrial fibrillation, other types of arrhythmia
and signals that are too noisy to interpret. In order to ease learning over the
temporal dimension, we introduce a novel task formulation that harnesses the
natural segmentation of ECG signals into heartbeats to drastically reduce the
number of time steps per sequence. Additionally, we extend our RNNs with an
attention mechanism that enables us to reason about which heartbeats our RNNs
focus on to make their decisions. Through the use of attention, our model
maintains a high degree of interpretability, while also achieving
state-of-the-art classification performance with an average F1 score of 0.79 on
an unseen test set (n=3,658).
",ECG Heart Rhythm Classification,heart rhythm classification using recurrent neural networks
"  Many iterative procedures in stochastic optimization exhibit a transient
phase followed by a stationary phase. During the transient phase the procedure
converges towards a region of interest, and during the stationary phase the
procedure oscillates in that region, commonly around a single point. In this
paper, we develop a statistical diagnostic test to detect such phase transition
in the context of stochastic gradient descent with constant learning rate. We
present theory and experiments suggesting that the region where the proposed
diagnostic is activated coincides with the convergence region. For a class of
loss functions, we derive a closed-form solution describing such region.
Finally, we suggest an application to speed up convergence of stochastic
gradient descent by halving the learning rate each time stationarity is
detected. This leads to a new variant of stochastic gradient descent, which in
many settings is comparable to state-of-art.
",Phase Transition Detection in Stochastic Optimization,phase transition detection in stochastic optimization
"  This paper presents the results and conclusions of our participation in the
Clickbait Challenge 2017 on automatic clickbait detection in social media. We
first describe linguistically-infused neural network models and identify
informative representations to predict the level of clickbaiting present in
Twitter posts. Our models allow to answer the question not only whether a post
is a clickbait or not, but to what extent it is a clickbait post e.g., not at
all, slightly, considerably, or heavily clickbaity using a score ranging from 0
to 1. We evaluate the predictive power of models trained on varied text and
image representations extracted from tweets. Our best performing model that
relies on the tweet text and linguistic markers of biased language extracted
from the tweet and the corresponding page yields mean squared error (MSE) of
0.04, mean absolute error (MAE) of 0.16 and R2 of 0.43 on the held-out test
data. For the binary classification setup (clickbait vs. non-clickbait), our
model achieved F1 score of 0.69. We have not found that image representations
combined with text yield significant performance improvement yet. Nevertheless,
this work is the first to present preliminary analysis of objects extracted
using Google Tensorflow object detection API from images in clickbait vs.
non-clickbait Twitter posts. Finally, we outline several steps to improve model
performance as a part of the future work.
",Clickbait Detection in Social Media,automatic clickbait detection in social media
"  Learning-based approaches to robotic manipulation are limited by the
scalability of data collection and accessibility of labels. In this paper, we
present a multi-task domain adaptation framework for instance grasping in
cluttered scenes by utilizing simulated robot experiments. Our neural network
takes monocular RGB images and the instance segmentation mask of a specified
target object as inputs, and predicts the probability of successfully grasping
the specified object for each candidate motor command. The proposed transfer
learning framework trains a model for instance grasping in simulation and uses
a domain-adversarial loss to transfer the trained model to real robots using
indiscriminate grasping data, which is available both in simulation and the
real world. We evaluate our model in real-world robot experiments, comparing it
with alternative model architectures as well as an indiscriminate grasping
baseline.
",Robot Learning for Grasping in Cluttered Scenes,domain adaptation in robotic manipulation
"  Deep learning-based robotic grasping has made significant progress thanks to
algorithmic improvements and increased data availability. However,
state-of-the-art models are often trained on as few as hundreds or thousands of
unique object instances, and as a result generalization can be a challenge.
  In this work, we explore a novel data generation pipeline for training a deep
neural network to perform grasp planning that applies the idea of domain
randomization to object synthesis. We generate millions of unique, unrealistic
procedurally generated objects, and train a deep neural network to perform
grasp planning on these objects.
  Since the distribution of successful grasps for a given object can be highly
multimodal, we propose an autoregressive grasp planning model that maps sensor
inputs of a scene to a probability distribution over possible grasps. This
model allows us to sample grasps efficiently at test time (or avoid sampling
entirely).
  We evaluate our model architecture and data generation pipeline in simulation
and the real world. We find we can achieve a $>$90% success rate on previously
unseen realistic objects at test time in simulation despite having only been
trained on random objects. We also demonstrate an 80% success rate on
real-world grasp attempts despite having only been trained on random simulated
objects.
",Robust Grasp Planning via Domain Randomization,deep learning for robotic grasping
"  We consider two questions at the heart of machine learning; how can we
predict if a minimum will generalize to the test set, and why does stochastic
gradient descent find minima that generalize well? Our work responds to Zhang
et al. (2016), who showed deep neural networks can easily memorize randomly
labeled training data, despite generalizing well on real labels of the same
inputs. We show that the same phenomenon occurs in small linear models. These
observations are explained by the Bayesian evidence, which penalizes sharp
minima but is invariant to model parameterization. We also demonstrate that,
when one holds the learning rate fixed, there is an optimum batch size which
maximizes the test set accuracy. We propose that the noise introduced by small
mini-batches drives the parameters towards minima whose evidence is large.
Interpreting stochastic gradient descent as a stochastic differential equation,
we identify the ""noise scale"" $g = \epsilon (\frac{N}{B} - 1) \approx \epsilon
N/B$, where $\epsilon$ is the learning rate, $N$ the training set size and $B$
the batch size. Consequently the optimum batch size is proportional to both the
learning rate and the size of the training set, $B_{opt} \propto \epsilon N$.
We verify these predictions empirically.
",Stochastic Gradient Descent and Generalization in Machine Learning,generalization in machine learning
"  Manifold learning based methods have been widely used for non-linear
dimensionality reduction (NLDR). However, in many practical settings, the need
to process streaming data is a challenge for such methods, owing to the high
computational complexity involved. Moreover, most methods operate under the
assumption that the input data is sampled from a single manifold, embedded in a
high dimensional space. We propose a method for streaming NLDR when the
observed data is either sampled from multiple manifolds or irregularly sampled
from a single manifold. We show that existing NLDR methods, such as Isomap,
fail in such situations, primarily because they rely on smoothness and
continuity of the underlying manifold, which is violated in the scenarios
explored in this paper. However, the proposed algorithm is able to learn
effectively in presence of multiple, and potentially intersecting, manifolds,
while allowing for the input data to arrive as a massive stream.
",Streaming Non-Linear Dimensionality Reduction,streaming non-linear dimensionality reduction
"  Online Goal Babbling and Direction Sampling are recently proposed methods for
direct learning of inverse kinematics mappings from scratch even in
high-dimensional sensorimotor spaces following the paradigm of ""learning while
behaving"". To learn inverse statics mappings - primarily for gravity
compensation - from scratch and without using any closed-loop controller, we
modify and enhance the Online Goal Babbling and Direction Sampling schemes.
Moreover, we exploit symmetries in the inverse statics mappings to drastically
reduce the number of samples required for learning inverse statics models.
Results for a 2R planar robot, a 3R simplified human arm, and a 4R humanoid
robot arm clearly demonstrate that their inverse statics mappings can be
learned successfully with our modified online Goal Babbling scheme.
Furthermore, we show that the number of samples required for the 2R and 3R arms
can be reduced by a factor of at least 8 and 16 resp. -depending on the number
of discovered symmetries.
",Learning Inverse Statics Mappings in Robotics,robotics
"  We consider the problem of computing the Fourier transform of
high-dimensional vectors, distributedly over a cluster of machines consisting
of a master node and multiple worker nodes, where the worker nodes can only
store and process a fraction of the inputs. We show that by exploiting the
algebraic structure of the Fourier transform operation and leveraging concepts
from coding theory, one can efficiently deal with the straggler effects. In
particular, we propose a computation strategy, named as coded FFT, which
achieves the optimal recovery threshold, defined as the minimum number of
workers that the master node needs to wait for in order to compute the output.
This is the first code that achieves the optimum robustness in terms of
tolerating stragglers or failures for computing Fourier transforms.
Furthermore, the reconstruction process for coded FFT can be mapped to MDS
decoding, which can be solved efficiently. Moreover, we extend coded FFT to
settings including computing general $n$-dimensional Fourier transforms, and
provide the optimal computing strategy for those settings.
",Distributed Fourier Transform Computing,distributed computing for high-dimensional fourier transforms
"  Cross-validation under sample selection bias can, in principle, be done by
importance-weighting the empirical risk. However, the importance-weighted risk
estimator produces sub-optimal hyperparameter estimates in problem settings
where large weights arise with high probability. We study its sampling variance
as a function of the training data distribution and introduce a control variate
to increase its robustness to problematically large weights.
",Importance Weighting in Cross-Validation,handling sample selection bias in cross-validation
"  In this work we propose Lasagne, a methodology to learn locality and
structure aware graph node embeddings in an unsupervised way. In particular, we
show that the performance of existing random-walk based approaches depends
strongly on the structural properties of the graph, e.g., the size of the
graph, whether the graph has a flat or upward-sloping Network Community Profile
(NCP), whether the graph is expander-like, whether the classes of interest are
more k-core-like or more peripheral, etc. For larger graphs with flat NCPs that
are strongly expander-like, existing methods lead to random walks that expand
rapidly, touching many dissimilar nodes, thereby leading to lower-quality
vector representations that are less useful for downstream tasks. Rather than
relying on global random walks or neighbors within fixed hop distances, Lasagne
exploits strongly local Approximate Personalized PageRank stationary
distributions to more precisely engineer local information into node
embeddings. This leads, in particular, to more meaningful and more useful
vector representations of nodes in poorly-structured graphs. We show that
Lasagne leads to significant improvement in downstream multi-label
classification for larger graphs with flat NCPs, that it is comparable for
smaller graphs with upward-sloping NCPs, and that is comparable to existing
methods for link prediction tasks.
",Graph Node Embeddings,graph node embedding
"  Deep reinforcement learning (RL) has proven a powerful technique in many
sequential decision making domains. However, Robotics poses many challenges for
RL, most notably training on a physical system can be expensive and dangerous,
which has sparked significant interest in learning control policies using a
physics simulator. While several recent works have shown promising results in
transferring policies trained in simulation to the real world, they often do
not fully utilize the advantage of working with a simulator. In this work, we
exploit the full state observability in the simulator to train better policies
which take as input only partial observations (RGBD images). We do this by
employing an actor-critic training algorithm in which the critic is trained on
full states while the actor (or policy) gets rendered images as input. We show
experimentally on a range of simulated tasks that using these asymmetric inputs
significantly improves performance. Finally, we combine this method with domain
randomization and show real robot experiments for several tasks like picking,
pushing, and moving a block. We achieve this simulation to real world transfer
without training on any real world data.
",Simulation-Based Robotics Reinforcement Learning,deep reinforcement learning in robotics
"  An increasing number of sensors on mobile, Internet of things (IoT), and
wearable devices generate time-series measurements of physical activities.
Though access to the sensory data is critical to the success of many beneficial
applications such as health monitoring or activity recognition, a wide range of
potentially sensitive information about the individuals can also be discovered
through access to sensory data and this cannot easily be protected using
traditional privacy approaches.
  In this paper, we propose a privacy-preserving sensing framework for managing
access to time-series data in order to provide utility while protecting
individuals' privacy. We introduce Replacement AutoEncoder, a novel algorithm
which learns how to transform discriminative features of data that correspond
to sensitive inferences, into some features that have been more observed in
non-sensitive inferences, to protect users' privacy. This efficiency is
achieved by defining a user-customized objective function for deep
autoencoders. Our replacement method will not only eliminate the possibility of
recognizing sensitive inferences, it also eliminates the possibility of
detecting the occurrence of them. That is the main weakness of other approaches
such as filtering or randomization. We evaluate the efficacy of the algorithm
with an activity recognition task in a multi-sensing environment using
extensive experiments on three benchmark datasets. We show that it can retain
the recognition accuracy of state-of-the-art techniques while simultaneously
preserving the privacy of sensitive information. Finally, we utilize the GANs
for detecting the occurrence of replacement, after releasing data, and show
that this can be done only if the adversarial network is trained on the users'
original data.
",Privacy-Preserving Sensing in IoT Devices,privacy-preserving sensing framework for iot and wearable devices
"  A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.
",Random Neural Networks Analysis,random neural networks
"  Experience replay is a key technique behind many recent advances in deep
reinforcement learning. Allowing the agent to learn from earlier memories can
speed up learning and break undesirable temporal correlations. Despite its
wide-spread application, very little is understood about the properties of
experience replay. How does the amount of memory kept affect learning dynamics?
Does it help to prioritize certain experiences? In this paper, we address these
questions by formulating a dynamical systems ODE model of Q-learning with
experience replay. We derive analytic solutions of the ODE for a simple
setting. We show that even in this very simple setting, the amount of memory
kept can substantially affect the agent's performance. Too much or too little
memory both slow down learning. Moreover, we characterize regimes where
prioritized replay harms the agent's learning. We show that our analytic
solutions have excellent agreement with experiments. Finally, we propose a
simple algorithm for adaptively changing the memory buffer size which achieves
consistently good empirical performance.
",Experience Replay in Reinforcement Learning,experience replay in reinforcement learning
"  Learning social media data embedding by deep models has attracted extensive
research interest as well as boomed a lot of applications, such as link
prediction, classification, and cross-modal search. However, for social images
which contain both link information and multimodal contents (e.g., text
description, and visual content), simply employing the embedding learnt from
network structure or data content results in sub-optimal social image
representation. In this paper, we propose a novel social image embedding
approach called Deep Multimodal Attention Networks (DMAN), which employs a deep
model to jointly embed multimodal contents and link information. Specifically,
to effectively capture the correlations between multimodal contents, we propose
a multimodal attention network to encode the fine-granularity relation between
image regions and textual words. To leverage the network structure for
embedding learning, a novel Siamese-Triplet neural network is proposed to model
the links among images. With the joint deep model, the learnt embedding can
capture both the multimodal contents and the nonlinear network information.
Extensive experiments are conducted to investigate the effectiveness of our
approach in the applications of multi-label classification and cross-modal
search. Compared to state-of-the-art image embeddings, our proposed DMAN
achieves significant improvement in the tasks of multi-label classification and
cross-modal search.
",Social Media Image Embeddings,social media image embedding
"  Deep neural networks (DNNs) have become increasingly important due to their
excellent empirical performance on a wide range of problems. However,
regularization is generally achieved by indirect means, largely due to the
complex set of functions defined by a network and the difficulty in measuring
function complexity. There exists no method in the literature for additive
regularization based on a norm of the function, as is classically considered in
statistical learning theory. In this work, we propose sampling-based
approximations to weighted function norms as regularizers for deep neural
networks. We provide, to the best of our knowledge, the first proof in the
literature of the NP-hardness of computing function norms of DNNs, motivating
the necessity of an approximate approach. We then derive a generalization bound
for functions trained with weighted norms and prove that a natural stochastic
optimization strategy minimizes the bound. Finally, we empirically validate the
improved performance of the proposed regularization strategies for both convex
function sets as well as DNNs on real-world classification and image
segmentation tasks demonstrating improved performance over weight decay,
dropout, and batch normalization. Source code will be released at the time of
publication.
",Regularization of Deep Neural Networks,deep neural network regularization
"  Dictionaries are collections of vectors used for representations of elements
in Euclidean spaces. While recent research on optimal dictionaries is focussed
on providing sparse (i.e., $\ell_0$-optimal,) representations, here we consider
the problem of finding optimal dictionaries such that representations of
samples of a random vector are optimal in an $\ell_2$-sense. For us, optimality
of representation is equivalent to minimization of the average $\ell_2$-norm of
the coefficients used to represent the random vector, with the lengths of the
dictionary vectors being specified a priori. With the help of recent results on
rank-$1$ decompositions of symmetric positive semidefinite matrices and the
theory of majorization, we provide a complete characterization of
$\ell_2$-optimal dictionaries. Our results are accompanied by polynomial time
algorithms that construct $\ell_2$-optimal dictionaries from given data.
",Optimal Dictionary Learning for Vector Representations,optimal dictionary learning for euclidean spaces
"  In this paper, we study the pooled data problem of identifying the labels
associated with a large collection of items, based on a sequence of pooled
tests revealing the counts of each label within the pool. In the noiseless
setting, we identify an exact asymptotic threshold on the required number of
tests with optimal decoding, and prove a phase transition between complete
success and complete failure. In addition, we present a novel noisy variation
of the problem, and provide an information-theoretic framework for
characterizing the required number of tests for general random noise models.
Our results reveal that noise can make the problem considerably more difficult,
with strict increases in the scaling laws even at low noise levels. Finally, we
demonstrate similar behavior in an approximate recovery setting, where a given
number of errors is allowed in the decoded labels.
",Pooled Data Decoding,pooled data labeling
"  Should we input known genome sequence features or input sequence itself in
deep learning framework? As deep learning more popular in various applications,
researchers often come to question whether to generate features or use raw
sequences for deep learning. To answer this question, we study the prediction
accuracy of precursor miRNA prediction of feature-based deep belief network and
sequence-based convolution neural network. Tested on a variant of six-layer
convolution neural net and three-layer deep belief network, we find the raw
sequence input based convolution neural network model performs similar or
slightly better than feature based deep belief networks with best accuracy
values of 0.995 and 0.990, respectively. Both the models outperform existing
benchmarks models. The results shows us that if provided large enough data,
well devised raw sequence based deep learning models can replace feature based
deep learning models. However, construction of well behaved deep learning model
can be very challenging. In cased features can be easily extracted,
feature-based deep learning models may be a better alternative.
",Raw Sequence vs Feature-Based Deep Learning Models,comparison of feature-based and sequence-based deep learning models for precursor mirna prediction
"  In this work, we pose the question of whether, by considering qualitative
information such as a sample target image as input, one can produce a rendered
image of scientific data that is similar to the target. The algorithm resulting
from our research allows one to ask the question of whether features like those
in the target image exists in a given dataset. In that way, our method is one
of imagery query or reverse engineering, as opposed to manual parameter
tweaking of the full visualization pipeline. For target images, we can use
real-world photographs of physical phenomena. Our method leverages deep neural
networks and evolutionary optimization. Using a trained similarity function
that measures the difference between renderings of a phenomenon and real-world
photographs, our method optimizes rendering parameters. We demonstrate the
efficacy of our method using a superstorm simulation dataset and images found
online. We also discuss a parallel implementation of our method, which was run
on NCSA's Blue Waters.
",Image-based Scientific Data Visualization,image generation and rendering
"  The principle goal of computational mechanics is to define pattern and
structure so that the organization of complex systems can be detected and
quantified. Computational mechanics developed from efforts in the 1970s and
early 1980s to identify strange attractors as the mechanism driving weak fluid
turbulence via the method of reconstructing attractor geometry from measurement
time series and in the mid-1980s to estimate equations of motion directly from
complex time series. In providing a mathematical and operational definition of
structure it addressed weaknesses of these early approaches to discovering
patterns in natural systems.
  Since then, computational mechanics has led to a range of results from
theoretical physics and nonlinear mathematics to diverse applications---from
closed-form analysis of Markov and non-Markov stochastic processes that are
ergodic or nonergodic and their measures of information and intrinsic
computation to complex materials and deterministic chaos and intelligence in
Maxwellian demons to quantum compression of classical processes and the
evolution of computation and language.
  This brief review clarifies several misunderstandings and addresses concerns
recently raised regarding early works in the field (1980s). We show that
misguided evaluations of the contributions of computational mechanics are
groundless and stem from a lack of familiarity with its basic goals and from a
failure to consider its historical context. For all practical purposes, its
modern methods and results largely supersede the early works. This not only
renders recent criticism moot and shows the solid ground on which computational
mechanics stands but, most importantly, shows the significant progress achieved
over three decades and points to the many intriguing and outstanding challenges
in understanding the computational nature of complex dynamic systems.
",Computational Mechanics History,computational mechanics
"  The problem of Shannon entropy estimation in countable infinite alphabets is
addressed from the study and use of convergence results of the entropy
functional, which is known to be discontinuous with respect to the total
variation distance in $\infty$-alphabets. Sufficient conditions for the
convergence of the entropy are used, including scenarios with both finitely and
infinitely supported assumptions on the distributions. From this new
perspective, four plug-in histogram-based estimators are studied showing that
convergence results are instrumental to derive new strong consistency and rate
of convergences results. Different scenarios and conditions are used on both
the estimators and the underlying distribution, considering for example finite
and unknown supported assumptions and summable tail bounded conditions.
",Shannon Entropy Estimation in Infinite Alphabets,shannon entropy estimation in countable infinite alphabets
"  This article proposes a Bayesian nonparametric method for forecasting,
imputation, and clustering in sparsely observed, multivariate time series data.
The method is appropriate for jointly modeling hundreds of time series with
widely varying, non-stationary dynamics. Given a collection of $N$ time series,
the Bayesian model first partitions them into independent clusters using a
Chinese restaurant process prior. Within a cluster, all time series are modeled
jointly using a novel ""temporally-reweighted"" extension of the Chinese
restaurant process mixture. Markov chain Monte Carlo techniques are used to
obtain samples from the posterior distribution, which are then used to form
predictive inferences. We apply the technique to challenging forecasting and
imputation tasks using seasonal flu data from the US Center for Disease Control
and Prevention, demonstrating superior forecasting accuracy and competitive
imputation accuracy as compared to multiple widely used baselines. We further
show that the model discovers interpretable clusters in datasets with hundreds
of time series, using macroeconomic data from the Gapminder Foundation.
",Time Series Analysis,bayesian methods for time series analysis
"  The past decade has witnessed a successful application of deep learning to
solving many challenging problems in machine learning and artificial
intelligence. However, the loss functions of deep neural networks (especially
nonlinear networks) are still far from being well understood from a theoretical
aspect. In this paper, we enrich the current understanding of the landscape of
the square loss functions for three types of neural networks. Specifically,
when the parameter matrices are square, we provide an explicit characterization
of the global minimizers for linear networks, linear residual networks, and
nonlinear networks with one hidden layer. Then, we establish two quadratic
types of landscape properties for the square loss of these neural networks,
i.e., the gradient dominance condition within the neighborhood of their full
rank global minimizers, and the regularity condition along certain directions
and within the neighborhood of their global minimizers. These two landscape
properties are desirable for the optimization around the global minimizers of
the loss function for these neural networks.
",Deep Learning Loss Function Analysis,deep neural network optimization
"  Data-driven predictive analytics are in use today across a number of
industrial applications, but further integration is hindered by the requirement
of similarity among model training and test data distributions. This paper
addresses the need of learning from possibly nonstationary data streams, or
under concept drift, a commonly seen phenomenon in practical applications. A
simple dual-learner ensemble strategy, alternating learners framework, is
proposed. A long-memory model learns stable concepts from a long relevant time
window, while a short-memory model learns transient concepts from a small
recent window. The difference in prediction performance of these two models is
monitored and induces an alternating policy to select, update and reset the two
models. The method features an online updating mechanism to maintain the
ensemble accuracy, and a concept-dependent trigger to focus on relevant data.
Through empirical studies the method demonstrates effective tracking and
prediction when the steaming data carry abrupt and/or gradual changes.
",Concept Drift Handling in Predictive Analytics,handling concept drift in data streams
"  Humans exhibit a wide range of adaptive and robust dynamic motion behavior
that is yet unmatched by autonomous control systems. These capabilities are
essential for real-time behavior generation in cluttered environments. Recent
work suggests that human capabilities rely on task structure learning and
embedded or ecological cognition in the form of perceptual guidance. This paper
describes the experimental investigation of the functional elements of human
motion guidance, focusing on the control and perceptual mechanisms. The motion,
control, and perceptual data from first-person guidance experiments is
decomposed into elemental segments based on invariants. These elements are then
analyzed to determine their functional characteristics. The resulting model
explains the structure of the agent-environment interaction and provides lawful
descriptions of specific perceptual guidance and control mechanisms.
",Human Motion Guidance,human motion guidance in cluttered environments
"  Most commonly used distributed machine learning systems are either
synchronous or centralized asynchronous. Synchronous algorithms like
AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous
algorithms using a parameter server suffer from 1) communication bottleneck at
parameter servers when workers are many, and 2) significantly worse convergence
when the traffic to parameter server is congested. Can we design an algorithm
that is robust in a heterogeneous environment, while being communication
efficient and maintaining the best-possible convergence rate? In this paper, we
propose an asynchronous decentralized stochastic gradient decent algorithm
(AD-PSGD) satisfying all above expectations. Our theoretical analysis shows
AD-PSGD converges at the optimal $O(1/\sqrt{K})$ rate as SGD and has linear
speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of
decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and
standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a
heterogeneous environment. When training ResNet-50 on ImageNet with up to 128
GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each
epoch can be up to 4-8X faster than its synchronous counterparts in a
network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the
first asynchronous algorithm that achieves a similar epoch-wise convergence
rate as AllReduce-SGD, at an over 100-GPU scale.
",Distributed Machine Learning Algorithms for Heterogeneous Environments,distributed machine learning
"  We demonstrate that it is possible to train large recurrent language models
with user-level differential privacy guarantees with only a negligible cost in
predictive accuracy. Our work builds on recent advances in the training of deep
networks on user-partitioned data and privacy accounting for stochastic
gradient descent. In particular, we add user-level privacy protection to the
federated averaging algorithm, which makes ""large step"" updates from user-level
data. Our work demonstrates that given a dataset with a sufficiently large
number of users (a requirement easily met by even small internet-scale
datasets), achieving differential privacy comes at the cost of increased
computation, rather than in decreased utility as in most prior work. We find
that our private LSTM language models are quantitatively and qualitatively
similar to un-noised models when trained on a large dataset.
",Differential Privacy in Language Models,federated learning with differential privacy
"  With the development of next generation sequencing techniques, it is fast and
cheap to determine protein sequences but relatively slow and expensive to
extract useful information from protein sequences because of limitations of
traditional biological experimental techniques. Protein function prediction has
been a long standing challenge to fill the gap between the huge amount of
protein sequences and the known function. In this paper, we propose a novel
method to convert the protein function problem into a language translation
problem by the new proposed protein sequence language ""ProLan"" to the protein
function language ""GOLan"", and build a neural machine translation model based
on recurrent neural networks to translate ""ProLan"" language to ""GOLan""
language. We blindly tested our method by attending the latest third Critical
Assessment of Function Annotation (CAFA 3) in 2016, and also evaluate the
performance of our methods on selected proteins whose function was released
after CAFA competition. The good performance on the training and testing
datasets demonstrates that our new proposed method is a promising direction for
protein function prediction. In summary, we first time propose a method which
converts the protein function prediction problem to a language translation
problem and applies a neural machine translation model for protein function
prediction.
",Protein Function Prediction using Neural Machine Translation,protein function prediction
"  Deep learning typically requires training a very capable architecture using
large datasets. However, many important learning problems demand an ability to
draw valid inferences from small size datasets, and such problems pose a
particular challenge for deep learning. In this regard, various researches on
""meta-learning"" are being actively conducted. Recent work has suggested a
Memory Augmented Neural Network (MANN) for meta-learning. MANN is an
implementation of a Neural Turing Machine (NTM) with the ability to rapidly
assimilate new data in its memory, and use this data to make accurate
predictions. In models such as MANN, the input data samples and their
appropriate labels from previous step are bound together in the same memory
locations. This often leads to memory interference when performing a task as
these models have to retrieve a feature of an input from a certain memory
location and read only the label information bound to that location. In this
paper, we tried to address this issue by presenting a more robust MANN. We
revisited the idea of meta-learning and proposed a new memory augmented neural
network by explicitly splitting the external memory into feature and label
memories. The feature memory is used to store the features of input data
samples and the label memory stores their labels. Hence, when predicting the
label of a given input, our model uses its feature memory unit as a reference
to extract the stored feature of the input, and based on that feature, it
retrieves the label information of the input from the label memory unit. In
order for the network to function in this framework, a new memory-writingmodule
to encode label information into the label memory in accordance with the
meta-learning task structure is designed. Here, we demonstrate that our model
outperforms MANN by a large margin in supervised one-shot classification tasks
using Omniglot and MNIST datasets.
",Meta-Learning with Memory Augmented Neural Networks,meta-learning with memory augmented neural networks
"  Can we learn a binary classifier from only positive data, without any
negative data or unlabeled data? We show that if one can equip positive data
with confidence (positive-confidence), one can successfully learn a binary
classifier, which we name positive-confidence (Pconf) classification. Our work
is related to one-class classification which is aimed at ""describing"" the
positive class by clustering-related methods, but one-class classification does
not have the ability to tune hyper-parameters and their aim is not on
""discriminating"" positive and negative classes. For the Pconf classification
problem, we provide a simple empirical risk minimization framework that is
model-independent and optimization-independent. We theoretically establish the
consistency and an estimation error bound, and demonstrate the usefulness of
the proposed method for training deep neural networks through experiments.
",Positive-Confidence Classification,positive-confidence classification
"  A study of the classification problem in context of information theory is
presented in the paper. Current research in that field is focused on
optimisation and bayesian approach. Although that gives satisfying results,
they require a vast amount of data and computations to train on. Authors
propose a new concept named Informational Neurobayesian Approach (INA), which
allows to solve the same problems, but requires significantly less training
data as well as computational power. Experiments were conducted to compare its
performance with the traditional one and the results showed that capacity of
the INA is quite promising.
",Informational Neurobayesian Approach,information theory and machine learning
"  We identify a strong equivalence between neural network based machine
learning (ML) methods and the formulation of statistical data assimilation
(DA), known to be a problem in statistical physics. DA, as used widely in
physical and biological sciences, systematically transfers information in
observations to a model of the processes producing the observations. The
correspondence is that layer label in the ML setting is the analog of time in
the data assimilation setting. Utilizing aspects of this equivalence we discuss
how to establish the global minimum of the cost functions in the ML context,
using a variational annealing method from DA. This provides a design method for
optimal networks for ML applications and may serve as the basis for
understanding the success of ""deep learning"". Results from an ML example are
presented.
  When the layer label is taken to be continuous, the Euler-Lagrange equation
for the ML optimization problem is an ordinary differential equation, and we
see that the problem being solved is a two point boundary value problem. The
use of continuous layers is denoted ""deepest learning"". The Hamiltonian version
provides a direct rationale for back propagation as a solution method for the
canonical momentum; however, it suggests other solution methods are to be
preferred.
",Equivalence of Neural Networks and Data Assimilation,neural networks and data assimilation
"  Bayesian neural networks with latent variables are scalable and flexible
probabilistic models: They account for uncertainty in the estimation of the
network weights and, by making use of latent variables, can capture complex
noise patterns in the data. We show how to extract and decompose uncertainty
into epistemic and aleatoric components for decision-making purposes. This
allows us to successfully identify informative points for active learning of
functions with heteroscedastic and bimodal noise. Using the decomposition we
further define a novel risk-sensitive criterion for reinforcement learning to
identify policies that balance expected cost, model-bias and noise aversion.
",Uncertainty Quantification in Bayesian Neural Networks,bayesian neural networks
"  Power plant is a complex and nonstationary system for which the traditional
machine learning modeling approaches fall short of expectations. The
ensemble-based online learning methods provide an effective way to continuously
learn from the dynamic environment and autonomously update models to respond to
environmental changes. This paper proposes such an online ensemble regression
approach to model power plant performance, which is critically important for
operation optimization. The experimental results on both simulated and real
data show that the proposed method can achieve performance with less than 1%
mean average percentage error, which meets the general expectations in field
operations.
",Power Plant Performance Modeling,power plant performance modeling
"  Heart rate variability (HRV) is a vital measure of the autonomic nervous
system functionality and a key indicator of cardiovascular condition. This
paper proposes a novel method, called pattern tree which is an extension of
Willem's context tree to real-valued data, to investigate HRV via an
atypicality framework. In a previous paper atypicality was developed as method
for mining and discovery in ""Big Data,"" which requires a universal approach.
Using the proposed pattern tree as a universal source coder in this framework
led to discovery of arrhythmias and unknown patterns in HRV Holter Monitoring.
",HRV Pattern Analysis,cardiovascular health
"  Ensembles of classifier models typically deliver superior performance and can
outperform single classifier models given a dataset and classification task at
hand. However, the gain in performance comes together with the lack in
comprehensibility, posing a challenge to understand how each model affects the
classification outputs and where the errors come from. We propose a tight
visual integration of the data and the model space for exploring and combining
classifier models. We introduce a workflow that builds upon the visual
integration and enables the effective exploration of classification outputs and
models. We then present a use case in which we start with an ensemble
automatically selected by a standard ensemble selection algorithm, and show how
we can manipulate models and alternative combinations.
",Visualizing Ensemble Classifier Models,model combination and visualization for ensemble learning
"  We propose a method (TT-GP) for approximate inference in Gaussian Process
(GP) models. We build on previous scalable GP research including stochastic
variational inference based on inducing inputs, kernel interpolation, and
structure exploiting algebra. The key idea of our method is to use Tensor Train
decomposition for variational parameters, which allows us to train GPs with
billions of inducing inputs and achieve state-of-the-art results on several
benchmarks. Further, our approach allows for training kernels based on deep
neural networks without any modifications to the underlying GP model. A neural
network learns a multidimensional embedding for the data, which is used by the
GP to make the final prediction. We train GP and neural network parameters
end-to-end without pretraining, through maximization of GP marginal likelihood.
We show the efficiency of the proposed approach on several regression and
classification benchmark datasets including MNIST, CIFAR-10, and Airline.
",Gaussian Process Inference,machine learning
"  Algorithmic game theory (AGT) focuses on the design and analysis of
algorithms for interacting agents, with interactions rigorously formalized
within the framework of games. Results from AGT find applications in domains
such as online bidding auctions for web advertisements and network routing
protocols. Monotone games are games where agent strategies naturally converge
to an equilibrium state. Previous results in AGT have been obtained for convex,
socially-convex, or smooth games, but not monotone games. Our primary
theoretical contributions are defining the monotone game setting and its
extension to the online setting, a new notion of regret for this setting, and
accompanying algorithms that achieve sub-linear regret. We demonstrate the
utility of online monotone game theory on a variety of problem domains
including variational inequalities, reinforcement learning, and generative
adversarial networks.
",Monotone Game Theory,algorithmic game theory
"  Docking is an important tool in computational drug discovery that aims to
predict the binding pose of a ligand to a target protein through a combination
of pose scoring and optimization. A scoring function that is differentiable
with respect to atom positions can be used for both scoring and gradient-based
optimization of poses for docking. Using a differentiable grid-based atomic
representation as input, we demonstrate that a scoring function learned by
training a convolutional neural network (CNN) to identify binding poses can
also be applied to pose optimization. We also show that an iteratively-trained
CNN that includes poses optimized by the first CNN in its training set performs
even better at optimizing randomly initialized poses than either the first CNN
scoring function or AutoDock Vina.
",Deep Learning in Docking-Based Drug Discovery,machine learning-based molecular docking
"  We establish that first-order methods avoid saddle points for almost all
initializations. Our results apply to a wide variety of first-order methods,
including gradient descent, block coordinate descent, mirror descent and
variants thereof. The connecting thread is that such algorithms can be studied
from a dynamical systems perspective in which appropriate instantiations of the
Stable Manifold Theorem allow for a global stability analysis. Thus, neither
access to second-order derivative information nor randomness beyond
initialization is necessary to provably avoid saddle points.
",Convergence of First-Order Optimization Methods,avoidance of saddle points in first-order optimization methods
"  We propose a novel framework for the differentially private ERM, input
perturbation. Existing differentially private ERM implicitly assumed that the
data contributors submit their private data to a database expecting that the
database invokes a differentially private mechanism for publication of the
learned model. In input perturbation, each data contributor independently
randomizes her/his data by itself and submits the perturbed data to the
database. We show that the input perturbation framework theoretically
guarantees that the model learned with the randomized data eventually satisfies
differential privacy with the prescribed privacy parameters. At the same time,
input perturbation guarantees that local differential privacy is guaranteed to
the server. We also show that the excess risk bound of the model learned with
input perturbation is $O(1/n)$ under a certain condition, where $n$ is the
sample size. This is the same as the excess risk bound of the state-of-the-art.
",Differentially Private Input Perturbation for ERM,differentially private learning
"  We propose a novel pooling strategy that learns how to adaptively rank deep
convolutional features for selecting more informative representations. To this
end, we exploit discriminative analysis to project the features onto a space
spanned by the number of classes in the dataset under study. This maps the
notion of labels in the feature space into instances in the projected space. We
employ these projected distances as a measure to rank the existing features
with respect to their specific discriminant power for each individual class. We
then apply multipartite ranking to score the separability of the instances and
aggregate one-versus-all scores to compute an overall distinction score for
each feature. For the pooling, we pick features with the highest scores in a
pooling window instead of maximum, average or stochastic random assignments.
Our experiments on various benchmarks confirm that the proposed strategy of
multipartite pooling is highly beneficial to consistently improve the
performance of deep convolutional networks via better generalization of the
trained models for the test-time data.
",Adaptive Feature Ranking,deep learning
"  Transfer learning is a popular practice in deep neural networks, but
fine-tuning of large number of parameters is a hard task due to the complex
wiring of neurons between splitting layers and imbalance distributions of data
in pretrained and transferred domains. The reconstruction of the original
wiring for the target domain is a heavy burden due to the size of
interconnections across neurons. We propose a distributed scheme that tunes the
convolutional filters individually while backpropagates them jointly by means
of basic probability assignment. Some of the most recent advances in evidence
theory show that in a vast variety of the imbalanced regimes, optimizing of
some proper objective functions derived from contingency matrices prevents
biases towards high-prior class distributions. Therefore, the original filters
get gradually transferred based on individual contributions to overall
performance of the target domain. This largely reduces the expected complexity
of transfer learning whilst highly improves precision. Our experiments on
standard benchmarks and scenarios confirm the consistent improvement of our
distributed deep transfer learning strategy.
",Distributed Deep Transfer Learning,distributed deep transfer learning
"  A common practice in most of deep convolutional neural architectures is to
employ fully-connected layers followed by Softmax activation to minimize
cross-entropy loss for the sake of classification. Recent studies show that
substitution or addition of the Softmax objective to the cost functions of
support vector machines or linear discriminant analysis is highly beneficial to
improve the classification performance in hybrid neural networks. We propose a
novel paradigm to link the optimization of several hybrid objectives through
unified backpropagation. This highly alleviates the burden of extensive
boosting for independent objective functions or complex formulation of
multiobjective gradients. Hybrid loss functions are linked by basic probability
assignment from evidence theory. We conduct our experiments for a variety of
scenarios and standard datasets to evaluate the advantage of our proposed
unification approach to deliver consistent improvements into the classification
performance of deep convolutional neural networks.
",Hybrid Objective Functions for Deep Neural Networks,hybrid neural network optimization
"  Introducing inequality constraints in Gaussian process (GP) models can lead
to more realistic uncertainties in learning a great variety of real-world
problems. We consider the finite-dimensional Gaussian approach from Maatouk and
Bay (2017) which can satisfy inequality conditions everywhere (either
boundedness, monotonicity or convexity). Our contributions are threefold.
First, we extend their approach in order to deal with general sets of linear
inequalities. Second, we explore several Markov Chain Monte Carlo (MCMC)
techniques to approximate the posterior distribution. Third, we investigate
theoretical and numerical properties of the constrained likelihood for
covariance parameter estimation. According to experiments on both artificial
and real data, our full framework together with a Hamiltonian Monte Carlo-based
sampler provides efficient results on both data fitting and uncertainty
quantification.
",Gaussian Process Modeling with Inequality Constraints,constrained gaussian processes for uncertainty quantification
"  The Wasserstein distance received a lot of attention recently in the
community of machine learning, especially for its principled way of comparing
distributions. It has found numerous applications in several hard problems,
such as domain adaptation, dimensionality reduction or generative models.
However, its use is still limited by a heavy computational cost. Our goal is to
alleviate this problem by providing an approximation mechanism that allows to
break its inherent complexity. It relies on the search of an embedding where
the Euclidean distance mimics the Wasserstein distance. We show that such an
embedding can be found with a siamese architecture associated with a decoder
network that allows to move from the embedding space back to the original input
space. Once this embedding has been found, computing optimization problems in
the Wasserstein space (e.g. barycenters, principal directions or even
archetypes) can be conducted extremely fast. Numerical experiments supporting
this idea are conducted on image datasets, and show the wide potential benefits
of our method.
",Fast Wasserstein Distance Approximation,approximating the wasserstein distance for machine learning
"  Our goal is to improve variance reducing stochastic methods through better
control variates. We first propose a modification of SVRG which uses the
Hessian to track gradients over time, rather than to recondition, increasing
the correlation of the control variates and leading to faster theoretical
convergence close to the optimum. We then propose accurate and computationally
efficient approximations to the Hessian, both using a diagonal and a low-rank
matrix. Finally, we demonstrate the effectiveness of our method on a wide range
of problems.
",Variance Reduction in Stochastic Optimization,stochastic optimization
"  Camera sensors can only capture a limited range of luminance simultaneously,
and in order to create high dynamic range (HDR) images a set of different
exposures are typically combined. In this paper we address the problem of
predicting information that have been lost in saturated image areas, in order
to enable HDR reconstruction from a single exposure. We show that this problem
is well-suited for deep learning algorithms, and propose a deep convolutional
neural network (CNN) that is specifically designed taking into account the
challenges in predicting HDR values. To train the CNN we gather a large dataset
of HDR images, which we augment by simulating sensor saturation for a range of
cameras. To further boost robustness, we pre-train the CNN on a simulated HDR
dataset created from a subset of the MIT Places database. We demonstrate that
our approach can reconstruct high-resolution visually convincing HDR results in
a wide range of situations, and that it generalizes well to reconstruction of
images captured with arbitrary and low-end cameras that use unknown camera
response functions and post-processing. Furthermore, we compare to existing
methods for HDR expansion, and show high quality results also for image based
lighting. Finally, we evaluate the results in a subjective experiment performed
on an HDR display. This shows that the reconstructed HDR images are visually
convincing, with large improvements as compared to existing methods.
",Single-Shot HDR Reconstruction,high dynamic range (hdr) image reconstruction
"  In this paper, we deal with the task of building a dynamic ensemble of chain
classifiers for multi-label classification. To do so, we proposed two concepts
of classifier chains algorithms that are able to change label order of the
chain without rebuilding the entire model. Such modes allows anticipating the
instance-specific chain order without a significant increase in computational
burden. The proposed chain models are built using the Naive Bayes classifier
and nearest neighbour approach as a base single-label classifiers. To take the
benefits of the proposed algorithms, we developed a simple heuristic that
allows the system to find relatively good label order. The heuristic sort
labels according to the label-specific classification quality gained during the
validation phase. The heuristic tries to minimise the phenomenon of error
propagation in the chain. The experimental results showed that the proposed
model based on Naive Bayes classifier the above-mentioned heuristic is an
efficient tool for building dynamic chain classifiers.
",Dynamic Ensemble of Chain Classifiers,dynamic ensemble of chain classifiers for multi-label classification
"  Recent advances in model compression have provided procedures for compressing
large neural networks to a fraction of their original size while retaining most
if not all of their accuracy. However, all of these approaches rely on access
to the original training set, which might not always be possible if the network
to be compressed was trained on a very large dataset, or on a dataset whose
release poses privacy or safety concerns as may be the case for biometrics
tasks. We present a method for data-free knowledge distillation, which is able
to compress deep neural networks trained on large-scale datasets to a fraction
of their size leveraging only some extra metadata to be provided with a
pretrained model release. We also explore different kinds of metadata that can
be used with our method, and discuss tradeoffs involved in using each of them.
",Data-Free Knowledge Distillation,data-free knowledge distillation for neural network compression
"  Clinical measurements collected over time are naturally represented as
multivariate time series (MTS), which often contain missing data. An
autoencoder can learn low dimensional vectorial representations of MTS that
preserve important data characteristics, but cannot deal explicitly with
missing data. In this work, we propose a new framework that combines an
autoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts
for missingness patterns in MTS. Via kernel alignment, we incorporate TCK in
the autoencoder to improve the learned representations in presence of missing
data. We consider a classification problem of MTS with missing values,
representing blood samples of patients with surgical site infection. With our
approach, rather than with a standard autoencoder, we learn representations in
low dimensions that can be classified better.
",Handling Missing Data in Multivariate Time Series Classification,time series data imputation and classification
"  We present Deep Voice 3, a fully-convolutional attention-based neural
text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural
speech synthesis systems in naturalness while training ten times faster. We
scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more
than eight hundred hours of audio from over two thousand speakers. In addition,
we identify common error modes of attention-based speech synthesis networks,
demonstrate how to mitigate them, and compare several different waveform
synthesis methods. We also describe how to scale inference to ten million
queries per day on one single-GPU server.
",Text-to-Speech Synthesis Models,text-to-speech synthesis
"  A popular approach to semi-supervised learning proceeds by endowing the input
data with a graph structure in order to extract geometric information and
incorporate it into a Bayesian framework. We introduce new theory that gives
appropriate scalings of graph parameters that provably lead to a well-defined
limiting posterior as the size of the unlabeled data set grows. Furthermore, we
show that these consistency results have profound algorithmic implications.
When consistency holds, carefully designed graph-based Markov chain Monte Carlo
algorithms are proved to have a uniform spectral gap, independent of the number
of unlabeled inputs. Several numerical experiments corroborate both the
statistical consistency and the algorithmic scalability established by the
theory.
",Graph-based Semi-supervised Learning,graph-based semi-supervised learning
"  Similar to convolution neural networks, recurrent neural networks (RNNs)
typically suffer from over-parameterization. Quantizing bit-widths of weights
and activations results in runtime efficiency on hardware, yet it often comes
at the cost of reduced accuracy. This paper proposes a quantization approach
that increases model size with bit-width reduction. This approach will allow
networks to perform at their baseline accuracy while still maintaining the
benefits of reduced precision and overall model size reduction.
",Quantization of Neural Networks,quantization techniques for recurrent neural networks
"  We present a novel notion of complexity that interpolates between and
generalizes some classic existing complexity notions in learning theory: for
estimators like empirical risk minimization (ERM) with arbitrary bounded
losses, it is upper bounded in terms of data-independent Rademacher complexity;
for generalized Bayesian estimators, it is upper bounded by the data-dependent
information complexity (also known as stochastic or PAC-Bayesian,
$\mathrm{KL}(\text{posterior} \operatorname{\|} \text{prior})$ complexity. For
(penalized) ERM, the new complexity reduces to (generalized) normalized maximum
likelihood (NML) complexity, i.e. a minimax log-loss individual-sequence
regret. Our first main result bounds excess risk in terms of the new
complexity. Our second main result links the new complexity via Rademacher
complexity to $L_2(P)$ entropy, thereby generalizing earlier results of Opper,
Haussler, Lugosi, and Cesa-Bianchi who did the log-loss case with $L_\infty$.
Together, these results recover optimal bounds for VC- and large (polynomial
entropy) classes, replacing localized Rademacher complexity by a simpler
analysis which almost completely separates the two aspects that determine the
achievable rates: 'easiness' (Bernstein) conditions and model complexity.
",Complexity Measures in Learning Theory,machine learning complexity theory
"  Recent breakthroughs in computer vision make use of large deep neural
networks, utilizing the substantial speedup offered by GPUs. For applications
running on limited hardware, however, high precision real-time processing can
still be a challenge. One approach to solving this problem is training networks
with binary or ternary weights, thus removing the need to calculate
multiplications and significantly reducing memory size. In this work, we
introduce LR-nets (Local reparameterization networks), a new method for
training neural networks with discrete weights using stochastic parameters. We
show how a simple modification to the local reparameterization trick,
previously used to train Gaussian distributed weights, enables the training of
discrete weights. Using the proposed training we test both binary and ternary
models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art
results on most experiments.
",Efficient Neural Network Training,computer vision and neural networks
"  In this paper, we make an important step towards the black-box machine
teaching by considering the cross-space machine teaching, where the teacher and
the learner use different feature representations and the teacher can not fully
observe the learner's model. In such scenario, we study how the teacher is
still able to teach the learner to achieve faster convergence rate than the
traditional passive learning. We propose an active teacher model that can
actively query the learner (i.e., make the learner take exams) for estimating
the learner's status and provably guide the learner to achieve faster
convergence. The sample complexities for both teaching and query are provided.
In the experiments, we compare the proposed active teacher with the omniscient
teacher and verify the effectiveness of the active teacher model.
",Active Machine Teaching,machine learning
"  In this paper, we propose an implicit gradient descent algorithm for the
classic $k$-means problem. The implicit gradient step or backward Euler is
solved via stochastic fixed-point iteration, in which we randomly sample a
mini-batch gradient in every iteration. It is the average of the fixed-point
trajectory that is carried over to the next gradient step. We draw connections
between the proposed stochastic backward Euler and the recent entropy
stochastic gradient descent (Entropy-SGD) for improving the training of deep
neural networks. Numerical experiments on various synthetic and real datasets
show that the proposed algorithm provides better clustering results compared to
$k$-means algorithms in the sense that it decreased the objective function (the
cluster) and is much more robust to initialization.
",Implicit Gradient Descent for K-Means Clustering,implicit gradient descent for k-means clustering
"  SGD (Stochastic Gradient Descent) is a popular algorithm for large scale
optimization problems due to its low iterative cost. However, SGD can not
achieve linear convergence rate as FGD (Full Gradient Descent) because of the
inherent gradient variance. To attack the problem, mini-batch SGD was proposed
to get a trade-off in terms of convergence rate and iteration cost. In this
paper, a general CVI (Convergence-Variance Inequality) equation is presented to
state formally the interaction of convergence rate and gradient variance. Then
a novel algorithm named SSAG (Stochastic Stratified Average Gradient) is
introduced to reduce gradient variance based on two techniques, stratified
sampling and averaging over iterations that is a key idea in SAG (Stochastic
Average Gradient). Furthermore, SSAG can achieve linear convergence rate of
$\mathcal {O}((1-\frac{\mu}{8CL})^k)$ at smaller storage and iterative costs,
where $C\geq 2$ is the category number of training data. This convergence rate
depends mainly on the variance between classes, but not on the variance within
the classes. In the case of $C\ll N$ ($N$ is the training data size), SSAG's
convergence rate is much better than SAG's convergence rate of $\mathcal
{O}((1-\frac{\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG
and many other algorithms.
",Stochastic Gradient Descent Variance Reduction,machine learning
"  In the setting of nonparametric regression, we propose and study a
combination of stochastic gradient methods with Nystr\""om subsampling, allowing
multiple passes over the data and mini-batches. Generalization error bounds for
the studied algorithm are provided. Particularly, optimal learning rates are
derived considering different possible choices of the step-size, the mini-batch
size, the number of iterations/passes, and the subsampling level. In comparison
with state-of-the-art algorithms such as the classic stochastic gradient
methods and kernel ridge regression with Nystr\""om, the studied algorithm has
advantages on the computational complexity, while achieving the same optimal
learning rates. Moreover, our results indicate that using mini-batches can
reduce the total computational cost while achieving the same optimal
statistical results.
",Stochastic Gradient Methods in Nonparametric Regression,machine learning
"  In this paper, we design and analyze a new zeroth-order online algorithm,
namely, the zeroth-order online alternating direction method of multipliers
(ZOO-ADMM), which enjoys dual advantages of being gradient-free operation and
employing the ADMM to accommodate complex structured regularizers. Compared to
the first-order gradient-based online algorithm, we show that ZOO-ADMM requires
$\sqrt{m}$ times more iterations, leading to a convergence rate of
$O(\sqrt{m}/\sqrt{T})$, where $m$ is the number of optimization variables, and
$T$ is the number of iterations. To accelerate ZOO-ADMM, we propose two
minibatch strategies: gradient sample averaging and observation averaging,
resulting in an improved convergence rate of $O(\sqrt{1+q^{-1}m}/\sqrt{T})$,
where $q$ is the minibatch size. In addition to convergence analysis, we also
demonstrate ZOO-ADMM to applications in signal processing, statistics, and
machine learning.
",Zeroth-Order Optimization Algorithms,zeroth-order online optimization algorithms
"  Identifying a potentially large number of simultaneous line outages in power
transmission networks in real time is a computationally hard problem. This is
because the number of hypotheses grows exponentially with the network size. A
new ""Learning-to-Infer"" method is developed for efficient inference of every
line status in the network. Optimizing the line outage detector is transformed
to and solved as a discriminative learning problem based on Monte Carlo samples
generated with power flow simulations. A major advantage of the developed
Learning-to-Infer method is that the labeled data used for training can be
generated in an arbitrarily large amount rapidly and at very little cost. As a
result, the power of offline training is fully exploited to learn very complex
classifiers for effective real-time multi-line outage identification. The
proposed methods are evaluated in the IEEE 30, 118 and 300 bus systems.
Excellent performance in identifying multi-line outages in real time is
achieved with a reasonably small amount of data.
",Real-Time Multi-Line Outage Identification in Power Transmission Networks,power grid monitoring
"  We propose the use of incomplete dot products (IDP) to dynamically adjust the
number of input channels used in each layer of a convolutional neural network
during feedforward inference. IDP adds monotonically non-increasing
coefficients, referred to as a ""profile"", to the channels during training. The
profile orders the contribution of each channel in non-increasing order. At
inference time, the number of channels used can be dynamically adjusted to
trade off accuracy for lowered power consumption and reduced latency by
selecting only a beginning subset of channels. This approach allows for a
single network to dynamically scale over a computation range, as opposed to
training and deploying multiple networks to support different levels of
computation scaling. Additionally, we extend the notion to multiple profiles,
each optimized for some specific range of computation scaling. We present
experiments on the computation and accuracy trade-offs of IDP for popular image
classification models and datasets. We demonstrate that, for MNIST and
CIFAR-10, IDP reduces computation significantly, e.g., by 75%, without
significantly compromising accuracy. We argue that IDP provides a convenient
and effective means for devices to lower computation costs dynamically to
reflect the current computation budget of the system. For example, VGG-16 with
50% IDP (using only the first 50% of channels) achieves 70% in accuracy on the
CIFAR-10 dataset compared to the standard network which achieves only 35%
accuracy when using the reduced channel set.
",Dynamic Channel Pruning in CNNs,convolutional neural network compression techniques
"  Many computer vision applications involve modeling complex spatio-temporal
patterns in high-dimensional motion data. Recently, restricted Boltzmann
machines (RBMs) have been widely used to capture and represent spatial patterns
in a single image or temporal patterns in several time slices. To model global
dynamics and local spatial interactions, we propose to theoretically extend the
conventional RBMs by introducing another term in the energy function to
explicitly model the local spatial interactions in the input data. A learning
method is then proposed to perform efficient learning for the proposed model.
We further introduce a new method for multi-class classification that can
effectively estimate the infeasible partition functions of different RBMs such
that RBM is treated as a generative model for classification purpose. The
improved RBM model is evaluated on two computer vision applications: facial
expression recognition and human action recognition. Experimental results on
benchmark databases demonstrate the effectiveness of the proposed algorithm.
",Spatio-Temporal Pattern Modeling with Restricted Boltzmann Machines,extending restricted boltzmann machines for computer vision applications
"  Deep neural networks are powerful learning models that achieve
state-of-the-art performance on many computer vision, speech, and language
processing tasks. In this paper, we study a fundamental question that arises
when designing deep network architectures: Given a target network architecture
can we design a smaller network architecture that approximates the operation of
the target network? The question is, in part, motivated by the challenge of
parameter reduction (compression) in modern deep neural networks, as the ever
increasing storage and memory requirements of these networks pose a problem in
resource constrained environments.
  In this work, we focus on deep convolutional neural network architectures,
and propose a novel randomized tensor sketching technique that we utilize to
develop a unified framework for approximating the operation of both the
convolutional and fully connected layers. By applying the sketching technique
along different tensor dimensions, we design changes to the convolutional and
fully connected layers that substantially reduce the number of effective
parameters in a network. We show that the resulting smaller network can be
trained directly, and has a classification accuracy that is comparable to the
original network.
",Neural Network Compression,neural network compression
"  In this paper, we present a novel Deep Triphone Embedding (DTE)
representation derived from Deep Neural Network (DNN) to encapsulate the
discriminative information present in the adjoining speech frames. DTEs are
generated using a four hidden layer DNN with 3000 nodes in each hidden layer at
the first-stage. This DNN is trained with the tied-triphone classification
accuracy as an optimization criterion. Thereafter, we retain the activation
vectors (3000) of the last hidden layer, for each speech MFCC frame, and
perform dimension reduction to further obtain a 300 dimensional representation,
which we termed as DTE. DTEs along with MFCC features are fed into a
second-stage four hidden layer DNN, which is subsequently trained for the task
of tied-triphone classification. Both DNNs are trained using tri-phone labels
generated from a tied-state triphone HMM-GMM system, by performing a
forced-alignment between the transcriptions and MFCC feature frames. We conduct
the experiments on publicly available TED-LIUM speech corpus. The results show
that the proposed DTE method provides an improvement of absolute 2.11% in
phoneme recognition, when compared with a competitive hybrid tied-state
triphone HMM-DNN system.
",Deep Triphone Embedding for Speech Recognition,deep triphone embedding for speech recognition
"  We study an online linear classification problem, in which the data is
generated by strategic agents who manipulate their features in an effort to
change the classification outcome. In rounds, the learner deploys a classifier,
and an adversarially chosen agent arrives, possibly manipulating her features
to optimally respond to the learner. The learner has no knowledge of the
agents' utility functions or ""real"" features, which may vary widely across
agents. Instead, the learner is only able to observe their ""revealed
preferences"" --- i.e. the actual manipulated feature vectors they provide. For
a broad family of agent cost functions, we give a computationally efficient
learning algorithm that is able to obtain diminishing ""Stackelberg regret"" ---
a form of policy regret that guarantees that the learner is obtaining loss
nearly as small as that of the best classifier in hindsight, even allowing for
the fact that agents will best-respond differently to the optimal classifier.
",Strategic Classification,adversarial online learning
"  We derive a new Bayesian Information Criterion (BIC) by formulating the
problem of estimating the number of clusters in an observed data set as
maximization of the posterior probability of the candidate models. Given that
some mild assumptions are satisfied, we provide a general BIC expression for a
broad class of data distributions. This serves as a starting point when
deriving the BIC for specific distributions. Along this line, we provide a
closed-form BIC expression for multivariate Gaussian distributed variables. We
show that incorporating the data structure of the clustering problem into the
derivation of the BIC results in an expression whose penalty term is different
from that of the original BIC. We propose a two-step cluster enumeration
algorithm. First, a model-based unsupervised learning algorithm partitions the
data according to a given set of candidate models. Subsequently, the number of
clusters is determined as the one associated with the model for which the
proposed BIC is maximal. The performance of the proposed two-step algorithm is
tested using synthetic and real data sets.
",Bayesian Information Criterion for Cluster Analysis,bayesian information criterion for cluster detection
"  Deep convolutional semantic segmentation (DCSS) learning doesn't converge to
an optimal local minimum with random parameters initializations; a pre-trained
model on the same domain becomes necessary to achieve convergence.In this work,
we propose a joint cooperative end-to-end learning method for DCSS. It
addresses many drawbacks with existing deep semantic segmentation learning; the
proposed approach simultaneously learn both segmentation and classification;
taking away the essential need of the pre-trained model for learning
convergence. We present an improved inception based architecture with partial
attention gating (PAG) over encoder information. The PAG also adds to achieve
faster convergence and better accuracy for segmentation task. We will show the
effectiveness of this learning on a diabetic retinopathy classification and
segmentation dataset.
",Cooperative Deep Semantic Segmentation Learning,deep learning for computer vision
"  Many real-world analytics problems involve two significant challenges:
prediction and optimization. Due to the typically complex nature of each
challenge, the standard paradigm is predict-then-optimize. By and large,
machine learning tools are intended to minimize prediction error and do not
account for how the predictions will be used in the downstream optimization
problem. In contrast, we propose a new and very general framework, called Smart
""Predict, then Optimize"" (SPO), which directly leverages the optimization
problem structure, i.e., its objective and constraints, for designing better
prediction models. A key component of our framework is the SPO loss function
which measures the decision error induced by a prediction.
  Training a prediction model with respect to the SPO loss is computationally
challenging, and thus we derive, using duality theory, a convex surrogate loss
function which we call the SPO+ loss. Most importantly, we prove that the SPO+
loss is statistically consistent with respect to the SPO loss under mild
conditions. Our SPO+ loss function can tractably handle any polyhedral, convex,
or even mixed-integer optimization problem with a linear objective. Numerical
experiments on shortest path and portfolio optimization problems show that the
SPO framework can lead to significant improvement under the
predict-then-optimize paradigm, in particular when the prediction model being
trained is misspecified. We find that linear models trained using SPO+ loss
tend to dominate random forest algorithms, even when the ground truth is highly
nonlinear.
",Predictive Modeling for Optimization Problems,machine learning and optimization
"  Due to the lack of enough generalization in the state-space, common methods
in Reinforcement Learning (RL) suffer from slow learning speed especially in
the early learning trials. This paper introduces a model-based method in
discrete state-spaces for increasing learning speed in terms of required
experience (but not required computational time) by exploiting generalization
in the experiences of the subspaces. A subspace is formed by choosing a subset
of features in the original state representation (full-space). Generalization
and faster learning in a subspace are due to many-to-one mapping of experiences
from the full-space to each state in the subspace. Nevertheless, due to
inherent perceptual aliasing in the subspaces, the policy suggested by each
subspace does not generally converge to the optimal policy. Our approach,
called Model Based Learning with Subspaces (MoBLeS), calculates confidence
intervals of the estimated Q-values in the full-space and in the subspaces.
These confidence intervals are used in the decision making, such that the agent
benefits the most from the possible generalization while avoiding from
detriment of the perceptual aliasing in the subspaces. Convergence of MoBLeS to
the optimal policy is theoretically investigated. Additionally, we show through
several experiments that MoBLeS improves the learning speed in the early
trials.
",Accelerating Reinforcement Learning with Subspace Methods,reinforcement learning
"  We propose a novel algorithm for sequential matrix completion in a
recommender system setting, where the $(i,j)$th entry of the matrix corresponds
to a user $i$'s rating of product $j$. The objective of the algorithm is to
provide a sequential policy for user-product pair recommendation which will
yield the highest possible ratings after a finite time horizon. The algorithm
uses a Gamma process factor model with two posterior-focused bandit policies,
Thompson Sampling and Information-Directed Sampling. While Thompson Sampling
shows competitive performance in simulations, state-of-the-art performance is
obtained from Information-Directed Sampling, which makes its recommendations
based off a ratio between the expected reward and a measure of information
gain. To our knowledge, this is the first implementation of Information
Directed Sampling on large real datasets.
  This approach contributes to a recent line of research on bandit approaches
to collaborative filtering including Kawale et al. (2015), Li et al. (2010),
Bresler et al. (2014), Li et al. (2016), Deshpande & Montanari (2012), and Zhao
et al. (2013). The setting of this paper, as has been noted in Kawale et al.
(2015) and Zhao et al. (2013), presents significant challenges to bounding
regret after finite horizons. We discuss these challenges in relation to
simpler models for bandits with side information, such as linear or gaussian
process bandits, and hope the experiments presented here motivate further
research toward theoretical guarantees.
",Sequential Matrix Completion in Recommender Systems,recommender systems
"  Policy gradient methods are widely used in reinforcement learning algorithms
to search for better policies in the parameterized policy space. They do
gradient search in the policy space and are known to converge very slowly.
Nesterov developed an accelerated gradient search algorithm for convex
optimization problems. This has been recently extended for non-convex and also
stochastic optimization. We use Nesterov's acceleration for policy gradient
search in the well-known actor-critic algorithm and show the convergence using
ODE method. We tested this algorithm on a scheduling problem. Here an incoming
job is scheduled into one of the four queues based on the queue lengths. We see
from experimental results that algorithm using Nesterov's acceleration has
significantly better performance compared to algorithm which do not use
acceleration. To the best of our knowledge this is the first time Nesterov's
acceleration has been used with actor-critic algorithm.
",Accelerating Policy Gradient Methods,reinforcement learning with nesterov's acceleration
"  We consider the multi-label ranking approach to multi-label learning.
Boosting is a natural method for multi-label ranking as it aggregates weak
predictions through majority votes, which can be directly used as scores to
produce a ranking of the labels. We design online boosting algorithms with
provable loss bounds for multi-label ranking. We show that our first algorithm
is optimal in terms of the number of learners required to attain a desired
accuracy, but it requires knowledge of the edge of the weak learners. We also
design an adaptive algorithm that does not require this knowledge and is hence
more practical. Experimental results on real data sets demonstrate that our
algorithms are at least as good as existing batch boosting algorithms.
",Multi-Label Ranking with Boosting Algorithms,multi-label learning
"  This paper formulates the protocol for prediction of packs, which a special
case of prediction under delayed feedback. Under this protocol, the learner
must make a few predictions without seeing the outcomes and then the outcomes
are revealed. We develop the theory of prediction with expert advice for packs.
By applying Vovk's Aggregating Algorithm to this problem we obtain a number of
algorithms with tight upper bounds. We carry out empirical experiments on
housing data.
",Prediction with Delayed Feedback,prediction under delayed feedback
"  Visual exploration of high-dimensional real-valued datasets is a fundamental
task in exploratory data analysis (EDA). Existing methods use predefined
criteria to choose the representation of data. There is a lack of methods that
(i) elicit from the user what she has learned from the data and (ii) show
patterns that she does not know yet. We construct a theoretical model where
identified patterns can be input as knowledge to the system. The knowledge
syntax here is intuitive, such as ""this set of points forms a cluster"", and
requires no knowledge of maths. This background knowledge is used to find a
Maximum Entropy distribution of the data, after which the system provides the
user data projections in which the data and the Maximum Entropy distribution
differ the most, hence showing the user aspects of the data that are maximally
informative given the user's current knowledge. We provide an open source EDA
system with tailored interactive visualizations to demonstrate these concepts.
We study the performance of the system and present use cases on both synthetic
and real data. We find that the model and the prototype system allow the user
to learn information efficiently from various data sources and the system works
sufficiently fast in practice. We conclude that the information theoretic
approach to exploratory data analysis where patterns observed by a user are
formalized as constraints provides a principled, intuitive, and efficient basis
for constructing an EDA system.
",Interactive Visual Data Exploration,exploratory data analysis (eda)
"  We develop an algorithm for systematic design of a large artificial neural
network using a progression property. We find that some non-linear functions,
such as the rectifier linear unit and its derivatives, hold the property. The
systematic design addresses the choice of network size and regularization of
parameters. The number of nodes and layers in network increases in progression
with the objective of consistently reducing an appropriate cost. Each layer is
optimized at a time, where appropriate parameters are learned using convex
optimization. Regularization parameters for convex optimization do not need a
significant manual effort for tuning. We also use random instances for some
weight matrices, and that helps to reduce the number of parameters we learn.
The developed network is expected to show good generalization power due to
appropriate regularization and use of random weights in the layers. This
expectation is verified by extensive experiments for classification and
regression problems, using standard databases.
",Systematic Design of Artificial Neural Networks,artificial neural network design
"  Though a large body of computer vision research has investigated developing
generic semantic representations, efforts towards developing a similar
representation for 3D has been limited. In this paper, we learn a generic 3D
representation through solving a set of foundational proxy 3D tasks:
object-centric camera pose estimation and wide baseline feature matching. Our
method is based upon the premise that by providing supervision over a set of
carefully selected foundational tasks, generalization to novel tasks and
abstraction capabilities can be achieved. We empirically show that the internal
representation of a multi-task ConvNet trained to solve the above core problems
generalizes to novel 3D tasks (e.g., scene layout estimation, object pose
estimation, surface normal estimation) without the need for fine-tuning and
shows traits of abstraction abilities (e.g., cross-modality pose estimation).
In the context of the core supervised tasks, we demonstrate our representation
achieves state-of-the-art wide baseline feature matching results without
requiring apriori rectification (unlike SIFT and the majority of learned
features). We also show 6DOF camera pose estimation given a pair local image
patches. The accuracy of both supervised tasks come comparable to humans.
Finally, we contribute a large-scale dataset composed of object-centric street
view scenes along with point correspondences and camera pose information, and
conclude with a discussion on the learned representation and open research
questions.
",Learning Generic 3D Representations,3d computer vision
"  In this paper, we propose deep learning architectures (FNN, CNN and LSTM) to
forecast a regression model for time dependent data. These algorithm's are
designed to handle Floating Car Data (FCD) historic speeds to predict road
traffic data. For this we aggregate the speeds into the network inputs in an
innovative way. We compare the RMSE thus obtained with the results of a simpler
physical model, and show that the latter achieves better RMSE accuracy. We also
propose a new indicator, which evaluates the algorithms improvement when
compared to a benchmark prediction. We conclude by questioning the interest of
using deep learning methods for this specific regression task.
",Deep Learning for Traffic Prediction,deep learning in transportation
"  Mobile phones provide an excellent opportunity for building context-aware
applications. In particular, location-based services are important
context-aware services that are more and more used for enforcing security
policies, for supporting indoor room navigation, and for providing personalized
assistance. However, a major problem still remains unaddressed---the lack of
solutions that work across buildings while not using additional infrastructure
and also accounting for privacy and reliability needs. In this paper, a
privacy-preserving, multi-modal, cross-building, collaborative localization
platform is proposed based on Wi-Fi RSSI (existing infrastructure), Cellular
RSSI, sound and light levels, that enables room-level localization as main
application (though sub room level granularity is possible). The privacy is
inherently built into the solution based on onion routing, and
perturbation/randomization techniques, and exploits the idea of weighted
collaboration to increase the reliability as well as to limit the effect of
noisy devices (due to sensor noise/privacy). The proposed solution has been
analyzed in terms of privacy, accuracy, optimum parameters, and other overheads
on location data collected at multiple indoor and outdoor locations using an
Android app.
",Indoor Localization Systems,context-aware indoor localization
"  High-dimensional data in many areas such as computer vision and machine
learning tasks brings in computational and analytical difficulty. Feature
selection which selects a subset from observed features is a widely used
approach for improving performance and effectiveness of machine learning models
with high-dimensional data. In this paper, we propose a novel AutoEncoder
Feature Selector (AEFS) for unsupervised feature selection which combines
autoencoder regression and group lasso tasks. Compared to traditional feature
selection methods, AEFS can select the most important features by excavating
both linear and nonlinear information among features, which is more flexible
than the conventional self-representation method for unsupervised feature
selection with only linear assumptions. Experimental results on benchmark
dataset show that the proposed method is superior to the state-of-the-art
method.
",Unsupervised Feature Selection,machine learning
"  In this paper, a sequential probing method for interference constraint
learning is proposed to allow a centralized Cognitive Radio Network (CRN)
accessing the frequency band of a Primary User (PU) in an underlay cognitive
scenario with a designed PU protection specification. The main idea is that the
CRN probes the PU and subsequently eavesdrops the reverse PU link to acquire
the binary ACK/NACK packet. This feedback indicates whether the probing-induced
interference is harmful or not and can be used to learn the PU interference
constraint. The cognitive part of this sequential probing process is the
selection of the power levels of the Secondary Users (SUs) which aims to learn
the PU interference constraint with a minimum number of probing attempts while
setting a limit on the number of harmful probing-induced interference events or
equivalently of NACK packet observations over a time window. This constrained
design problem is studied within the Active Learning (AL) framework and an
optimal solution is derived and implemented with a sophisticated, accurate and
fast Bayesian Learning method, the Expectation Propagation (EP). The
performance of this solution is also demonstrated through numerical simulations
and compared with modified versions of AL techniques we developed in earlier
work.
",Interference Constraint Learning in Cognitive Radio Networks,cognitive radio network interference constraint learning
"  Deep Neural Networks (DNNs) often struggle with one-shot learning where we
have only one or a few labeled training examples per category. In this paper,
we argue that by using side information, we may compensate the missing
information across classes. We introduce two statistical approaches for fusing
side information into data representation learning to improve one-shot
learning. First, we propose to enforce the statistical dependency between data
representations and multiple types of side information. Second, we introduce an
attention mechanism to efficiently treat examples belonging to the
'lots-of-examples' classes as quasi-samples (additional training samples) for
'one-example' classes. We empirically show that our learning architecture
improves over traditional softmax regression networks as well as
state-of-the-art attentional regression networks on one-shot recognition tasks.
",One-Shot Learning with Side Information,one-shot learning
"  We establish novel generalization bounds for learning algorithms that
converge to global minima. We do so by deriving black-box stability results
that only depend on the convergence of a learning algorithm and the geometry
around the minimizers of the loss function. The results are shown for nonconvex
loss functions satisfying the Polyak-{\L}ojasiewicz (PL) and the quadratic
growth (QG) conditions. We further show that these conditions arise for some
neural networks with linear activations. We use our black-box results to
establish the stability of optimization algorithms such as stochastic gradient
descent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and
the stochastic variance reduced gradient method (SVRG), in both the PL and the
strongly convex setting. Our results match or improve state-of-the-art
generalization bounds and can easily be extended to similar optimization
algorithms. Finally, we show that although our results imply comparable
stability for SGD and GD in the PL setting, there exist simple neural networks
with multiple local minima where SGD is stable but GD is not.
",Generalization Bounds for Optimization Algorithms,machine learning
"  Generative adversarial networks (GANs) are a family of generative models that
do not minimize a single training criterion. Unlike other generative models,
the data distribution is learned via a game between a generator (the generative
model) and a discriminator (a teacher providing training signal) that each
minimize their own cost. GANs are designed to reach a Nash equilibrium at which
each player cannot reduce their cost without changing the other players'
parameters. One useful approach for the theory of GANs is to show that a
divergence between the training distribution and the model distribution obtains
its minimum value at equilibrium. Several recent research directions have been
motivated by the idea that this divergence is the primary guide for the
learning process and that every step of learning should decrease the
divergence. We show that this view is overly restrictive. During GAN training,
the discriminator provides learning signal in situations where the gradients of
the divergences between distributions would not be useful. We provide empirical
counterexamples to the view of GAN training as divergence minimization.
Specifically, we demonstrate that GANs are able to learn distributions in
situations where the divergence minimization point of view predicts they would
fail. We also show that gradient penalties motivated from the divergence
minimization perspective are equally helpful when applied in other contexts in
which the divergence minimization perspective does not predict they would be
helpful. This contributes to a growing body of evidence that GAN training may
be more usefully viewed as approaching Nash equilibria via trajectories that do
not necessarily minimize a specific divergence at each step.
",GAN Training Dynamics,generative adversarial networks
"  Our everyday interactions with pervasive systems generate traces that capture
various aspects of human behavior and enable machine learning algorithms to
extract latent information about users. In this paper, we propose a machine
learning interpretability framework that enables users to understand how these
generated traces violate their privacy.
","""Machine Learning Privacy Interpretability""",data privacy
"  Providing long-range forecasts is a fundamental challenge in time series
modeling, which is only compounded by the challenge of having to form such
forecasts when a time series has never previously been observed. The latter
challenge is the time series version of the cold-start problem seen in
recommender systems which, to our knowledge, has not been addressed in previous
work. A similar problem occurs when a long range forecast is required after
only observing a small number of time points --- a warm start forecast. With
these aims in mind, we focus on forecasting seasonal profiles---or baseline
demand---for periods on the order of a year in three cases: the long range case
with multiple previously observed seasonal profiles, the cold start case with
no previous observed seasonal profiles, and the warm start case with only a
single partially observed profile. Classical time series approaches that
perform iterated step-ahead forecasts based on previous observations struggle
to provide accurate long range predictions; in settings with little to no
observed data, such approaches are simply not applicable. Instead, we present a
straightforward framework which combines ideas from high-dimensional regression
and matrix factorization on a carefully constructed data matrix. Key to our
formulation and resulting performance is leveraging (1) repeated patterns over
fixed periods of time and across series, and (2) metadata associated with the
individual series; without this additional data, the cold-start/warm-start
problems are nearly impossible to solve. We demonstrate that our framework can
accurately forecast an array of seasonal profiles on multiple large scale
datasets.
",Time Series Forecasting for Cold-Start and Warm-Start Problems,time series forecasting
"  Optimization plays a key role in machine learning. Recently, stochastic
second-order methods have attracted much attention due to their low
computational cost in each iteration. However, these algorithms might perform
poorly especially if it is hard to approximate the Hessian well and
efficiently. As far as we know, there is no effective way to handle this
problem. In this paper, we resort to Nesterov's acceleration technique to
improve the convergence performance of a class of second-order methods called
approximate Newton. We give a theoretical analysis that Nesterov's acceleration
technique can improve the convergence performance for approximate Newton just
like for first-order methods. We accordingly propose an accelerated regularized
sub-sampled Newton. Our accelerated algorithm performs much better than the
original regularized sub-sampled Newton in experiments, which validates our
theory empirically. Besides, the accelerated regularized sub-sampled Newton has
good performance comparable to or even better than classical algorithms.
",Accelerating Second-Order Optimization Methods,machine learning optimization
"  Previous models for learning entity and relationship embeddings of knowledge
graphs such as TransE, TransH, and TransR aim to explore new links based on
learned representations. However, these models interpret relationships as
simple translations on entity embeddings. In this paper, we try to learn more
complex connections between entities and relationships. In particular, we use a
Convolutional Neural Network (CNN) to learn entity and relationship
representations in knowledge graphs. In our model, we treat entities and
relationships as one-dimensional numerical sequences with the same length.
After that, we combine each triplet of head, relationship, and tail together as
a matrix with height 3. CNN is applied to the triplets to get confidence
scores. Positive and manually corrupted negative triplets are used to train the
embeddings and the CNN model simultaneously. Experimental results on public
benchmark datasets show that the proposed model outperforms state-of-the-art
models on exploring unseen relationships, which proves that CNN is effective to
learn complex interactive patterns between entities and relationships.
",Entity and Relationship Embeddings in Knowledge Graphs,knowledge graph embeddings using convolutional neural networks
"  Adaptive optimal control using value iteration initiated from a stabilizing
control policy is theoretically analyzed in terms of stability of the system
during the learning stage without ignoring the effects of approximation errors.
This analysis includes the system operated using any single/constant resulting
control policy and also using an evolving/time-varying control policy. A
feature of the presented results is providing estimations of the \textit{region
of attraction} so that if the initial condition is within the region, the whole
trajectory will remain inside it and hence, the function approximation results
remain valid.
",Adaptive Optimal Control Stability Analysis,adaptive control
"  Deep learning models (aka Deep Neural Networks) have revolutionized many
fields including computer vision, natural language processing, speech
recognition, and is being increasingly used in clinical healthcare
applications. However, few works exist which have benchmarked the performance
of the deep learning models with respect to the state-of-the-art machine
learning models and prognostic scoring systems on publicly available healthcare
datasets. In this paper, we present the benchmarking results for several
clinical prediction tasks such as mortality prediction, length of stay
prediction, and ICD-9 code group prediction using Deep Learning models,
ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA
scores. We used the Medical Information Mart for Intensive Care III (MIMIC-III)
(v1.4) publicly available dataset, which includes all patients admitted to an
ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the
benchmarking tasks. Our results show that deep learning models consistently
outperform all the other approaches especially when the `raw' clinical time
series data is used as input features to the models.
",Benchmarking Deep Learning Models in Healthcare,healthcare ai applications
"  Performance of data-driven network for tumor classification varies with
stain-style of histopathological images. This article proposes the stain-style
transfer (SST) model based on conditional generative adversarial networks
(GANs) which is to learn not only the certain color distribution but also the
corresponding histopathological pattern. Our model considers feature-preserving
loss in addition to well-known GAN loss. Consequently our model does not only
transfers initial stain-styles to the desired one but also prevent the
degradation of tumor classifier on transferred images. The model is examined
using the CAMELYON16 dataset.
",Stain-Style Transfer for Histopathological Image Analysis,medical imaging
"  The study of representations invariant to common transformations of the data
is important to learning. Most techniques have focused on local approximate
invariance implemented within expensive optimization frameworks lacking
explicit theoretical guarantees. In this paper, we study kernels that are
invariant to a unitary group while having theoretical guarantees in addressing
the important practical issue of unavailability of transformed versions of
labelled data. A problem we call the Unlabeled Transformation Problem which is
a special form of semi-supervised learning and one-shot learning. We present a
theoretically motivated alternate approach to the invariant kernel SVM based on
which we propose Max-Margin Invariant Features (MMIF) to solve this problem. As
an illustration, we design an framework for face recognition and demonstrate
the efficacy of our approach on a large scale semi-synthetic dataset with
153,000 images and a new challenging protocol on Labelled Faces in the Wild
(LFW) while out-performing strong baselines.
",Invariant Representations for Learning,invariant learning
"  Availability of an explainable deep learning model that can be applied to
practical real world scenarios and in turn, can consistently, rapidly and
accurately identify specific and minute traits in applicable fields of
biological sciences, is scarce. Here we consider one such real world example
viz., accurate identification, classification and quantification of biotic and
abiotic stresses in crop research and production. Up until now, this has been
predominantly done manually by visual inspection and require specialized
training. However, such techniques are hindered by subjectivity resulting from
inter- and intra-rater cognitive variability. Here, we demonstrate the ability
of a machine learning framework to identify and classify a diverse set of
foliar stresses in the soybean plant with remarkable accuracy. We also present
an explanation mechanism using gradient-weighted class activation mapping that
isolates the visual symptoms used by the model to make predictions. This
unsupervised identification of unique visual symptoms for each stress provides
a quantitative measure of stress severity, allowing for identification,
classification and quantification in one framework. The learnt model appears to
be agnostic to species and make good predictions for other (non-soybean)
species, demonstrating an ability of transfer learning.
","""Plant Stress Identification using Explainable Deep Learning""",explainable machine learning in crop stress detection
"  Transfer learning using deep neural networks as feature extractors has become
increasingly popular over the past few years. It allows to obtain
state-of-the-art accuracy on datasets too small to train a deep neural network
on its own, and it provides cutting edge descriptors that, combined with
nonparametric learning methods, allow rapid and flexible deployment of
performing solutions in computationally restricted settings. In this paper, we
are interested in showing that the features extracted using deep neural
networks have specific properties which can be used to improve accuracy of
downstream nonparametric learning methods. Namely, we demonstrate that for some
distributions where information is embedded in a few coordinates, segmenting
feature vectors can lead to better accuracy. We show how this model can be
applied to real datasets by performing experiments using three mainstream deep
neural network feature extractors and four databases, in vision and audio.
",Deep Learning for Feature Extraction,transfer learning in deep neural networks
"  Development systems for deep learning (DL), such as Theano, Torch,
TensorFlow, or MXNet, are easy-to-use tools for creating complex neural network
models. Since gradient computations are automatically baked in, and execution
is mapped to high performance hardware, these models can be trained end-to-end
on large amounts of data. However, it is currently not easy to implement many
basic machine learning primitives in these systems (such as Gaussian processes,
least squares estimation, principal components analysis, Kalman smoothing),
mainly because they lack efficient support of linear algebra primitives as
differentiable operators. We detail how a number of matrix decompositions
(Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators.
We have implemented these primitives in MXNet, running on CPU and GPU in single
and double precision. We sketch use cases of these new operators, learning
Gaussian process and Bayesian linear regression models, where we demonstrate
very substantial reductions in implementation complexity and running time
compared to previous codes. Our MXNet extension allows end-to-end learning of
hybrid models, which combine deep neural networks (DNNs) with Bayesian
concepts, with applications in advanced Gaussian process models, scalable
Bayesian optimization, and Bayesian active learning.
","""Differentiable Linear Algebra Operators for Deep Learning""",deep learning primitives
"  In this work, we addressed the issue of applying a stochastic classifier and
a local, fuzzy confusion matrix under the framework of multi-label
classification. We proposed a novel solution to the problem of correcting label
pairwise ensembles. The main step of the correction procedure is to compute
classifier- specific competence and cross-competence measures, which estimates
error pattern of the underlying classifier. We considered two improvements of
the method of obtaining confusion matrices. The first one is aimed to deal with
imbalanced labels. The other utilizes double labelled instances which are
usually removed during the pairwise transformation. The proposed methods were
evaluated using 29 benchmark datasets. In order to assess the efficiency of the
introduced models, they were compared against 1 state-of-the-art approach and
the correction scheme based on the original method of confusion matrix
estimation. The comparison was performed using four different multi-label
evaluation measures: macro and micro-averaged F1 loss, zero-one loss and
Hamming loss. Additionally, we investigated relations between classification
quality, which is expressed in terms of different quality criteria, and
characteristics of multi-label datasets such as average imbalance ratio or
label density. The experimental study reveals that the correction approaches
significantly outperforms the reference method only in terms of zero-one loss.
",Multi-Label Classification Correction Methods,multi-label classification using stochastic classifiers and fuzzy confusion matrices
"  We present a new model-based integrative method for clustering objects given
both vectorial data, which describes the feature of each object, and network
data, which indicates the similarity of connected objects. The proposed general
model is able to cluster the two types of data simultaneously within one
integrative probabilistic model, while traditional methods can only handle one
data type or depend on transforming one data type to another. Bayesian
inference of the clustering is conducted based on a Markov chain Monte Carlo
algorithm. A special case of the general model combining the Gaussian mixture
model and the stochastic block model is extensively studied. We used both
synthetic data and real data to evaluate this new method and compare it with
alternative methods. The results show that our simultaneous clustering method
performs much better. This improvement is due to the power of the model-based
probabilistic approach for efficiently integrating information.
",Integrative Clustering of Vectorial and Network Data,simultaneous clustering of vectorial and network data
"  Recent research has revealed that the output of Deep Neural Networks (DNN)
can be easily altered by adding relatively small perturbations to the input
vector. In this paper, we analyze an attack in an extremely limited scenario
where only one pixel can be modified. For that we propose a novel method for
generating one-pixel adversarial perturbations based on differential evolution
(DE). It requires less adversarial information (a black-box attack) and can
fool more types of networks due to the inherent features of DE. The results
show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and
16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least
one target class by modifying just one pixel with 74.03% and 22.91% confidence
on average. We also show the same vulnerability on the original CIFAR-10
dataset. Thus, the proposed attack explores a different take on adversarial
machine learning in an extreme limited scenario, showing that current DNNs are
also vulnerable to such low dimension attacks. Besides, we also illustrate an
important application of DE (or broadly speaking, evolutionary computation) in
the domain of adversarial machine learning: creating tools that can effectively
generate low-cost adversarial attacks against neural networks for evaluating
robustness.
",One-Pixel Adversarial Attacks on Deep Neural Networks,one-pixel adversarial attacks on deep neural networks
"  When each data point is a large graph, graph statistics such as densities of
certain subgraphs (motifs) can be used as feature vectors for machine learning.
While intuitive, motif counts are expensive to compute and difficult to work
with theoretically. Via graphon theory, we give an explicit quantitative bound
for the ability of motif homomorphisms to distinguish large networks under both
generative and sampling noise. Furthermore, we give similar bounds for the
graph spectrum and connect it to homomorphism densities of cycles. This results
in an easily computable classifier on graph data with theoretical performance
guarantee. Our method yields competitive results on classification tasks for
the autoimmune disease Lupus Erythematosus.
",Graph Classification with Motif-based Features,machine learning for graph data classification
"  The fast iterative soft thresholding algorithm (FISTA) is used to solve
convex regularized optimization problems in machine learning. Distributed
implementations of the algorithm have become popular since they enable the
analysis of large datasets. However, existing formulations of FISTA communicate
data at every iteration which reduces its performance on modern distributed
architectures. The communication costs of FISTA, including bandwidth and
latency costs, is closely tied to the mathematical formulation of the
algorithm. This work reformulates FISTA to communicate data at every k
iterations and reduce data communication when operating on large data sets. We
formulate the algorithm for two different optimization methods on the Lasso
problem and show that the latency cost is reduced by a factor of k while
bandwidth and floating-point operation costs remain the same. The convergence
rates and stability properties of the reformulated algorithms are similar to
the standard formulations. The performance of communication-avoiding FISTA and
Proximal Newton methods is evaluated on 1 to 1024 nodes for multiple benchmarks
and demonstrate average speedups of 3-10x with scaling properties that
outperform the classical algorithms.
",Distributed Optimization Algorithms for Machine Learning,distributed machine learning
"  This paper presents a method for identifying mechanical parameters of robots
or objects, such as their mass and friction coefficients. Key features are the
use of off-the-shelf physics engines and the adaptation of a Bayesian
optimization technique towards minimizing the number of real-world experiments
needed for model-based reinforcement learning. The proposed framework
reproduces in a physics engine experiments performed on a real robot and
optimizes the model's mechanical parameters so as to match real-world
trajectories. The optimized model is then used for learning a policy in
simulation, before real-world deployment. It is well understood, however, that
it is hard to exactly reproduce real trajectories in simulation. Moreover, a
near-optimal policy can be frequently found with an imperfect model. Therefore,
this work proposes a strategy for identifying a model that is just good enough
to approximate the value of a locally optimal policy with a certain confidence,
instead of wasting effort on identifying the most accurate model. Evaluations,
performed both in simulation and on a real robotic manipulation task, indicate
that the proposed strategy results in an overall time-efficient, integrated
model identification and learning solution, which significantly improves the
data-efficiency of existing policy search algorithms.
",Robot Model Identification,model-based reinforcement learning for robot control
"  This paper reviews the checkered history of predictive distributions in
statistics and discusses two developments, one from recent literature and the
other new. The first development is bringing predictive distributions into
machine learning, whose early development was so deeply influenced by two
remarkable groups at the Institute of Automation and Remote Control. The second
development is combining predictive distributions with kernel methods, which
were originated by one of those groups, including Emmanuel Braverman.
",Predictive Distributions in Machine Learning,predictive distributions in statistics and machine learning
"  We propose a new algorithm for finite sum optimization which we call the
curvature-aided incremental aggregated gradient (CIAG) method. Motivated by the
problem of training a classifier for a d-dimensional problem, where the number
of training data is $m$ and $m \gg d \gg 1$, the CIAG method seeks to
accelerate incremental aggregated gradient (IAG) methods using aids from the
curvature (or Hessian) information, while avoiding the evaluation of matrix
inverses required by the incremental Newton (IN) method. Specifically, our idea
is to exploit the incrementally aggregated Hessian matrix to trace the full
gradient vector at every incremental step, therefore achieving an improved
linear convergence rate over the state-of-the-art IAG methods. For strongly
convex problems, the fast linear convergence rate requires the objective
function to be close to quadratic, or the initial point to be close to optimal
solution. Importantly, we show that running one iteration of the CIAG method
yields the same improvement to the optimality gap as running one iteration of
the full gradient method, while the complexity is $O(d^2)$ for CIAG and $O(md)$
for the full gradient. Overall, the CIAG method strikes a balance between the
high computation complexity incremental Newton-type methods and the slow IAG
method. Our numerical results support the theoretical findings and show that
the CIAG method often converges with much fewer iterations than IAG, and
requires much shorter running time than IN when the problem dimension is high.
",Optimization Algorithms,machine learning/ optimization
"  Dynamic time warping constitutes a major tool for analyzing time series. In
particular, computing a mean series of a given sample of series in dynamic time
warping spaces (by minimizing the Fr\'echet function) is a challenging
computational problem, so far solved by several heuristic and inexact
strategies. We spot some inaccuracies in the literature on exact mean
computation in dynamic time warping spaces. Our contributions comprise an exact
dynamic program computing a mean (useful for benchmarking and evaluating known
heuristics). Based on this dynamic program, we empirically study properties
like uniqueness and length of a mean. Moreover, experimental evaluations reveal
substantial deficits of state-of-the-art heuristics in terms of their output
quality. We also give an exact polynomial-time algorithm for the special case
of binary time series.
",Mean Computation in Dynamic Time Warping Spaces,dynamic time warping
"  In recent years, analyzing task-based fMRI (tfMRI) data has become an
essential tool for understanding brain function and networks. However, due to
the sheer size of tfMRI data, its intrinsic complex structure, and lack of
ground truth of underlying neural activities, modeling tfMRI data is hard and
challenging. Previously proposed data-modeling methods including Independent
Component Analysis (ICA) and Sparse Dictionary Learning only provided a weakly
established model based on blind source separation under the strong assumption
that original fMRI signals could be linearly decomposed into time series
components with corresponding spatial maps. Meanwhile, analyzing and learning a
large amount of tfMRI data from a variety of subjects has been shown to be very
demanding but yet challenging even with technological advances in computational
hardware. Given the Convolutional Neural Network (CNN), a robust method for
learning high-level abstractions from low-level data such as tfMRI time series,
in this work we propose a fast and scalable novel framework for distributed
deep Convolutional Autoencoder model. This model aims to both learn the complex
hierarchical structure of the tfMRI data and to leverage the processing power
of multiple GPUs in a distributed fashion. To implement such a model, we have
created an enhanced processing pipeline on the top of Apache Spark and
Tensorflow library, leveraging from a very large cluster of GPU machines.
Experimental data from applying the model on the Human Connectome Project (HCP)
show that the proposed model is efficient and scalable toward tfMRI big data
analytics, thus enabling data-driven extraction of hierarchical neuroscientific
information from massive fMRI big data in the future.
",Distributed Deep Learning for fMRI Data Analysis,deep learning for fmri data analytics
"  Probabilistic methods for classifying text form a rich tradition in machine
learning and natural language processing. For many important problems, however,
class prediction is uninteresting because the class is known, and instead the
focus shifts to estimating latent quantities related to the text, such as
affect or ideology. We focus on one such problem of interest, estimating the
ideological positions of 55 Irish legislators in the 1991 D\'ail confidence
vote. To solve the D\'ail scaling problem and others like it, we develop a text
modeling framework that allows actors to take latent positions on a ""gray""
spectrum between ""black"" and ""white"" polar opposites. We are able to validate
results from this model by measuring the influences exhibited by individual
words, and we are able to quantify the uncertainty in the scaling estimates by
using a sentence-level block bootstrap. Applying our method to the D\'ail
debate, we are able to scale the legislators between extreme pro-government and
pro-opposition in a way that reveals nuances in their speeches not captured by
their votes or party affiliations.
",Ideology Detection from Text Data,text-based ideology estimation
"  This paper describes a novel text-to-speech (TTS) technique based on deep
convolutional neural networks (CNN), without use of any recurrent units.
Recurrent neural networks (RNN) have become a standard technique to model
sequential data recently, and this technique has been used in some cutting-edge
neural TTS techniques. However, training RNN components often requires a very
powerful computer, or a very long time, typically several days or weeks. Recent
other studies, on the other hand, have shown that CNN-based sequence synthesis
can be much faster than RNN-based techniques, because of high
parallelizability. The objective of this paper is to show that an alternative
neural TTS based only on CNN alleviate these economic costs of training. In our
experiment, the proposed Deep Convolutional TTS was sufficiently trained
overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while
the quality of the synthesized speech was almost acceptable.
",CNN-based Text-to-Speech Synthesis,text-to-speech (tts) using convolutional neural networks (cnn)
"  We consider the non-stochastic Multi-Armed Bandit problem in a setting where
there is a fixed and known metric on the action space that determines a cost
for switching between any pair of actions. The loss of the online learner has
two components: the first is the usual loss of the selected actions, and the
second is an additional loss due to switching between actions. Our main
contribution gives a tight characterization of the expected minimax regret in
this setting, in terms of a complexity measure $\mathcal{C}$ of the underlying
metric which depends on its covering numbers. In finite metric spaces with $k$
actions, we give an efficient algorithm that achieves regret of the form
$\widetilde{O}(\max\{\mathcal{C}^{1/3}T^{2/3},\sqrt{kT}\})$, and show that this
is the best possible. Our regret bound generalizes previous known regret bounds
for some special cases: (i) the unit-switching cost regret
$\widetilde{\Theta}(\max\{k^{1/3}T^{2/3},\sqrt{kT}\})$ where
$\mathcal{C}=\Theta(k)$, and (ii) the interval metric with regret
$\widetilde{\Theta}(\max\{T^{2/3},\sqrt{kT}\})$ where $\mathcal{C}=\Theta(1)$.
For infinite metrics spaces with Lipschitz loss functions, we derive a tight
regret bound of $\widetilde{\Theta}(T^{\frac{d+1}{d+2}})$ where $d \ge 1$ is
the Minkowski dimension of the space, which is known to be tight even when
there are no switching costs.
",Multi-Armed Bandit with Switching Costs,non-stochastic multi-armed bandit problem
"  Building on the previous work of Lee et al. and Ferdinand et al. on coded
computation, we propose a sequential approximation framework for solving
optimization problems in a distributed manner. In a distributed computation
system, latency caused by individual processors (""stragglers"") usually causes a
significant delay in the overall process. The proposed method is powered by a
sequential computation scheme, which is designed specifically for systems with
stragglers. This scheme has the desirable property that the user is guaranteed
to receive useful (approximate) computation results whenever a processor
finishes its subtask, even in the presence of uncertain latency. In this paper,
we give a coding theorem for sequentially computing matrix-vector
multiplications, and the optimality of this coding scheme is also established.
As an application of the results, we demonstrate solving optimization problems
using a sequential approximation approach, which accelerates the algorithm in a
distributed system with stragglers.
",Distributed Optimization in Straggler-Prone Systems,distributed optimization
"  We propose and evaluate new techniques for compressing and speeding up dense
matrix multiplications as found in the fully connected and recurrent layers of
neural networks for embedded large vocabulary continuous speech recognition
(LVCSR). For compression, we introduce and study a trace norm regularization
technique for training low rank factored versions of matrix multiplications.
Compared to standard low rank training, we show that our method leads to good
accuracy versus number of parameter trade-offs and can be used to speed up
training of large models. For speedup, we enable faster inference on ARM
processors through new open sourced kernels optimized for small batch sizes,
resulting in 3x to 7x speed ups over the widely used gemmlowp library. Beyond
LVCSR, we expect our techniques and kernels to be more generally applicable to
embedded neural networks with large fully connected or recurrent layers.
",Matrix Compression for Efficient Neural Networks,deep learning computation optimization for embedded systems
"  Machine Learning (ML) techniques, such as Neural Network, are widely used in
today's applications. However, there is still a big gap between the current ML
systems and users' requirements. ML systems focus on improving the performance
of models in training, while individual users cares more about response time
and expressiveness of the tool. Many existing research and product begin to
move computation towards edge devices. Based on the numerical computing system
Owl, we propose to build the Zoo system to support construction, compose, and
deployment of ML models on edge and local devices.
",Edge AI Deployment,machine learning on edge devices
"  The increasing popularity of server usage has brought a plenty of anomaly log
events, which have threatened a vast collection of machines. Recognizing and
categorizing the anomalous events thereby is a much salient work for our
systems, especially the ones generate the massive amount of data and harness it
for technology value creation and business development. To assist in focusing
on the classification and the prediction of anomaly events, and gaining
critical insights from system event records, we propose a novel log
preprocessing method which is very effective to filter abundant information and
retain critical characteristics. Additionally, a competitive approach for
automated classification of anomalous events detected from the distributed
system logs with the state-of-the-art deep (Convolutional Neural Network)
architectures is proposed in this paper. We measure a series of deep CNN
algorithms with varied hyper-parameter combinations by using standard
evaluation metrics, the results of our study reveals the advantages and
potential capabilities of the proposed deep CNN models for anomaly event
classification tasks on real-world systems. The optimal classification
precision of our approach is 98.14%, which surpasses the popular traditional
machine learning methods.
",Anomaly Detection in Server Logs,anomaly detection and classification in server logs using deep learning
"  Modern compression algorithms are often the result of laborious
domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took
years to develop and were largely hand-designed. We present a deep neural
network model which optimizes all the steps of a wideband speech coding
pipeline (compression, quantization, entropy coding, and decompression)
end-to-end directly from raw speech data -- no manual feature engineering
necessary, and it trains in hours. In testing, our DNN-based coder performs on
par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps).
It also runs in realtime on a 3.8GhZ Intel CPU.
",Deep Learning for Speech Compression,deep learning for audio compression
"  We recorded high-density EEG in a flanker task experiment (31 subjects) and
an online BCI control paradigm (4 subjects). On these datasets, we evaluated
the use of transfer learning for error decoding with deep convolutional neural
networks (deep ConvNets). In comparison with a regularized linear discriminant
analysis (rLDA) classifier, ConvNets were significantly better in both intra-
and inter-subject decoding, achieving an average accuracy of 84.1 % within
subject and 81.7 % on unknown subjects (flanker task). Neither method was,
however, able to generalize reliably between paradigms. Visualization of
features the ConvNets learned from the data showed plausible patterns of brain
activity, revealing both similarities and differences between the different
kinds of errors. Our findings indicate that deep learning techniques are useful
to infer information about the correctness of action in BCI applications,
particularly for the transfer of pre-trained classifiers to new recording
sessions or subjects.
",Deep Learning in Brain-Computer Interface,neural network applications in brain-computer interfaces
"  We investigate anomaly detection in an unsupervised framework and introduce
Long Short Term Memory (LSTM) neural network based algorithms. In particular,
given variable length data sequences, we first pass these sequences through our
LSTM based structure and obtain fixed length sequences. We then find a decision
function for our anomaly detectors based on the One Class Support Vector
Machines (OC-SVM) and Support Vector Data Description (SVDD) algorithms. As the
first time in the literature, we jointly train and optimize the parameters of
the LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective
gradient and quadratic programming based training methods. To apply the
gradient based training method, we modify the original objective criteria of
the OC-SVM and SVDD algorithms, where we prove the convergence of the modified
objective criteria to the original criteria. We also provide extensions of our
unsupervised formulation to the semi-supervised and fully supervised
frameworks. Thus, we obtain anomaly detection algorithms that can process
variable length data sequences while providing high performance, especially for
time series data. Our approach is generic so that we also apply this approach
to the Gated Recurrent Unit (GRU) architecture by directly replacing our LSTM
based structure with the GRU based structure. In our experiments, we illustrate
significant performance gains achieved by our algorithms with respect to the
conventional methods.
",Anomaly Detection using LSTM and SVM,anomaly detection using lstm-based neural networks
"  Building classification models is an intrinsically practical exercise that
requires many design decisions prior to deployment. We aim to provide some
guidance in this decision making process. Specifically, given a classification
problem with real valued attributes, we consider which classifier or family of
classifiers should one use. Strong contenders are tree based homogeneous
ensembles, support vector machines or deep neural networks. All three families
of model could claim to be state-of-the-art, and yet it is not clear when one
is preferable to the others. Our extensive experiments with over 200 data sets
from two distinct archives demonstrate that, rather than choose a single family
and expend computing resources on optimising that model, it is significantly
better to build simpler versions of classifiers from each family and ensemble.
We show that the Heterogeneous Ensembles of Standard Classification Algorithms
(HESCA), which ensembles based on error estimates formed on the train data, is
significantly better (in terms of error, balanced error, negative log
likelihood and area under the ROC curve) than its individual components,
picking the component that is best on train data, and a support vector machine
tuned over 1089 different parameter configurations. We demonstrate HESCA+,
which contains a deep neural network, a support vector machine and two decision
tree forests, is significantly better than its components, picking the best
component, and HESCA. We analyse the results further and find that HESCA and
HESCA+ are of particular value when the train set size is relatively small and
the problem has multiple classes. HESCA is a fast approach that is, on average,
as good as state-of-the-art classifiers, whereas HESCA+ is significantly better
than average and represents a strong benchmark for future research.
",Classifier Selection for Classification Problems,classifier ensembles and selection for classification problems
"  The original problem of supervised classification considers the task of
automatically assigning objects to their respective classes on the basis of
numerical measurements derived from these objects. Classifiers are the tools
that implement the actual functional mapping from these measurements---also
called features or inputs---to the so-called class label---or output. The
fields of pattern recognition and machine learning study ways of constructing
such classifiers. The main idea behind supervised methods is that of learning
from examples: given a number of example input-output relations, to what extent
can the general mapping be learned that takes any new and unseen feature vector
to its correct class? This chapter provides a basic introduction to the
underlying ideas of how to come to a supervised classification problem. In
addition, it provides an overview of some specific classification techniques,
delves into the issues of object representation and classifier evaluation, and
(very) briefly covers some variations on the basic supervised classification
task that may also be of interest to the practitioner.
",Supervised Classification Fundamentals,supervised classification
"  This paper presents first and second order convergence analysis of the
sparsity aware l0-RLS adaptive filter. The theorems 1 and 2 state the steady
state value of mean and mean square deviation of the adaptive filter weight
vector.
",Convergence Analysis of Adaptive Filters,adaptive filtering
"  Deep neural networks (DNNs) have recently achieved great success in many
visual recognition tasks. However, existing deep neural network models are
computationally expensive and memory intensive, hindering their deployment in
devices with low memory resources or in applications with strict latency
requirements. Therefore, a natural thought is to perform model compression and
acceleration in deep networks without significantly decreasing the model
performance. During the past five years, tremendous progress has been made in
this area. In this paper, we review the recent techniques for compacting and
accelerating DNN models. In general, these techniques are divided into four
categories: parameter pruning and quantization, low-rank factorization,
transferred/compact convolutional filters, and knowledge distillation. Methods
of parameter pruning and quantization are described first, after that the other
techniques are introduced. For each category, we also provide insightful
analysis about the performance, related applications, advantages, and
drawbacks. Then we go through some very recent successful methods, for example,
dynamic capacity networks and stochastic depths networks. After that, we survey
the evaluation matrices, the main datasets used for evaluating the model
performance, and recent benchmark efforts. Finally, we conclude this paper,
discuss remaining the challenges and possible directions for future work.
",Model Compression in Deep Neural Networks,deep neural network compression and acceleration techniques
"  Mass segmentation provides effective morphological features which are
important for mass diagnosis. In this work, we propose a novel end-to-end
network for mammographic mass segmentation which employs a fully convolutional
network (FCN) to model a potential function, followed by a CRF to perform
structured learning. Because the mass distribution varies greatly with pixel
position, the FCN is combined with a position priori. Further, we employ
adversarial training to eliminate over-fitting due to the small sizes of
mammogram datasets. Multi-scale FCN is employed to improve the segmentation
performance. Experimental results on two public datasets, INbreast and
DDSM-BCRP, demonstrate that our end-to-end network achieves better performance
than state-of-the-art approaches.
\footnote{https://github.com/wentaozhu/adversarial-deep-structural-networks.git}
",Mammographic Mass Segmentation,medical imaging - mammographic mass segmentation
"  Data and knowledge representation are fundamental concepts in machine
learning. The quality of the representation impacts the performance of the
learning model directly. Feature learning transforms or enhances raw data to
structures that are effectively exploited by those models. In recent years,
several works have been using complex networks for data representation and
analysis. However, no feature learning method has been proposed for such
category of techniques. Here, we present an unsupervised feature learning
mechanism that works on datasets with binary features. First, the dataset is
mapped into a feature--sample network. Then, a multi-objective optimization
process selects a set of new vertices to produce an enhanced version of the
network. The new features depend on a nonlinear function of a combination of
preexisting features. Effectively, the process projects the input data into a
higher-dimensional space. To solve the optimization problem, we design two
metaheuristics based on the lexicographic genetic algorithm and the improved
strength Pareto evolutionary algorithm (SPEA2). We show that the enhanced
network contains more information and can be exploited to improve the
performance of machine learning methods. The advantages and disadvantages of
each optimization strategy are discussed.
",Unsupervised Feature Learning on Complex Networks,unsupervised feature learning for binary data
"  Deep Neural Networks (DNNs) are universal function approximators providing
state-of- the-art solutions on wide range of applications. Common perceptual
tasks such as speech recognition, image classification, and object tracking are
now commonly tackled via DNNs. Some fundamental problems remain: (1) the lack
of a mathematical framework providing an explicit and interpretable
input-output formula for any topology, (2) quantification of DNNs stability
regarding adversarial examples (i.e. modified inputs fooling DNN predictions
whilst undetectable to humans), (3) absence of generalization guarantees and
controllable behaviors for ambiguous patterns, (4) leverage unlabeled data to
apply DNNs to domains where expert labeling is scarce as in the medical field.
Answering those points would provide theoretical perspectives for further
developments based on a common ground. Furthermore, DNNs are now deployed in
tremendous societal applications, pushing the need to fill this theoretical gap
to ensure control, reliability, and interpretability.
",Theoretical Foundations of Deep Neural Networks,challenges and limitations of deep neural networks
"  Mobile robots, be it autonomous or teleoperated, require stable communication
with the base station to exchange valuable information. Given the stochastic
elements in radio signal propagation, such as shadowing and fading, and the
possibilities of unpredictable events or hardware failures, communication loss
often presents a significant mission risk, both in terms of probability and
impact, especially in Urban Search and Rescue (USAR) operations. Depending on
the circumstances, disconnected robots are either abandoned or attempt to
autonomously back-trace their way to the base station. Although recent results
in Communication-Aware Motion Planning can be used to effectively manage
connectivity with robots, there are no results focusing on autonomously
re-establishing the wireless connectivity of a mobile robot without
back-tracking or using detailed a priori information of the network.
  In this paper, we present a robust and online radio signal mapping method
using Gaussian Random Fields and propose a Resilient Communication-Aware Motion
Planner (RCAMP) that integrates the above signal mapping framework with a
motion planner. RCAMP considers both the environment and the physical
constraints of the robot, based on the available sensory information. We also
propose a self-repair strategy using RCMAP, that takes both connectivity and
the goal position into account when driving to a connection-safe position in
the event of a communication loss. We demonstrate the proposed planner in a set
of realistic simulations of an exploration task in single or multi-channel
communication scenarios.
",Robot Communication Recovery in USAR Operations,robust communication-aware motion planning for mobile robots
"  This extended paper presents 1) a novel hierarchy and recursion extension to
the process tree model; and 2) the first, recursion aware process model
discovery technique that leverages hierarchical information in event logs,
typically available for software systems. This technique allows us to analyze
the operational processes of software systems under real-life conditions at
multiple levels of granularity. The work can be positioned in-between reverse
engineering and process mining. An implementation of the proposed approach is
available as a ProM plugin. Experimental results based on real-life (software)
event logs demonstrate the feasibility and usefulness of the approach and show
the huge potential to speed up discovery by exploiting the available hierarchy.
",Process Model Discovery in Software Systems,process mining
"  The success of semi-supervised manifold learning is highly dependent on the
quality of the labeled samples. Active manifold learning aims to select and
label representative landmarks on a manifold from a given set of samples to
improve semi-supervised manifold learning. In this paper, we propose a novel
active manifold learning method based on a unified framework of manifold
landmarking. In particular, our method combines geometric manifold landmarking
methods with algebraic ones. We achieve this by using the Gershgorin circle
theorem to construct an upper bound on the learning error that depends on the
landmarks and the manifold's alignment matrix in a way that captures both the
geometric and algebraic criteria. We then attempt to select landmarks so as to
minimize this bound by iteratively deleting the Gershgorin circles
corresponding to the selected landmarks. We also analyze the complexity,
scalability, and robustness of our method through simulations, and demonstrate
its superiority compared to existing methods. Experiments in regression and
classification further verify that our method performs better than its
competitors.
",Active Manifold Learning,active manifold learning
"  The Fisher information metric is an important foundation of information
geometry, wherein it allows us to approximate the local geometry of a
probability distribution. Recurrent neural networks such as the
Sequence-to-Sequence (Seq2Seq) networks that have lately been used to yield
state-of-the-art performance on speech translation or image captioning have so
far ignored the geometry of the latent embedding, that they iteratively learn.
We propose the information geometric Seq2Seq (GeoSeq2Seq) network which
abridges the gap between deep recurrent neural networks and information
geometry. Specifically, the latent embedding offered by a recurrent network is
encoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism
common in computer vision. We utilise such a network to predict the shortest
routes between two nodes of a graph by learning the adjacency matrix using the
GeoSeq2Seq formalism; our results show that for such a problem the
probabilistic representation of the latent embedding supersedes the
non-probabilistic embedding by 10-15\%.
",Information Geometry in Neural Networks,information geometry and neural networks
"  Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
",Regularization Techniques for Deep Neural Networks,neural network regularization
"  Principal component analysis (PCA) has well-documented merits for data
extraction and dimensionality reduction. PCA deals with a single dataset at a
time, and it is challenged when it comes to analyzing multiple datasets. Yet in
certain setups, one wishes to extract the most significant information of one
dataset relative to other datasets. Specifically, the interest may be on
identifying, namely extracting features that are specific to a single target
dataset but not the others. This paper develops a novel approach for such
so-termed discriminative data analysis, and establishes its optimality in the
least-squares (LS) sense under suitable data modeling assumptions. The
criterion reveals linear combinations of variables by maximizing the ratio of
the variance of the target data to that of the remainders. The novel approach
solves a generalized eigenvalue problem by performing SVD just once. Numerical
tests using synthetic and real datasets showcase the merits of the proposed
approach relative to its competing alternatives.
",Multivariate Data Analysis,"data analysis, dimensionality reduction"
"  This work provides a simplified proof of the statistical minimax optimality
of (iterate averaged) stochastic gradient descent (SGD), for the special case
of least squares. This result is obtained by analyzing SGD as a stochastic
process and by sharply characterizing the stationary covariance matrix of this
process. The finite rate optimality characterization captures the constant
factors and addresses model mis-specification.
",Statistical Optimality of Stochastic Gradient Descent,stochastic optimization
"  A key attribute that drives the unprecedented success of modern Recurrent
Neural Networks (RNNs) on learning tasks which involve sequential data, is
their ability to model intricate long-term temporal dependencies. However, a
well established measure of RNNs long-term memory capacity is lacking, and thus
formal understanding of the effect of depth on their ability to correlate data
throughout time is limited. Specifically, existing depth efficiency results on
convolutional networks do not suffice in order to account for the success of
deep RNNs on data of varying lengths. In order to address this, we introduce a
measure of the network's ability to support information flow across time,
referred to as the Start-End separation rank, which reflects the distance of
the function realized by the recurrent network from modeling no dependency
between the beginning and end of the input sequence. We prove that deep
recurrent networks support Start-End separation ranks which are combinatorially
higher than those supported by their shallow counterparts. Thus, we establish
that depth brings forth an overwhelming advantage in the ability of recurrent
networks to model long-term dependencies, and provide an exemplar of
quantifying this key attribute which may be readily extended to other RNN
architectures of interest, e.g. variants of LSTM networks. We obtain our
results by considering a class of recurrent networks referred to as Recurrent
Arithmetic Circuits, which merge the hidden state with the input via the
Multiplicative Integration operation, and empirically demonstrate the discussed
phenomena on common RNNs. Finally, we employ the tool of quantum Tensor
Networks to gain additional graphic insight regarding the complexity brought
forth by depth in recurrent networks.
",Measuring Depth Advantage in Recurrent Neural Networks,measuring long-term memory capacity in recurrent neural networks
"  In this work we introduce malware detection from raw byte sequences as a
fruitful research area to the larger machine learning community. Building a
neural network for such a problem presents a number of interesting challenges
that have not occurred in tasks such as image processing or NLP. In particular,
we note that detection from raw bytes presents a sequence problem with over two
million time steps and a problem where batch normalization appear to hinder the
learning process. We present our initial work in building a solution to tackle
this problem, which has linear complexity dependence on the sequence length,
and allows for interpretable sub-regions of the binary to be identified. In
doing so we will discuss the many challenges in building a neural network to
process data at this scale, and the methods we used to work around them.
",Malware Detection with Neural Networks,malware detection
"  In this paper, we study stochastic non-convex optimization with non-convex
random functions. Recent studies on non-convex optimization revolve around
establishing second-order convergence, i.e., converging to a nearly
second-order optimal stationary points. However, existing results on stochastic
non-convex optimization are limited, especially with a high probability
second-order convergence. We propose a novel updating step (named NCG-S) by
leveraging a stochastic gradient and a noisy negative curvature of a stochastic
Hessian, where the stochastic gradient and Hessian are based on a proper
mini-batch of random functions. Building on this step, we develop two
algorithms and establish their high probability second-order convergence. To
the best of our knowledge, the proposed stochastic algorithms are the first
with a second-order convergence in {\it high probability} and a time complexity
that is {\it almost linear} in the problem's dimensionality.
",Stochastic Non-Convex Optimization,stochastic non-convex optimization
"  Graphs (networks) are ubiquitous and allow us to model entities (nodes) and
the dependencies (edges) between them. Learning a useful feature representation
from graph data lies at the heart and success of many machine learning tasks
such as classification, anomaly detection, link prediction, among many others.
Many existing techniques use random walks as a basis for learning features or
estimating the parameters of a graph model for a downstream prediction task.
Examples include recent node embedding methods such as DeepWalk, node2vec, as
well as graph-based deep learning algorithms. However, the simple random walk
used by these methods is fundamentally tied to the identity of the node. This
has three main disadvantages. First, these approaches are inherently
transductive and do not generalize to unseen nodes and other graphs. Second,
they are not space-efficient as a feature vector is learned for each node which
is impractical for large graphs. Third, most of these approaches lack support
for attributed graphs.
  To make these methods more generally applicable, we propose a framework for
inductive network representation learning based on the notion of attributed
random walk that is not tied to node identity and is instead based on learning
a function $\Phi : \mathrm{\rm \bf x} \rightarrow w$ that maps a node attribute
vector $\mathrm{\rm \bf x}$ to a type $w$. This framework serves as a basis for
generalizing existing methods such as DeepWalk, node2vec, and many other
previous methods that leverage traditional random walks.
",Graph Representation Learning,inductive network representation learning
"  This paper presents a method for constructing human-robot interaction
policies in settings where multimodality, i.e., the possibility of multiple
highly distinct futures, plays a critical role in decision making. We are
motivated in this work by the example of traffic weaving, e.g., at highway
on-ramps/off-ramps, where entering and exiting cars must swap lanes in a short
distance---a challenging negotiation even for experienced drivers due to the
inherent multimodal uncertainty of who will pass whom. Our approach is to learn
multimodal probability distributions over future human actions from a dataset
of human-human exemplars and perform real-time robot policy construction in the
resulting environment model through massively parallel sampling of human
responses to candidate robot action sequences. Direct learning of these
distributions is made possible by recent advances in the theory of conditional
variational autoencoders (CVAEs), whereby we learn action distributions
simultaneously conditioned on the present interaction history, as well as
candidate future robot actions in order to take into account response dynamics.
We demonstrate the efficacy of this approach with a human-in-the-loop
simulation of a traffic weaving scenario.
",Human-Robot Interaction in Multimodal Environments,human-robot interaction
"  Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.
",Interpretable Neural Networks for Image Classification,explainable artificial intelligence
"  The continuous dynamical system approach to deep learning is explored in
order to devise alternative frameworks for training algorithms. Training is
recast as a control problem and this allows us to formulate necessary
optimality conditions in continuous time using the Pontryagin's maximum
principle (PMP). A modification of the method of successive approximations is
then used to solve the PMP, giving rise to an alternative training algorithm
for deep learning. This approach has the advantage that rigorous error
estimates and convergence results can be established. We also show that it may
avoid some pitfalls of gradient-based methods, such as slow convergence on flat
landscapes near saddle points. Furthermore, we demonstrate that it obtains
favorable initial convergence rate per-iteration, provided Hamiltonian
maximization can be efficiently carried out - a step which is still in need of
improvement. Overall, the approach opens up new avenues to attack problems
associated with deep learning, such as trapping in slow manifolds and
inapplicability of gradient-based methods for discrete trainable variables.
",Continuous Optimization in Deep Learning,continuous dynamical systems in deep learning
"  Many real-world data mining applications need varying cost for different
types of classification errors and thus call for cost-sensitive classification
algorithms. Existing algorithms for cost-sensitive classification are
successful in terms of minimizing the cost, but can result in a high error rate
as the trade-off. The high error rate holds back the practical use of those
algorithms. In this paper, we propose a novel cost-sensitive classification
methodology that takes both the cost and the error rate into account. The
methodology, called soft cost-sensitive classification, is established from a
multicriteria optimization problem of the cost and the error rate, and can be
viewed as regularizing cost-sensitive classification with the error rate. The
simple methodology allows immediate improvements of existing cost-sensitive
classification algorithms. Experiments on the benchmark and the real-world data
sets show that our proposed methodology indeed achieves lower test error rates
and similar (sometimes lower) test costs than existing cost-sensitive
classification algorithms. We also demonstrate that the methodology can be
extended for considering the weighted error rate instead of the original error
rate. This extension is useful for tackling unbalanced classification problems.
",Cost-Sensitive Classification Algorithms,cost-sensitive classification
"  The concepts of unitary evolution matrices and associative memory have
boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art
performance in a variety of sequential tasks. However, RNN still have a limited
capacity to manipulate long-term memory. To bypass this weakness the most
successful applications of RNN use external techniques such as attention
mechanisms. In this paper we propose a novel RNN model that unifies the
state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM
is its rotational operation, which is, naturally, a unitary matrix, providing
architectures with the power to learn long-term dependencies by overcoming the
vanishing and exploding gradients problem. Moreover, the rotational unit also
serves as associative memory. We evaluate our model on synthetic memorization,
question answering and language modeling tasks. RUM learns the Copying Memory
task completely and improves the state-of-the-art result in the Recall task.
RUM's performance in the bAbI Question Answering task is comparable to that of
models with attention mechanism. We also improve the state-of-the-art result to
1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB)
task, which is to signify the applications of RUM to real-world sequential
data. The universality of our construction, at the core of RNN, establishes RUM
as a promising approach to language modeling, speech recognition and machine
translation.
",Rotational Unit of Memory (RUM) in RNNs,recurrent neural networks
"  Preserving the utility of published datasets while simultaneously providing
provable privacy guarantees is a well-known challenge. On the one hand,
context-free privacy solutions, such as differential privacy, provide strong
privacy guarantees, but often lead to a significant reduction in utility. On
the other hand, context-aware privacy solutions, such as information theoretic
privacy, achieve an improved privacy-utility tradeoff, but assume that the data
holder has access to dataset statistics. We circumvent these limitations by
introducing a novel context-aware privacy framework called generative
adversarial privacy (GAP). GAP leverages recent advancements in generative
adversarial networks (GANs) to allow the data holder to learn privatization
schemes from the dataset itself. Under GAP, learning the privacy mechanism is
formulated as a constrained minimax game between two players: a privatizer that
sanitizes the dataset in a way that limits the risk of inference attacks on the
individuals' private variables, and an adversary that tries to infer the
private variables from the sanitized dataset. To evaluate GAP's performance, we
investigate two simple (yet canonical) statistical dataset models: (a) the
binary data model, and (b) the binary Gaussian mixture model. For both models,
we derive game-theoretically optimal minimax privacy mechanisms, and show that
the privacy mechanisms learned from data (in a generative adversarial fashion)
match the theoretically optimal ones. This demonstrates that our framework can
be easily applied in practice, even in the absence of dataset statistics.
",Differential Privacy in Data Publishing,generative adversarial privacy for preserving utility and provable privacy guarantees
"  We describe an approach to understand the peculiar and counterintuitive
generalization properties of deep neural networks. The approach involves going
beyond worst-case theoretical capacity control frameworks that have been
popular in machine learning in recent years to revisit old ideas in the
statistical mechanics of neural networks. Within this approach, we present a
prototypical Very Simple Deep Learning (VSDL) model, whose behavior is
controlled by two control parameters, one describing an effective amount of
data, or load, on the network (that decreases when noise is added to the
input), and one with an effective temperature interpretation (that increases
when algorithms are early stopped). Using this model, we describe how a very
simple application of ideas from the statistical mechanics theory of
generalization provides a strong qualitative description of recently-observed
empirical results regarding the inability of deep neural networks not to
overfit training data, discontinuous learning and sharp transitions in the
generalization properties of learning algorithms, etc.
",Statistical Mechanics of Deep Neural Networks,generalization properties of deep neural networks
"  We present an algorithm for classification tasks on big data. Experiments
conducted as part of this study indicate that the algorithm can be as accurate
as ensemble methods such as random forests or gradient boosted trees. Unlike
ensemble methods, the models produced by the algorithm can be easily
interpreted. The algorithm is based on a divide and conquer strategy and
consists of two steps. The first step consists of using a decision tree to
segment the large dataset. By construction, decision trees attempt to create
homogeneous class distributions in their leaf nodes. However, non-homogeneous
leaf nodes are usually produced. The second step of the algorithm consists of
using a suitable classifier to determine the class labels for the
non-homogeneous leaf nodes. The decision tree segment provides a coarse segment
profile while the leaf level classifier can provide information about the
attributes that affect the label within a segment.
",Interpretable Classification Algorithms for Big Data,machine learning
"  In this study, we propose a novel deep neural network and its supervised
learning method that uses a feedforward supervisory signal. The method is
inspired by the human visual system and performs human-like association-based
learning without any backward error propagation. The feedforward supervisory
signal that produces the correct result is preceded by the target signal and
associates its confirmed label with the classification result of the target
signal. It effectively uses a large amount of information from the feedforward
signal, and forms a continuous and rich learning representation. The method is
validated using visual recognition tasks on the MNIST handwritten dataset.
",Deep Learning for Visual Recognition,neural networks
"  Graph embedding methods represent nodes in a continuous vector space,
preserving information from the graph (e.g. by sampling random walks). There
are many hyper-parameters to these methods (such as random walk length) which
have to be manually tuned for every graph. In this paper, we replace random
walk hyper-parameters with trainable parameters that we automatically learn via
backpropagation. In particular, we learn a novel attention model on the power
series of the transition matrix, which guides the random walk to optimize an
upstream objective. Unlike previous approaches to attention models, the method
that we propose utilizes attention parameters exclusively on the data (e.g. on
the random walk), and not used by the model for inference. We experiment on
link prediction tasks, as we aim to produce embeddings that best-preserve the
graph structure, generalizing to unseen information. We improve
state-of-the-art on a comprehensive suite of real world datasets including
social, collaboration, and biological networks. Adding attention to random
walks can reduce the error by 20% to 45% on datasets we attempted. Further, our
learned attention parameters are different for every graph, and our
automatically-found values agree with the optimal choice of hyper-parameter if
we manually tune existing methods.
",Graph Attention in Graph Embedding,graph embeddings with attention
"  This work addresses the problem of segmentation in time series data with
respect to a statistical parameter of interest in Bayesian models. It is common
to assume that the parameters are distinct within each segment. As such, many
Bayesian change point detection models do not exploit the segment parameter
patterns, which can improve performance. This work proposes a Bayesian
mean-shift change point detection algorithm that makes use of repetition in
segment parameters, by introducing segment class labels that utilise a
Dirichlet process prior. The performance of the proposed approach was assessed
on both synthetic and real world data, highlighting the enhanced performance
when using parameter labelling.
",Bayesian Change Point Detection,time series change point detection
"  In this paper, we present an initial attempt to learn evolution PDEs from
data. Inspired by the latest development of neural network designs in deep
learning, we propose a new feed-forward deep network, called PDE-Net, to
fulfill two objectives at the same time: to accurately predict dynamics of
complex systems and to uncover the underlying hidden PDE models. The basic idea
of the proposed PDE-Net is to learn differential operators by learning
convolution kernels (filters), and apply neural networks or other machine
learning methods to approximate the unknown nonlinear responses. Comparing with
existing approaches, which either assume the form of the nonlinear response is
known or fix certain finite difference approximations of differential
operators, our approach has the most flexibility by learning both differential
operators and the nonlinear responses. A special feature of the proposed
PDE-Net is that all filters are properly constrained, which enables us to
easily identify the governing PDE models while still maintaining the expressive
and predictive power of the network. These constrains are carefully designed by
fully exploiting the relation between the orders of differential operators and
the orders of sum rules of filters (an important concept originated from
wavelet theory). We also discuss relations of the PDE-Net with some existing
networks in computer vision such as Network-In-Network (NIN) and Residual
Neural Network (ResNet). Numerical experiments show that the PDE-Net has the
potential to uncover the hidden PDE of the observed dynamics, and predict the
dynamical behavior for a relatively long time, even in a noisy environment.
",Physics-Informed Neural Networks,machine learning for partial differential equations
"  When robots operate in unknown environments small errors in postions can lead
to large variations in the contact forces, especially with typical
high-impedance designs. This can potentially damage the surroundings and/or the
robot. Series elastic actuators (SEAs) are a popular way to reduce the output
impedance of a robotic arm to improve control authority over the force exerted
on the environment. However this increased control over forces with lower
impedance comes at the cost of lower positioning precision and bandwidth. This
article examines the use of an iteratively-learned feedforward command to
improve position tracking when using SEAs. Over each iteration, the output
responses of the system to the quantized inputs are used to estimate a
linearized local system models. These estimated models are obtained using a
complex-valued Gaussian Process Regression (cGPR) technique and then, used to
generate a new feedforward input command based on the previous iteration's
error. This article illustrates this iterative machine learning (IML) technique
for a two degree of freedom (2-DOF) robotic arm, and demonstrates successful
convergence of the IML approach to reduce the tracking error.
",Robot Arm Position Tracking,robotics: series elastic actuators and position tracking
"  In this work we addressed the issue of applying a stochastic classifier and a
local, fuzzy confusion matrix under the framework of multi-label
classification. We proposed a novel solution to the problem of correcting label
pairwise ensembles. The main step of the correction procedure is to compute
classifier-specific competence and cross-competence measures, which estimates
error pattern of the underlying classifier. At the fusion phase we employed two
weighting approaches based on information theory. The classifier weights
promote base classifiers which are the most susceptible to the correction based
on the fuzzy confusion matrix. During the experimental study, the proposed
approach was compared against two reference methods. The comparison was made in
terms of six different quality criteria. The conducted experiments reveals that
the proposed approach eliminates one of main drawbacks of the original
FCM-based approach i.e. the original approach is vulnerable to the imbalanced
class/label distribution. What is more, the obtained results shows that the
introduced method achieves satisfying classification quality under all
considered quality criteria. Additionally, the impact of fluctuations of data
set characteristics is reduced.
",Multi-Label Classification with Fuzzy Confusion Matrix,multi-label classification
"  We examine the problem of learning mappings from state to state, suitable for
use in a model-based reinforcement-learning setting, that simultaneously
generalize to novel states and can capture stochastic transitions. We show that
currently popular generative adversarial networks struggle to learn these
stochastic transition models but a modification to their loss functions results
in a powerful learning algorithm for this class of problems.
",Stochastic Transition Modeling in Reinforcement Learning,reinforcement learning and generative adversarial networks
"  Discriminating lung nodules as malignant or benign is still an underlying
challenge. To address this challenge, radiologists need computer aided
diagnosis (CAD) systems which can assist in learning discriminative imaging
features corresponding to malignant and benign nodules. However, learning
highly discriminative imaging features is an open problem. In this paper, our
aim is to learn the most discriminative features pertaining to lung nodules by
using an adversarial learning methodology. Specifically, we propose to use
unsupervised learning with Deep Convolutional-Generative Adversarial Networks
(DC-GANs) to generate lung nodule samples realistically. We hypothesize that
imaging features of lung nodules will be discriminative if it is hard to
differentiate them (fake) from real (true) nodules. To test this hypothesis, we
present Visual Turing tests to two radiologists in order to evaluate the
quality of the generated (fake) nodules. Extensive comparisons are performed in
discerning real, generated, benign, and malignant nodules. This experimental
set up allows us to validate the overall quality of the generated nodules,
which can then be used to (1) improve diagnostic decisions by mining highly
discriminative imaging features, (2) train radiologists for educational
purposes, and (3) generate realistic samples to train deep networks with big
data.
",Computer-Aided Diagnosis of Lung Nodules,computer-aided diagnosis for lung nodules
"  We develop a metalearning approach for learning hierarchically structured
policies, improving sample efficiency on unseen tasks through the use of shared
primitives---policies that are executed for large numbers of timesteps.
Specifically, a set of primitives are shared within a distribution of tasks,
and are switched between by task-specific policies. We provide a concrete
metric for measuring the strength of such hierarchies, leading to an
optimization problem for quickly reaching high reward on unseen tasks. We then
present an algorithm to solve this problem end-to-end through the use of any
off-the-shelf reinforcement learning method, by repeatedly sampling new tasks
and resetting task-specific policies. We successfully discover meaningful motor
primitives for the directional movement of four-legged robots, solely by
interacting with distributions of mazes. We also demonstrate the
transferability of primitives to solve long-timescale sparse-reward obstacle
courses, and we enable 3D humanoid robots to robustly walk and crawl with the
same policy.
",Meta-Learning for Hierarchical Policies in Robotics,hierarchical reinforcement learning
"  Pancreatic cancer has the poorest prognosis among all cancer types.
Intraductal Papillary Mucinous Neoplasms (IPMNs) are radiographically
identifiable precursors to pancreatic cancer; hence, early detection and
precise risk assessment of IPMN are vital. In this work, we propose a
Convolutional Neural Network (CNN) based computer aided diagnosis (CAD) system
to perform IPMN diagnosis and risk assessment by utilizing multi-modal MRI. In
our proposed approach, we use minimum and maximum intensity projections to ease
the annotation variations among different slices and type of MRIs. Then, we
present a CNN to obtain deep feature representation corresponding to each MRI
modality (T1-weighted and T2-weighted). At the final step, we employ canonical
correlation analysis (CCA) to perform a fusion operation at the feature level,
leading to discriminative canonical correlation features. Extracted features
are used for classification. Our results indicate significant improvements over
other potential approaches to solve this important problem. The proposed
approach doesn't require explicit sample balancing in cases of imbalance
between positive and negative examples. To the best of our knowledge, our study
is the first to automatically diagnose IPMN using multi-modal MRI.
",IPMN Diagnosis using Deep Learning,computer-aided diagnosis of pancreatic cancer using multi-modal mri
"  A low rank matrix X has been contaminated by uniformly distributed noise,
missing values, outliers and corrupt entries. Reconstruction of X from the
singular values and singular vectors of the contaminated matrix Y is a key
problem in machine learning, computer vision and data science. In this paper we
show that common contamination models (including arbitrary combinations of
uniform noise,missing values, outliers and corrupt entries) can be described
efficiently using a single framework. We develop an asymptotically optimal
algorithm that estimates X by manipulation of the singular values of Y , which
applies to any of the contamination models considered. Finally, we find an
explicit signal-to-noise cutoff, below which estimation of X from the singular
value decomposition of Y must fail, in a well-defined sense.
",Matrix Reconstruction from Noisy Data,matrix reconstruction from noisy data
"  Although the word-popularity based negative sampler has shown superb
performance in the skip-gram model, the theoretical motivation behind
oversampling popular (non-observed) words as negative samples is still not well
understood. In this paper, we start from an investigation of the gradient
vanishing issue in the skipgram model without a proper negative sampler. By
performing an insightful analysis from the stochastic gradient descent (SGD)
learning perspective, we demonstrate that, both theoretically and intuitively,
negative samples with larger inner product scores are more informative than
those with lower scores for the SGD learner in terms of both convergence rate
and accuracy. Understanding this, we propose an alternative sampling algorithm
that dynamically selects informative negative samples during each SGD update.
More importantly, the proposed sampler accounts for multi-dimensional
self-embedded features during the sampling process, which essentially makes it
more effective than the original popularity-based (one-dimensional) sampler.
Empirical experiments further verify our observations, and show that our
fine-grained samplers gain significant improvement over the existing ones
without increasing computational complexity.
",Negative Sampling in Skip-Gram Models,efficient negative sampling in skip-gram models
"  This paper focusses on ""safe"" screening techniques for the LASSO problem.
Motivated by the need for low-complexity algorithms, we propose a new approach,
dubbed ""joint"" screening test, allowing to screen a set of atoms by carrying
out one single test. The approach is particularized to two different sets of
atoms, respectively expressed as sphere and dome regions. After presenting the
mathematical derivations of the tests, we elaborate on their relative
effectiveness and discuss the practical use of such procedures.
",LASSO Screening Techniques,statistical methodology
"  The predictive power and overall computational efficiency of
Diffusion-convolutional neural networks make them an attractive choice for node
classification tasks. However, a naive dense-tensor-based implementation of
DCNNs leads to $\mathcal{O}(N^2)$ memory complexity which is prohibitive for
large graphs. In this paper, we introduce a simple method for thresholding
input graphs that provably reduces memory requirements of DCNNs to O(N) (i.e.
linear in the number of nodes in the input) without significantly affecting
predictive performance.
",Efficient Implementation of Diffusion-Convolutional Neural Networks,efficient node classification in large graphs using diffusion-convolutional neural networks
"  Stochasticity and limited precision of synaptic weights in neural network
models are key aspects of both biological and hardware modeling of learning
processes. Here we show that a neural network model with stochastic binary
weights naturally gives prominence to exponentially rare dense regions of
solutions with a number of desirable properties such as robustness and good
generalization performance, while typical solutions are isolated and hard to
find. Binary solutions of the standard perceptron problem are obtained from a
simple gradient descent procedure on a set of real values parametrizing a
probability distribution over the binary synapses. Both analytical and
numerical results are presented. An algorithmic extension aimed at training
discrete deep neural networks is also investigated.
",Stochastic Neural Network Models,stochastic neural networks
"  Modern large scale machine learning applications require stochastic
optimization algorithms to be implemented on distributed computational
architectures. A key bottleneck is the communication overhead for exchanging
information such as stochastic gradients among different workers. In this
paper, to reduce the communication cost we propose a convex optimization
formulation to minimize the coding length of stochastic gradients. To solve the
optimal sparsification efficiently, several simple and fast algorithms are
proposed for approximate solution, with theoretical guaranteed for sparseness.
Experiments on $\ell_2$ regularized logistic regression, support vector
machines, and convolutional neural networks validate our sparsification
approaches.
",Distributed Optimization in Machine Learning,distributed machine learning optimization
"  Energy statistics was proposed by Sz\' ekely in the 80's inspired by Newton's
gravitational potential in classical mechanics and it provides a model-free
hypothesis test for equality of distributions. In its original form, energy
statistics was formulated in Euclidean spaces. More recently, it was
generalized to metric spaces of negative type. In this paper, we consider a
formulation for the clustering problem using a weighted version of energy
statistics in spaces of negative type. We show that this approach leads to a
quadratically constrained quadratic program in the associated kernel space,
establishing connections with graph partitioning problems and kernel methods in
machine learning. To find local solutions of such an optimization problem, we
propose kernel k-groups, which is an extension of Hartigan's method to kernel
spaces. Kernel k-groups is cheaper than spectral clustering and has the same
computational cost as kernel k-means (which is based on Lloyd's heuristic) but
our numerical results show an improved performance, especially in higher
dimensions. Moreover, we verify the efficiency of kernel k-groups in community
detection in sparse stochastic block models which has fascinating applications
in several areas of science.
",Energy Statistics in Machine Learning,clustering and community detection using energy statistics in metric spaces
"  We show that the error probability of reconstructing kernel matrices from
Random Fourier Features for the Gaussian kernel function is at most
$\mathcal{O}(R^{2/3} \exp(-D))$, where $D$ is the number of random features and
$R$ is the diameter of the data domain. We also provide an
information-theoretic method-independent lower bound of $\Omega((1-\exp(-R^2))
\exp(-D))$. Compared to prior work, we are the first to show that the error
probability for random Fourier features is independent of the dimensionality of
data points. As applications of our theory, we obtain dimension-independent
bounds for kernel ridge regression and support vector machines.
",Random Fourier Features Error Bounds,kernel matrix reconstruction and error bounds
"  We introduce the ""inverse square root linear unit"" (ISRLU) to speed up
learning in deep neural networks. ISRLU has better performance than ELU but has
many of the same benefits. ISRLU and ELU have similar curves and
characteristics. Both have negative values, allowing them to push mean unit
activation closer to zero, and bring the normal gradient closer to the unit
natural gradient, ensuring a noise-robust deactivation state, lessening the
over fitting risk. The significant performance advantage of ISRLU on
traditional CPUs also carry over to more efficient HW implementations on HW/SW
codesign for CNNs/RNNs. In experiments with TensorFlow, ISRLU leads to faster
learning and better generalization than ReLU on CNNs. This work also suggests a
computationally efficient variant called the ""inverse square root unit"" (ISRU)
which can be used for RNNs. Many RNNs use either long short-term memory (LSTM)
and gated recurrent units (GRU) which are implemented with tanh and sigmoid
activation functions. ISRU has less com- putational complexity but still has a
similar curve to tanh and sigmoid.
",Deep Learning Activation Functions,deep learning activations
"  Conjugate gradient (CG) methods are a class of important methods for solving
linear equations and nonlinear optimization problems. In this paper, we propose
a new stochastic CG algorithm with variance reduction and we prove its linear
convergence with the Fletcher and Reeves method for strongly convex and smooth
functions. We experimentally demonstrate that the CG with variance reduction
algorithm converges faster than its counterparts for four learning models,
which may be convex, nonconvex or nonsmooth. In addition, its area under the
curve performance on six large-scale data sets is comparable to that of the
LIBLINEAR solver for the L2-regularized L2-loss but with a significant
improvement in computational efficiency
",Stochastic Conjugate Gradient Optimization,stochastic conjugate gradient methods
"  In this paper we provide faster algorithms for approximately solving
discounted Markov Decision Processes in multiple parameter regimes. Given a
discounted Markov Decision Process (DMDP) with $|S|$ states, $|A|$ actions,
discount factor $\gamma\in(0,1)$, and rewards in the range $[-M, M]$, we show
how to compute an $\epsilon$-optimal policy, with probability $1 - \delta$ in
time \[ \tilde{O}\left( \left(|S|^2 |A| + \frac{|S| |A|}{(1 - \gamma)^3}
\right)
  \log\left( \frac{M}{\epsilon} \right) \log\left( \frac{1}{\delta} \right)
\right) ~ . \] This contribution reflects the first nearly linear time, nearly
linearly convergent algorithm for solving DMDPs for intermediate values of
$\gamma$.
  We also show how to obtain improved sublinear time algorithms provided we can
sample from the transition function in $O(1)$ time. Under this assumption we
provide an algorithm which computes an $\epsilon$-optimal policy with
probability $1 - \delta$ in time \[ \tilde{O} \left(\frac{|S| |A| M^2}{(1 -
\gamma)^4 \epsilon^2} \log \left(\frac{1}{\delta}\right) \right) ~. \]
  Lastly, we extend both these algorithms to solve finite horizon MDPs. Our
algorithms improve upon the previous best for approximately computing optimal
policies for fixed-horizon MDPs in multiple parameter regimes.
  Interestingly, we obtain our results by a careful modification of approximate
value iteration. We show how to combine classic approximate value iteration
analysis with new techniques in variance reduction. Our fastest algorithms
leverage further insights to ensure that our algorithms make monotonic progress
towards the optimal value. This paper is one of few instances in using sampling
to obtain a linearly convergent linear programming algorithm and we hope that
the analysis may be useful more broadly.
",Discounted Markov Decision Processes,algorithms for solving markov decision processes
"  We consider online linear optimization over symmetric positive semi-definite
matrices, which has various applications including the online collaborative
filtering. The problem is formulated as a repeated game between the algorithm
and the adversary, where in each round t the algorithm and the adversary choose
matrices X_t and L_t, respectively, and then the algorithm suffers a loss given
by the Frobenius inner product of X_t and L_t. The goal of the algorithm is to
minimize the cumulative loss. We can employ a standard framework called Follow
the Regularized Leader (FTRL) for designing algorithms, where we need to choose
an appropriate regularization function to obtain a good performance guarantee.
We show that the log-determinant regularization works better than other popular
regularization functions in the case where the loss matrices L_t are all
sparse. Using this property, we show that our algorithm achieves an optimal
performance guarantee for the online collaborative filtering. The technical
contribution of the paper is to develop a new technique of deriving performance
bounds by exploiting the property of strong convexity of the log-determinant
with respect to the loss matrices, while in the previous analysis the strong
convexity is defined with respect to a norm. Intuitively, skipping the norm
analysis results in the improved bound. Moreover, we apply our method to online
linear optimization over vectors and show that the FTRL with the Burg entropy
regularizer, which is the analogue of the log-determinant regularizer in the
vector case, works well.
",Online Linear Optimization over Symmetric Matrices,online linear optimization over symmetric positive semi-definite matrices
"  In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an
increasing demand to reconstruct high quality images from limited number of
data. However, the existing solutions require either hardware changes or
computationally expansive algorithms. To overcome these limitations, here we
propose a novel deep learning approach that interpolates the missing RF data by
utilizing the sparsity of the RF data in the Fourier domain. Extensive
experimental results from sub-sampled RF data from a real US system confirmed
that the proposed method can effectively reduce the data rate without
sacrificing the image quality.
",Ultrasound Image Reconstruction,image reconstruction in ultrasound imaging
"  The goal of regression and classification methods in supervised learning is
to minimize the empirical risk, that is, the expectation of some loss function
quantifying the prediction error under the empirical distribution. When facing
scarce training data, overfitting is typically mitigated by adding
regularization terms to the objective that penalize hypothesis complexity. In
this paper we introduce new regularization techniques using ideas from
distributionally robust optimization, and we give new probabilistic
interpretations to existing techniques. Specifically, we propose to minimize
the worst-case expected loss, where the worst case is taken over the ball of
all (continuous or discrete) distributions that have a bounded transportation
distance from the (discrete) empirical distribution. By choosing the radius of
this ball judiciously, we can guarantee that the worst-case expected loss
provides an upper confidence bound on the loss on test data, thus offering new
generalization bounds. We prove that the resulting regularized learning
problems are tractable and can be tractably kernelized for many popular loss
functions. We validate our theoretical out-of-sample guarantees through
simulated and empirical experiments.
",Distributionally Robust Regularization Techniques,regularization techniques in supervised learning
"  Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by
incorporating deep neural networks in learning representations from the input
to RL. However, the conventional deep neural network architecture is limited in
learning representations for multi-task RL (MT-RL), as multiple tasks can refer
to different kinds of representations. In this paper, we thus propose a novel
deep neural network architecture, namely generalization tower network (GTN),
which can achieve MT-RL within a single learned model. Specifically, the
architecture of GTN is composed of both horizontal and vertical streams. In our
GTN architecture, horizontal streams are used to learn representation shared in
similar tasks. In contrast, the vertical streams are introduced to be more
suitable for handling diverse tasks, which encodes hierarchical shared
knowledge of these tasks. The effectiveness of the introduced vertical stream
is validated by experimental results. Experimental results further verify that
our GTN architecture is able to advance the state-of-the-art MT-RL, via being
tested on 51 Atari games.
",Multi-Task Reinforcement Learning Architectures,reinforcement learning
"  In reinforcement learning an agent interacts with the environment by taking
actions and observing the next state and reward. When sampled
probabilistically, these state transitions, rewards, and actions can all induce
randomness in the observed long-term return. Traditionally, reinforcement
learning algorithms average over this randomness to estimate the value
function. In this paper, we build on recent work advocating a distributional
approach to reinforcement learning in which the distribution over returns is
modeled explicitly instead of only estimating the mean. That is, we examine
methods of learning the value distribution instead of the value function. We
give results that close a number of gaps between the theoretical and
algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we
extend existing results to the approximate distribution setting. Second, we
present a novel distributional reinforcement learning algorithm consistent with
our theoretical formulation. Finally, we evaluate this new algorithm on the
Atari 2600 games, observing that it significantly outperforms many of the
recent improvements on DQN, including the related distributional algorithm C51.
",Distributional Reinforcement Learning,reinforcement learning
"  This paper proposes a deep neural network for estimating the directions of
arrival (DOA) of multiple sound sources. The proposed stacked convolutional and
recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS)
along with the DOA estimates in both azimuth and elevation. We avoid any
explicit feature extraction step by using the magnitudes and phases of the
spectrograms of all the channels as input to the network. The proposed DOAnet
is evaluated by estimating the DOAs of multiple concurrently present sources in
anechoic, matched and unmatched reverberant conditions. The results show that
the proposed DOAnet is capable of estimating the number of sources and their
respective DOAs with good precision and generate SPS with high signal-to-noise
ratio.
",Sound Source Localization,direction of arrival (doa) estimation for multiple sound sources
"  In this work, we tackle the problem of transform-invariant unsupervised
learning in the space of Covariance matrices and applications thereof. We begin
by introducing the Spectral Polytope Covariance Matrix (SPCM) Similarity
function; a similarity function for Covariance matrices, invariant to any type
of transformation. We then derive the SPCM-CRP mixture model, a
transform-invariant non-parametric clustering approach for Covariance matrices
that leverages the proposed similarity function, spectral embedding and the
distance-dependent Chinese Restaurant Process (dd-CRP) (Blei and Frazier,
2011). The scalability and applicability of these two contributions is
extensively validated on real-world Covariance matrix datasets from diverse
research fields. Finally, we couple the SPCM-CRP mixture model with the
Bayesian non-parametric Indian Buffet Process (IBP) - Hidden Markov Model (HMM)
(Fox et al., 2009), to jointly segment and discover transform-invariant action
primitives from complex sequential data. Resulting in a topic-modeling inspired
hierarchical model for unsupervised time-series data analysis which we call
ICSC-HMM (IBP Coupled SPCM-CRP Hidden Markov Model). The ICSC-HMM is validated
on kinesthetic demonstrations of uni-manual and bi-manual cooking tasks;
achieving unsupervised human-level decomposition of complex sequential tasks.
",Unsupervised Learning on Covariance Matrices,transform-invariant unsupervised learning for covariance matrices
"  We consider the problem of performing inverse reinforcement learning when the
trajectory of the expert is not perfectly observed by the learner. Instead, a
noisy continuous-time observation of the trajectory is provided to the learner.
This problem exhibits wide-ranging applications and the specific application we
consider here is the scenario in which the learner seeks to penetrate a
perimeter patrolled by a robot. The learner's field of view is limited due to
which it cannot observe the patroller's complete trajectory. Instead, we allow
the learner to listen to the expert's movement sound, which it can also use to
estimate the expert's state and action using an observation model. We treat the
expert's state and action as hidden data and present an algorithm based on
expectation maximization and maximum entropy principle to solve the non-linear,
non-convex problem. Related work considers discrete-time observations and an
observation model that does not include actions. In contrast, our technique
takes expectations over both state and action of the expert, enabling learning
even in the presence of extreme noise and broader applications.
",Noisy Inverse Reinforcement Learning,reinforcement learning with noisy observations
"  In our work, we bridge deep neural network design with numerical differential
equations. We show that many effective networks, such as ResNet, PolyNet,
FractalNet and RevNet, can be interpreted as different numerical
discretizations of differential equations. This finding brings us a brand new
perspective on the design of effective deep architectures. We can take
advantage of the rich knowledge in numerical analysis to guide us in designing
new and potentially more effective deep networks. As an example, we propose a
linear multi-step architecture (LM-architecture) which is inspired by the
linear multi-step method solving ordinary differential equations. The
LM-architecture is an effective structure that can be used on any ResNet-like
networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the
networks obtained by applying the LM-architecture on ResNet and ResNeXt
respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on
both CIFAR and ImageNet with comparable numbers of trainable parameters. In
particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly
compress ($>50$\%) the original networks while maintaining a similar
performance. This can be explained mathematically using the concept of modified
equation from numerical analysis. Last but not least, we also establish a
connection between stochastic control and noise injection in the training
process which helps to improve generalization of the networks. Furthermore, by
relating stochastic training strategy with stochastic dynamic system, we can
easily apply stochastic training to the networks with the LM-architecture. As
an example, we introduced stochastic depth to LM-ResNet and achieve significant
improvement over the original LM-ResNet on CIFAR10.
",Neural Networks Inspired by Numerical Differential Equations,deep neural network and numerical differential equations
"  Neural networks exhibit good generalization behavior in the
over-parameterized regime, where the number of network parameters exceeds the
number of observations. Nonetheless, current generalization bounds for neural
networks fail to explain this phenomenon. In an attempt to bridge this gap, we
study the problem of learning a two-layer over-parameterized neural network,
when the data is generated by a linearly separable function. In the case where
the network has Leaky ReLU activations, we provide both optimization and
generalization guarantees for over-parameterized networks. Specifically, we
prove convergence rates of SGD to a global minimum and provide generalization
guarantees for this global minimum that are independent of the network size.
Therefore, our result clearly shows that the use of SGD for optimization both
finds a global minimum, and avoids overfitting despite the high capacity of the
model. This is the first theoretical demonstration that SGD can avoid
overfitting, when learning over-specified neural network classifiers.
",Over-parameterized Neural Networks,neural network generalization and optimization
"  We describe a new training methodology for generative adversarial networks.
The key idea is to grow both the generator and discriminator progressively:
starting from a low resolution, we add new layers that model increasingly fine
details as training progresses. This both speeds the training up and greatly
stabilizes it, allowing us to produce images of unprecedented quality, e.g.,
CelebA images at 1024^2. We also propose a simple way to increase the variation
in generated images, and achieve a record inception score of 8.80 in
unsupervised CIFAR10. Additionally, we describe several implementation details
that are important for discouraging unhealthy competition between the generator
and discriminator. Finally, we suggest a new metric for evaluating GAN results,
both in terms of image quality and variation. As an additional contribution, we
construct a higher-quality version of the CelebA dataset.
",Progressive GAN Training,generative adversarial networks (gans) training methodology
"  Long short-term memory (LSTM) is normally used in recurrent neural network
(RNN) as basic recurrent unit. However,conventional LSTM assumes that the state
at current time step depends on previous time step. This assumption constraints
the time dependency modeling capability. In this study, we propose a new
variation of LSTM, advanced LSTM (A-LSTM), for better temporal context
modeling. We employ A-LSTM in weighted pooling RNN for emotion recognition. The
A-LSTM outperforms the conventional LSTM by 5.5% relatively. The A-LSTM based
weighted pooling RNN can also complement the state-of-the-art emotion
classification framework. This shows the advantage of A-LSTM.
",Advanced LSTM for Emotion Recognition,advancements in long short-term memory networks
"  We propose a principled method for kernel learning, which relies on a
Fourier-analytic characterization of translation-invariant or
rotation-invariant kernels. Our method produces a sequence of feature maps,
iteratively refining the SVM margin. We provide rigorous guarantees for
optimality and generalization, interpreting our algorithm as online
equilibrium-finding dynamics in a certain two-player min-max game. Evaluations
on synthetic and real-world datasets demonstrate scalability and consistent
improvements over related random features-based methods.
",Kernel Learning Methods,kernel learning
"  We propose a new statistical model suitable for machine learning of systems
with long distance correlations such as natural languages. The model is based
on directed acyclic graph decorated by multi-linear tensor maps in the vertices
and vector spaces in the edges, called tensor network. Such tensor networks
have been previously employed for effective numerical computation of the
renormalization group flow on the space of effective quantum field theories and
lattice models of statistical mechanics. We provide explicit algebro-geometric
analysis of the parameter moduli space for tree graphs, discuss model
properties and applications such as statistical translation.
",Tensor Networks for Machine Learning,tensor network models for machine learning
"  Standard deep learning systems require thousands or millions of examples to
learn a concept, and cannot integrate new concepts easily. By contrast, humans
have an incredible ability to do one-shot or few-shot learning. For instance,
from just hearing a word used in a sentence, humans can infer a great deal
about it, by leveraging what the syntax and semantics of the surrounding words
tells us. Here, we draw inspiration from this to highlight a simple technique
by which deep recurrent networks can similarly exploit their prior knowledge to
learn a useful representation for a new word from little data. This could make
natural language processing systems much more flexible, by allowing them to
learn continually from the new words they encounter.
",One-Shot Learning in NLP,few-shot learning in natural language processing
"  Since the creation of Generative Adversarial Networks (GANs), much work has
been done to improve their training stability, their generated image quality,
their range of application but nearly none of them explored their self-training
potential. Self-training has been used before the advent of deep learning in
order to allow training on limited labelled training data and has shown
impressive results in semi-supervised learning. In this work, we combine these
two ideas and make GANs self-trainable for semi-supervised learning tasks by
exploiting their infinite data generation potential. Results show that using
even the simplest form of self-training yields an improvement. We also show
results for a more complex self-training scheme that performs at least as well
as the basic self-training scheme but with significantly less data
augmentation.
",Self-Training of GANs for Semi-Supervised Learning,self-training in generative adversarial networks for semi-supervised learning
"  Nodes residing in different parts of a graph can have similar structural
roles within their local network topology. The identification of such roles
provides key insight into the organization of networks and can be used for a
variety of machine learning tasks. However, learning structural representations
of nodes is a challenging problem, and it has typically involved manually
specifying and tailoring topological features for each node. In this paper, we
develop GraphWave, a method that represents each node's network neighborhood
via a low-dimensional embedding by leveraging heat wavelet diffusion patterns.
Instead of training on hand-selected features, GraphWave learns these
embeddings in an unsupervised way. We mathematically prove that nodes with
similar network neighborhoods will have similar GraphWave embeddings even
though these nodes may reside in very different parts of the network, and our
method scales linearly with the number of edges. Experiments in a variety of
different settings demonstrate GraphWave's real-world potential for capturing
structural roles in networks, and our approach outperforms existing
state-of-the-art baselines in every experiment, by as much as 137%.
",Graph Node Representation Learning,graph-based node representation learning
"  We revisit fuzzy neural network with a cornerstone notion of generalized
hamming distance, which provides a novel and theoretically justified framework
to re-interpret many useful neural network techniques in terms of fuzzy logic.
In particular, we conjecture and empirically illustrate that, the celebrated
batch normalization (BN) technique actually adapts the normalized bias such
that it approximates the rightful bias induced by the generalized hamming
distance. Once the due bias is enforced analytically, neither the optimization
of bias terms nor the sophisticated batch normalization is needed. Also in the
light of generalized hamming distance, the popular rectified linear units
(ReLU) can be treated as setting a minimal hamming distance threshold between
network inputs and weights. This thresholding scheme, on the one hand, can be
improved by introducing double thresholding on both extremes of neuron outputs.
On the other hand, ReLUs turn out to be non-essential and can be removed from
networks trained for simple tasks like MNIST classification. The proposed
generalized hamming network (GHN) as such not only lends itself to rigorous
analysis and interpretation within the fuzzy logic theory but also demonstrates
fast learning speed, well-controlled behaviour and state-of-the-art
performances on a variety of learning tasks.
",Fuzzy Neural Networks,fuzzy neural networks
"  State-of-the-art methods in convex and non-convex optimization employ
higher-order derivative information, either implicitly or explicitly. We
explore the limitations of higher-order optimization and prove that even for
convex optimization, a polynomial dependence on the approximation guarantee and
higher-order smoothness parameters is necessary. As a special case, we show
Nesterov's accelerated cubic regularization method to be nearly tight.
",Higher-Order Optimization Limitations,optimization techniques
"  Multi-label classification is an important learning problem with many
applications. In this work, we propose a principled similarity-based approach
for multi-label learning called SML. We also introduce a similarity-based
approach for predicting the label set size. The experimental results
demonstrate the effectiveness of SML for multi-label classification where it is
shown to compare favorably with a wide variety of existing algorithms across a
range of evaluation criterion.
",Multi-Label Classification,multi-label classification
"  We examine gradient descent on unregularized logistic regression problems,
with homogeneous linear predictors on linearly separable datasets. We show the
predictor converges to the direction of the max-margin (hard margin SVM)
solution. The result also generalizes to other monotone decreasing loss
functions with an infimum at infinity, to multi-class problems, and to training
a weight layer in a deep network in a certain restricted setting. Furthermore,
we show this convergence is very slow, and only logarithmic in the convergence
of the loss itself. This can help explain the benefit of continuing to optimize
the logistic or cross-entropy loss even after the training error is zero and
the training loss is extremely small, and, as we show, even if the validation
loss increases. Our methodology can also aid in understanding implicit
regularization n more complex models and with other optimization methods.
",Convergence of Gradient Descent in Logistic Regression,convergence of gradient descent in logistic regression
"  Convolutional neural networks (CNNs) are being applied to an increasing
number of problems and fields due to their superior performance in
classification and regression tasks. Since two of the key operations that CNNs
implement are convolution and pooling, this type of networks is implicitly
designed to act on data described by regular structures such as images.
Motivated by the recent interest in processing signals defined in irregular
domains, we advocate a CNN architecture that operates on signals supported on
graphs. The proposed design replaces the classical convolution not with a
node-invariant graph filter (GF), which is the natural generalization of
convolution to graph domains, but with a node-varying GF. This filter extracts
different local features without increasing the output dimension of each layer
and, as a result, bypasses the need for a pooling stage while involving only
local operations. A second contribution is to replace the node-varying GF with
a hybrid node-varying GF, which is a new type of GF introduced in this paper.
While the alternative architecture can still be run locally without requiring a
pooling stage, the number of trainable parameters is smaller and can be
rendered independent of the data dimension. Tests are run on a synthetic source
localization problem and on the 20NEWS dataset.
",Graph Convolutional Neural Networks,graph convolutional neural networks
"  We propose a fully distributed actor-critic algorithm approximated by deep
neural networks, named \textit{Diff-DAC}, with application to single-task and
to average multitask reinforcement learning (MRL). Each agent has access to
data from its local task only, but it aims to learn a policy that performs well
on average for the whole set of tasks. During the learning process, agents
communicate their value-policy parameters to their neighbors, diffusing the
information across the network, so that they converge to a common policy, with
no need for a central node. The method is scalable, since the computational and
communication costs per agent grow with its number of neighbors. We derive
Diff-DAC's from duality theory and provide novel insights into the standard
actor-critic framework, showing that it is actually an instance of the dual
ascent method that approximates the solution of a linear program. Experiments
suggest that Diff-DAC can outperform the single previous distributed MRL
approach (i.e., Dist-MTLPS) and even the centralized architecture.
",Distributed Multi-Task Reinforcement Learning,distributed multi-task reinforcement learning
"  We study the consistency of Lipschitz learning on graphs in the limit of
infinite unlabeled data and finite labeled data. Previous work has conjectured
that Lipschitz learning is well-posed in this limit, but is insensitive to the
distribution of the unlabeled data, which is undesirable for semi-supervised
learning. We first prove that this conjecture is true in the special case of a
random geometric graph model with kernel-based weights. Then we go on to show
that on a random geometric graph with self-tuning weights, Lipschitz learning
is in fact highly sensitive to the distribution of the unlabeled data, and we
show how the degree of sensitivity can be adjusted by tuning the weights. In
both cases, our results follow from showing that the sequence of learned
functions converges to the viscosity solution of an $\infty$-Laplace type
equation, and studying the structure of the limiting equation.
",Lipschitz Learning on Graphs,mathematical optimization
"  The change detection problem is to determine if the Markov network structures
of two Markov random fields differ from one another given two sets of samples
drawn from the respective underlying distributions. We study the trade-off
between the sample sizes and the reliability of change detection, measured as a
minimax risk, for the important cases of the Ising models and the Gaussian
Markov random fields restricted to the models which have network structures
with $p$ nodes and degree at most $d$, and obtain information-theoretic lower
bounds for reliable change detection over these models. We show that for the
Ising model, $\Omega\left(\frac{d^2}{(\log d)^2}\log p\right)$ samples are
required from each dataset to detect even the sparsest possible changes, and
that for the Gaussian, $\Omega\left( \gamma^{-2} \log(p)\right)$ samples are
required from each dataset to detect change, where $\gamma$ is the smallest
ratio of off-diagonal to diagonal terms in the precision matrices of the
distributions. These bounds are compared to the corresponding results in
structure learning, and closely match them under mild conditions on the model
parameters. Thus, our change detection bounds inherit partial tightness from
the structure learning schemes in previous literature, demonstrating that in
certain parameter regimes, the naive structure learning based approach to
change detection is minimax optimal up to constant factors.
",Change Detection in Markov Random Fields,change detection in markov random fields
"  Despite advances in deep learning, neural networks can only learn multiple
tasks when trained on them jointly. When tasks arrive sequentially, they lose
performance on previously learnt tasks. This phenomenon called catastrophic
forgetting is a fundamental challenge to overcome before neural networks can
learn continually from incoming data. In this work, we derive inspiration from
human memory to develop an architecture capable of learning continuously from
sequentially incoming tasks, while averting catastrophic forgetting.
Specifically, our contributions are: (i) a dual memory architecture emulating
the complementary learning systems (hippocampus and the neocortex) in the human
brain, (ii) memory consolidation via generative replay of past experiences,
(iii) demonstrating advantages of generative replay and dual memories via
experiments, and (iv) improved performance retention on challenging tasks even
for low capacity models. Our architecture displays many characteristics of the
mammalian memory and provides insights on the connection between sleep and
learning.
",Overcoming Catastrophic Forgetting in Neural Networks,overcoming catastrophic forgetting in neural networks
"  Spectral graph convolutional neural networks (CNNs) require approximation to
the convolution to alleviate the computational complexity, resulting in
performance loss. This paper proposes the topology adaptive graph convolutional
network (TAGCN), a novel graph convolutional network defined in the vertex
domain. We provide a systematic way to design a set of fixed-size learnable
filters to perform convolutions on graphs. The topologies of these filters are
adaptive to the topology of the graph when they scan the graph to perform
convolution. The TAGCN not only inherits the properties of convolutions in CNN
for grid-structured data, but it is also consistent with convolution as defined
in graph signal processing. Since no approximation to the convolution is
needed, TAGCN exhibits better performance than existing spectral CNNs on a
number of data sets and is also computationally simpler than other recent
methods.
",Graph Convolutional Neural Networks,graph convolutional networks
"  Context plays an important role in human language understanding, thus it may
also be useful for machines learning vector representations of language. In
this paper, we explore an asymmetric encoder-decoder structure for unsupervised
context-based sentence representation learning. We carefully designed
experiments to show that neither an autoregressive decoder nor an RNN decoder
is required. After that, we designed a model which still keeps an RNN as the
encoder, while using a non-autoregressive convolutional decoder. We further
combine a suite of effective designs to significantly improve model efficiency
while also achieving better performance. Our model is trained on two different
large unlabelled corpora, and in both cases the transferability is evaluated on
a set of downstream NLP tasks. We empirically show that our model is simple and
fast while producing rich sentence representations that excel in downstream
tasks.
",Context-Aware Sentence Embeddings,unsupervised context-based sentence representation learning
"  In this paper, we provide an approach to clustering relational matrices whose
entries correspond to either similarities or dissimilarities between objects.
Our approach is based on the value of information, a parameterized,
information-theoretic criterion that measures the change in costs associated
with changes in information. Optimizing the value of information yields a
deterministic annealing style of clustering with many benefits. For instance,
investigators avoid needing to a priori specify the number of clusters, as the
partitions naturally undergo phase changes, during the annealing process,
whereby the number of clusters changes in a data-driven fashion. The
global-best partition can also often be identified.
",Relational Matrix Clustering,relational matrix clustering
"  There has been a recent surge of interest in studying permutation-based
models for ranking from pairwise comparison data. Despite being structurally
richer and more robust than parametric ranking models, permutation-based models
are less well understood statistically and generally lack efficient learning
algorithms. In this work, we study a prototype of permutation-based ranking
models, namely, the noisy sorting model. We establish the optimal rates of
learning the model under two sampling procedures. Furthermore, we provide a
fast algorithm to achieve near-optimal rates if the observations are sampled
independently. Along the way, we discover properties of the symmetric group
which are of theoretical interest.
",Permutation-Based Ranking Models,machine learning/statistics
"  We propose a method, called Label Embedding Network, which can learn label
representation (label embedding) during the training process of deep networks.
With the proposed method, the label embedding is adaptively and automatically
learned through back propagation. The original one-hot represented loss
function is converted into a new loss function with soft distributions, such
that the originally unrelated labels have continuous interactions with each
other during the training process. As a result, the trained model can achieve
substantially higher accuracy and with faster convergence speed. Experimental
results based on competitive tasks demonstrate the effectiveness of the
proposed method, and the learned label embedding is reasonable and
interpretable. The proposed method achieves comparable or even better results
than the state-of-the-art systems. The source code is available at
\url{https://github.com/lancopku/LabelEmb}.
",Label Embedding in Deep Learning,deep learning for text classification
"  Connections between nodes of fully connected neural networks are usually
represented by weight matrices. In this article, functional transfer matrices
are introduced as alternatives to the weight matrices: Instead of using real
weights, a functional transfer matrix uses real functions with trainable
parameters to represent connections between nodes. Multiple functional transfer
matrices are then stacked together with bias vectors and activations to form
deep functional transfer neural networks. These neural networks can be trained
within the framework of back-propagation, based on a revision of the delta
rules and the error transmission rule for functional connections. In
experiments, it is demonstrated that the revised rules can be used to train a
range of functional connections: 20 different functions are applied to neural
networks with up to 10 hidden layers, and most of them gain high test
accuracies on the MNIST database. It is also demonstrated that a functional
transfer matrix with a memory function can roughly memorise a non-cyclical
sequence of 400 digits.
",Functional Neural Networks,functional transfer matrices in neural networks
"  We propose a new localized inference algorithm for answering marginalization
queries in large graphical models with the correlation decay property. Given a
query variable and a large graphical model, we define a much smaller model in a
local region around the query variable in the target model so that the marginal
distribution of the query variable can be accurately approximated. We introduce
two approximation error bounds based on the Dobrushin's comparison theorem and
apply our bounds to derive a greedy expansion algorithm that efficiently guides
the selection of neighbor nodes for localized inference. We verify our
theoretical bounds on various datasets and demonstrate that our localized
inference algorithm can provide fast and accurate approximation for large
graphical models.
",Localized Inference in Graphical Models,localized inference in graphical models
"  Recent work has shown that the end-to-end approach using convolutional neural
network (CNN) is effective in various types of machine learning tasks. For
audio signals, the approach takes raw waveforms as input using an 1-D
convolution layer. In this paper, we improve the 1-D CNN architecture for music
auto-tagging by adopting building blocks from state-of-the-art image
classification models, ResNets and SENets, and adding multi-level feature
aggregation to it. We compare different combinations of the modules in building
CNN architectures. The results show that they achieve significant improvements
over previous state-of-the-art models on the MagnaTagATune dataset and
comparable results on Million Song Dataset. Furthermore, we analyze and
visualize our model to show how the 1-D CNN operates.
",Music Auto-Tagging using CNN,music auto-tagging
"  Uniformity testing and the more general identity testing are well studied
problems in distributional property testing. Most previous work focuses on
testing under $L_1$-distance. However, when the support is very large or even
continuous, testing under $L_1$-distance may require a huge (even infinite)
number of samples. Motivated by such issues, we consider the identity testing
in Wasserstein distance (a.k.a. transportation distance and earthmover
distance) on a metric space (discrete or continuous).
  In this paper, we propose the Wasserstein identity testing problem (Identity
Testing in Wasserstein distance). We obtain nearly optimal worst-case sample
complexity for the problem. Moreover, for a large class of probability
distributions satisfying the so-called ""Doubling Condition"", we provide nearly
instance-optimal sample complexity.
",Distributional Property Testing,wasserstein identity testing
"  In this paper, we propose a new loss function called generalized end-to-end
(GE2E) loss, which makes the training of speaker verification models more
efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike
TE2E, the GE2E loss function updates the network in a way that emphasizes
examples that are difficult to verify at each step of the training process.
Additionally, the GE2E loss does not require an initial stage of example
selection. With these properties, our model with the new loss function
decreases speaker verification EER by more than 10%, while reducing the
training time by 60% at the same time. We also introduce the MultiReader
technique, which allows us to do domain adaptation - training a more accurate
model that supports multiple keywords (i.e. ""OK Google"" and ""Hey Google"") as
well as multiple dialects.
",Speaker Verification Model Optimization,speaker verification
"  For many years, i-vector based audio embedding techniques were the dominant
approach for speaker verification and speaker diarization applications.
However, mirroring the rise of deep learning in various domains, neural network
based audio embeddings, also known as d-vectors, have consistently demonstrated
superior speaker verification performance. In this paper, we build on the
success of d-vector based speaker verification systems to develop a new
d-vector based approach to speaker diarization. Specifically, we combine
LSTM-based d-vector audio embeddings with recent work in non-parametric
clustering to obtain a state-of-the-art speaker diarization system. Our system
is evaluated on three standard public datasets, suggesting that d-vector based
diarization systems offer significant advantages over traditional i-vector
based systems. We achieved a 12.0% diarization error rate on NIST SRE 2000
CALLHOME, while our model is trained with out-of-domain data from voice search
logs.
",Speaker Diarization using Deep Learning,neural network-based audio embeddings for speaker diarization
"  Attention-based models have recently shown great performance on a range of
tasks, such as speech recognition, machine translation, and image captioning
due to their ability to summarize relevant information that expands through the
entire length of an input sequence. In this paper, we analyze the usage of
attention mechanisms to the problem of sequence summarization in our end-to-end
text-dependent speaker recognition system. We explore different topologies and
their variants of the attention layer, and compare different pooling methods on
the attention weights. Ultimately, we show that attention-based models can
improves the Equal Error Rate (EER) of our speaker verification system by
relatively 14% compared to our non-attention LSTM baseline model.
",Attention in Speaker Recognition,attention mechanisms in sequence summarization
"  We present a new approach for detecting related crime series, by unsupervised
learning of the latent feature embeddings from narratives of crime record via
the Gaussian-Bernoulli Restricted Boltzmann Machines (RBM). This is a
drastically different approach from prior work on crime analysis, which
typically considers only time and location and at most category information.
After the embedding, related cases are closer to each other in the Euclidean
feature space, and the unrelated cases are far apart, which is a good property
can enable subsequent analysis such as detection and clustering of related
cases. Experiments over several series of related crime incidents hand labeled
by the Atlanta Police Department reveal the promise of our embedding methods.
",Crime Pattern Detection,crime series detection
"  Recent work has addressed using formulas in linear temporal logic (LTL) as
specifications for agents planning in Markov Decision Processes (MDPs). We
consider the inverse problem: inferring an LTL specification from demonstrated
behavior trajectories in MDPs. We formulate this as a multiobjective
optimization problem, and describe state-based (""what actually happened"") and
action-based (""what the agent expected to happen"") objective functions based on
a notion of ""violation cost"". We demonstrate the efficacy of the approach by
employing genetic programming to solve this problem in two simple domains.
",Inverse Reinforcement Learning in MDPs,formalizing behavior specifications from demonstrated trajectories
"  In order for machine learning to be deployed and trusted in many
applications, it is crucial to be able to reliably explain why the machine
learning algorithm makes certain predictions. For example, if an algorithm
classifies a given pathology image to be a malignant tumor, then the doctor may
need to know which parts of the image led the algorithm to this classification.
How to interpret black-box predictors is thus an important and active area of
research. A fundamental question is: how much can we trust the interpretation
itself? In this paper, we show that interpretation of deep learning predictions
is extremely fragile in the following sense: two perceptively indistinguishable
inputs with the same predicted label can be assigned very different
interpretations. We systematically characterize the fragility of several
widely-used feature-importance interpretation methods (saliency maps, relevance
propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that
even small random perturbation can change the feature importance and new
systematic perturbations can lead to dramatically different interpretations
without changing the label. We extend these results to show that
interpretations based on exemplars (e.g. influence functions) are similarly
fragile. Our analysis of the geometry of the Hessian matrix gives insight on
why fragility could be a fundamental challenge to the current interpretation
approaches.
",Fragility of Interpretation in Deep Learning,machine learning interpretability
"  We consider the problem of optimizing a high-dimensional convex function
using stochastic zeroth-order queries. Under sparsity assumptions on the
gradients or function values, we present two algorithms: a successive
component/feature selection algorithm and a noisy mirror descent algorithm
using Lasso gradient estimates, and show that both algorithms have convergence
rates that de- pend only logarithmically on the ambient dimension of the
problem. Empirical results confirm our theoretical findings and show that the
algorithms we design outperform classical zeroth-order optimization methods in
the high-dimensional setting.
",Stochastic Optimization of High-Dimensional Convex Functions,high-dimensional convex optimization
"  This paper proposes an uncertain data clustering approach to quantitatively
analyze the complexity of prefabricated construction components through the
integration of quality performance-based measures with associated engineering
design information. The proposed model is constructed in three steps, which (1)
measure prefabricated construction product complexity (hereafter referred to as
product complexity) by introducing a Bayesian-based nonconforming quality
performance indicator; (2) score each type of product complexity by developing
a Hellinger distance-based distribution similarity measurement; and (3) cluster
products into homogeneous complexity groups by using the agglomerative
hierarchical clustering technique. An illustrative example is provided to
demonstrate the proposed approach, and a case study of an industrial company in
Edmonton, Canada, is conducted to validate the feasibility and applicability of
the proposed model. This research inventively defines and investigates product
complexity from the perspective of product quality performance with design
information associated. The research outcomes provide simplified,
interpretable, and informative insights for practitioners to better analyze and
manage product complexity. In addition to this practical contribution, a novel
hierarchical clustering technique is devised. This technique is capable of
clustering uncertain data (i.e., beta distributions) with lower computational
complexity and has the potential to be generalized to cluster all types of
uncertain data.
",Uncertain Data Clustering in Construction Engineering,uncertain data clustering for quantifying prefabricated construction component complexity
"  Currently known methods for this task either employ the computationally
intensive \emph{exponential mechanism} or require an access to the covariance
matrix, and therefore fail to utilize potential sparsity of the data. The
problem of designing simpler and more efficient methods for this task has been
raised as an open problem in \cite{kapralov2013differentially}.
  In this paper we address this problem by employing the output perturbation
mechanism. Despite being arguably the simplest and most straightforward
technique, it has been overlooked due to the large \emph{global sensitivity}
associated with publishing the leading eigenvector. We tackle this issue by
adopting a \emph{smooth sensitivity} based approach, which allows us to
establish differential privacy (in a worst-case manner) and near-optimal sample
complexity results under eigengap assumption. We consider both the pure and the
approximate notions of differential privacy, and demonstrate a tradeoff between
privacy level and sample complexity. We conclude by suggesting how our results
can be extended to related problems.
",Differentially Private Eigenvector Computation,differential privacy
"  Data augmentation is an essential part of the training process applied to
deep learning models. The motivation is that a robust training process for deep
learning models depends on large annotated datasets, which are expensive to be
acquired, stored and processed. Therefore a reasonable alternative is to be
able to automatically generate new annotated training samples using a process
known as data augmentation. The dominant data augmentation approach in the
field assumes that new training samples can be obtained via random geometric or
appearance transformations applied to annotated training samples, but this is a
strong assumption because it is unclear if this is a reliable generative model
for producing new training samples. In this paper, we provide a novel Bayesian
formulation to data augmentation, where new annotated training points are
treated as missing variables and generated based on the distribution learned
from the training set. For learning, we introduce a theoretically sound
algorithm --- generalised Monte Carlo expectation maximisation, and demonstrate
one possible implementation via an extension of the Generative Adversarial
Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the
better performance of our proposed method compared to the current dominant data
augmentation approach mentioned above --- the results also show that our
approach produces better classification results than similar GAN models.
",Bayesian Data Augmentation,data augmentation in deep learning
"  Graph convolutional networks (GCNs) are powerful deep neural networks for
graph-structured data. However, GCN computes the representation of a node
recursively from its neighbors, making the receptive field size grow
exponentially with the number of layers. Previous attempts on reducing the
receptive field size by subsampling neighbors do not have a convergence
guarantee, and their receptive field size per node is still in the order of
hundreds. In this paper, we develop control variate based algorithms which
allow sampling an arbitrarily small neighbor size. Furthermore, we prove new
theoretical guarantee for our algorithms to converge to a local optimum of GCN.
Empirical results show that our algorithms enjoy a similar convergence with the
exact algorithm using only two neighbors per node. The runtime of our
algorithms on a large Reddit dataset is only one seventh of previous neighbor
sampling algorithms.
",Efficient Graph Convolutional Networks,graph convolutional networks
"  Deep neural networks (DNNs) form the backbone of almost every
state-of-the-art technique in the fields such as computer vision, speech
processing, and text analysis. The recent advances in computational technology
have made the use of DNNs more practical. Despite the overwhelming performances
by DNN and the advances in computational technology, it is seen that very few
researchers try to train their models from the scratch. Training of DNNs still
remains a difficult and tedious job. The main challenges that researchers face
during training of DNNs are the vanishing/exploding gradient problem and the
highly non-convex nature of the objective function which has up to million
variables. The approaches suggested in He and Xavier solve the vanishing
gradient problem by providing a sophisticated initialization technique. These
approaches have been quite effective and have achieved good results on standard
datasets, but these same approaches do not work very well on more practical
datasets. We think the reason for this is not making use of data statistics for
initializing the network weights. Optimizing such a high dimensional loss
function requires careful initialization of network weights. In this work, we
propose a data dependent initialization and analyze its performance against the
standard initialization techniques such as He and Xavier. We performed our
experiments on some practical datasets and the results show our algorithm's
superior classification accuracy.
",Deep Neural Network Initialization Techniques,deep neural network training
"  Neural networks are vulnerable to adversarial examples and researchers have
proposed many heuristic attack and defense mechanisms. We address this problem
through the principled lens of distributionally robust optimization, which
guarantees performance under adversarial input perturbations. By considering a
Lagrangian penalty formulation of perturbing the underlying data distribution
in a Wasserstein ball, we provide a training procedure that augments model
parameter updates with worst-case perturbations of training data. For smooth
losses, our procedure provably achieves moderate levels of robustness with
little computational or statistical cost relative to empirical risk
minimization. Furthermore, our statistical guarantees allow us to efficiently
certify robustness for the population loss. For imperceptible perturbations,
our method matches or outperforms heuristic approaches.
",Adversarial Robustness in Neural Networks,adversarial examples and distributionally robust optimization
"  The support vector machine (SVM) is a widely used machine learning tool for
classification based on statistical learning theory. Given a set of training
data, the SVM finds a hyperplane that separates two different classes of data
points by the largest distance. While the standard form of SVM uses L2-norm
regularization, other regularization approaches are particularly attractive for
biomedical datasets where, for example, sparsity and interpretability of the
classifier's coefficient values are highly desired features. Therefore, in this
paper we consider different types of regularization approaches for SVMs, and
explore them in both synthetic and real biomedical datasets.
",SVM Regularization Techniques,regularization techniques in support vector machines for biomedical datasets
"  This paper develops variational continual learning (VCL), a simple but
general framework for continual learning that fuses online variational
inference (VI) and recent advances in Monte Carlo VI for neural networks. The
framework can successfully train both deep discriminative models and deep
generative models in complex continual learning settings where existing tasks
evolve over time and entirely new tasks emerge. Experimental results show that
VCL outperforms state-of-the-art continual learning methods on a variety of
tasks, avoiding catastrophic forgetting in a fully automatic way.
",Continual Learning for Neural Networks,variational continual learning
"  Molecular simulations produce very high-dimensional data-sets with millions
of data points. As analysis methods are often unable to cope with so many
dimensions, it is common to use dimensionality reduction and clustering methods
to reach a reduced representation of the data. Yet these methods often fail to
capture the most important features necessary for the construction of a Markov
model. Here we demonstrate the results of various dimensionality reduction
methods on two simulation data-sets, one of protein folding and another of
protein-ligand binding. The methods tested include a k-means clustering
variant, a non-linear auto encoder, principal component analysis and tICA. The
dimension-reduced data is then used to estimate the implied timescales of the
slowest process by a Markov state model analysis to assess the quality of the
projection. The projected dimensions learned from the data are visualized to
demonstrate which conformations the various methods choose to represent the
molecular process.
",Dimensionality Reduction in Molecular Simulations,dimensionality reduction techniques in molecular simulations
"  Quick Shift is a popular mode-seeking and clustering algorithm. We present
finite sample statistical consistency guarantees for Quick Shift on mode and
cluster recovery under mild distributional assumptions. We then apply our
results to construct a consistent modal regression algorithm.
",Quick Shift Algorithm Consistency,clustering algorithm
"  We study the multi-armed bandit problem where the rewards are realizations of
general non-stationary stochastic processes, a setting that generalizes many
existing lines of work and analyses. In particular, we present a theoretical
analysis and derive regret guarantees for rested bandits in which the reward
distribution of each arm changes only when we pull that arm. Remarkably, our
regret bounds are logarithmic in the number of rounds under several natural
conditions. We introduce a new algorithm based on classical UCB ideas combined
with the notion of weighted discrepancy, a useful tool for measuring the
non-stationarity of a stochastic process. We show that the notion of
discrepancy can be used to design very general algorithms and a unified
framework for the analysis of multi-armed rested bandit problems with
non-stationary rewards. In particular, we show that we can recover the regret
guarantees of many specific instances of bandit problems with non-stationary
rewards that have been studied in the literature. We also provide experiments
demonstrating that our algorithms can enjoy a significant improvement in
practice compared to standard benchmarks.
",Non-Stationary Multi-Armed Bandits,non-stationary multi-armed bandits
"  Regularization is one of the crucial ingredients of deep learning, yet the
term regularization has various definitions, and regularization methods are
often studied separately from each other. In our work we present a systematic,
unifying taxonomy to categorize existing methods. We distinguish methods that
affect data, network architectures, error terms, regularization terms, and
optimization procedures. We do not provide all details about the listed
methods; instead, we present an overview of how the methods can be sorted into
meaningful categories and sub-categories. This helps revealing links and
fundamental similarities between them. Finally, we include practical
recommendations both for users and for developers of new regularization
methods.
",Deep Learning Regularization Taxonomy,regularization methods in deep learning
"  Graph kernels have been successfully applied to many graph classification
problems. Typically, a kernel is first designed, and then an SVM classifier is
trained based on the features defined implicitly by this kernel. This two-stage
approach decouples data representation from learning, which is suboptimal. On
the other hand, Convolutional Neural Networks (CNNs) have the capability to
learn their own features directly from the raw data during training.
Unfortunately, they cannot handle irregular data such as graphs. We address
this challenge by using graph kernels to embed meaningful local neighborhoods
of the graphs in a continuous vector space. A set of filters is then convolved
with these patches, pooled, and the output is then passed to a feedforward
network. With limited parameter tuning, our approach outperforms strong
baselines on 7 out of 10 benchmark datasets.
",Graph-Based Neural Networks,graph convolutional neural networks
"  Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at
harnessing the energy efficiency of spike-domain processing by building on
computing elements that operate on, and exchange, spikes. In this paper, the
problem of training a two-layer SNN is studied for the purpose of
classification, under a Generalized Linear Model (GLM) probabilistic neural
model that was previously considered within the computational neuroscience
literature. Conventional classification rules for SNNs operate offline based on
the number of output spikes at each output neuron. In contrast, a novel
training method is proposed here for a first-to-spike decoding rule, whereby
the SNN can perform an early classification decision once spike firing is
detected at an output neuron. Numerical results bring insights into the optimal
parameter selection for the GLM neuron and on the accuracy-complexity trade-off
performance of conventional and first-to-spike decoding.
",Spiking Neural Networks for Classification,spiking neural networks (snns) for classification
"  Machine learning algorithms such as linear regression, SVM and neural network
have played an increasingly important role in the process of scientific
discovery. However, none of them is both interpretable and accurate on
nonlinear datasets. Here we present contextual regression, a method that joins
these two desirable properties together using a hybrid architecture of neural
network embedding and dot product layer. We demonstrate its high prediction
accuracy and sensitivity through the task of predictive feature selection on a
simulated dataset and the application of predicting open chromatin sites in the
human genome. On the simulated data, our method achieved high fidelity recovery
of feature contributions under random noise levels up to 200%. On the open
chromatin dataset, the application of our method not only outperformed the
state of the art method in terms of accuracy, but also unveiled two previously
unfound open chromatin related histone marks. Our method can fill the blank of
accurate and interpretable nonlinear modeling in scientific data mining tasks.
",Contextual Regression for Interpretable Nonlinear Modeling,machine learning for nonlinear modeling
"  The Madry Lab recently hosted a competition designed to test the robustness
of their adversarially trained MNIST model. Attacks were constrained to perturb
each pixel of the input image by a scaled maximal $L_\infty$ distortion
$\epsilon$ = 0.3. This discourages the use of attacks which are not optimized
on the $L_\infty$ distortion metric. Our experimental results demonstrate that
by relaxing the $L_\infty$ constraint of the competition, the elastic-net
attack to deep neural networks (EAD) can generate transferable adversarial
examples which, despite their high average $L_\infty$ distortion, have minimal
visual distortion. These results call into question the use of $L_\infty$ as a
sole measure for visual distortion, and further demonstrate the power of EAD at
generating robust adversarial examples.
",Adversarial Attacks on Image Classification Models,adversarial machine learning
"  In this work we establish the first linear convergence result for the
stochastic heavy ball method. The method performs SGD steps with a fixed
stepsize, amended by a heavy ball momentum term. In the analysis, we focus on
minimizing the expected loss and not on finite-sum minimization, which is
typically a much harder problem. While in the analysis we constrain ourselves
to quadratic loss, the overall objective is not necessarily strongly convex.
",Stochastic Optimization,stochastic optimization
"  Progress in probabilistic generative models has accelerated, developing
richer models with neural architectures, implicit densities, and with scalable
algorithms for their Bayesian inference. However, there has been limited
progress in models that capture causal relationships, for example, how
individual genetic factors cause major human diseases. In this work, we focus
on two challenges in particular: How do we build richer causal models, which
can capture highly nonlinear relationships and interactions between multiple
causes? How do we adjust for latent confounders, which are variables
influencing both cause and effect and which prevent learning of causal
relationships? To address these challenges, we synthesize ideas from causality
and modern probabilistic modeling. For the first, we describe implicit causal
models, a class of causal models that leverages neural architectures with an
implicit density. For the second, we describe an implicit causal model that
adjusts for confounders by sharing strength across examples. In experiments, we
scale Bayesian inference on up to a billion genetic measurements. We achieve
state of the art accuracy for identifying causal factors: we significantly
outperform existing genetics methods by an absolute difference of 15-45.3%.
",Causal Modeling in Genetics,causal modeling in probabilistic generative models
"  Panoramic video provides immersive and interactive experience by enabling
humans to control the field of view (FoV) through head movement (HM). Thus, HM
plays a key role in modeling human attention on panoramic video. This paper
establishes a database collecting subjects' HM in panoramic video sequences.
From this database, we find that the HM data are highly consistent across
subjects. Furthermore, we find that deep reinforcement learning (DRL) can be
applied to predict HM positions, via maximizing the reward of imitating human
HM scanpaths through the agent's actions. Based on our findings, we propose a
DRL-based HM prediction (DHP) approach with offline and online versions, called
offline-DHP and online-DHP. In offline-DHP, multiple DRL workflows are run to
determine potential HM positions at each panoramic frame. Then, a heat map of
the potential HM positions, named the HM map, is generated as the output of
offline-DHP. In online-DHP, the next HM position of one subject is estimated
given the currently observed HM position, which is achieved by developing a DRL
algorithm upon the learned offline-DHP model. Finally, the experiments validate
that our approach is effective in both offline and online prediction of HM
positions for panoramic video, and that the learned offline-DHP model can
improve the performance of online-DHP.
",Head Movement Prediction in Panoramic Video,panoramic video and human head movement
"  Adversarial perturbations of normal images are usually imperceptible to
humans, but they can seriously confuse state-of-the-art machine learning
models. What makes them so special in the eyes of image classifiers? In this
paper, we show empirically that adversarial examples mainly lie in the low
probability regions of the training distribution, regardless of attack types
and targeted models. Using statistical hypothesis testing, we find that modern
neural density models are surprisingly good at detecting imperceptible image
perturbations. Based on this discovery, we devised PixelDefend, a new approach
that purifies a maliciously perturbed image by moving it back towards the
distribution seen in the training data. The purified image is then run through
an unmodified classifier, making our method agnostic to both the classifier and
the attacking method. As a result, PixelDefend can be used to protect already
deployed models and be combined with other model-specific defenses. Experiments
show that our method greatly improves resilience across a wide variety of
state-of-the-art attacking methods, increasing accuracy on the strongest attack
from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.
",Adversarial Attacks on Image Classification,adversarial attacks on machine learning models
"  Across a variety of scientific disciplines, sparse inverse covariance
estimation is a popular tool for capturing the underlying dependency
relationships in multivariate data. Unfortunately, most estimators are not
scalable enough to handle the sizes of modern high-dimensional data sets (often
on the order of terabytes), and assume Gaussian samples. To address these
deficiencies, we introduce HP-CONCORD, a highly scalable optimization method
for estimating a sparse inverse covariance matrix based on a regularized
pseudolikelihood framework, without assuming Gaussianity. Our parallel proximal
gradient method uses a novel communication-avoiding linear algebra algorithm
and runs across a multi-node cluster with up to 1k nodes (24k cores), achieving
parallel scalability on problems with up to ~819 billion parameters (1.28
million dimensions); even on a single node, HP-CONCORD demonstrates
scalability, outperforming a state-of-the-art method. We also use HP-CONCORD to
estimate the underlying dependency structure of the brain from fMRI data, and
use the result to identify functional regions automatically. The results show
good agreement with a clustering from the neuroscience literature.
",Scalable Sparse Inverse Covariance Estimation,sparse inverse covariance matrix estimation
"  We study projection-free methods for constrained Riemannian optimization. In
particular, we propose the Riemannian Frank-Wolfe (RFW) method. We analyze
non-asymptotic convergence rates of RFW to an optimum for (geodesically) convex
problems, and to a critical point for nonconvex objectives. We also present a
practical setting under which RFW can attain a linear convergence rate. As a
concrete example, we specialize RFW to the manifold of positive definite
matrices and apply it to two tasks: (i) computing the matrix geometric mean
(Riemannian centroid); and (ii) computing the Bures-Wasserstein barycenter.
Both tasks involve geodesically convex interval constraints, for which we show
that the Riemannian ""linear"" oracle required by RFW admits a closed-form
solution; this result may be of independent interest. We further specialize RFW
to the special orthogonal group and show that here too, the Riemannian ""linear""
oracle can be solved in closed form. Here, we describe an application to the
synchronization of data matrices (Procrustes problem). We complement our
theoretical results with an empirical comparison of RFW against
state-of-the-art Riemannian optimization methods and observe that RFW performs
competitively on the task of computing Riemannian centroids.
",Riemannian Optimization Methods,constrained riemannian optimization
"  Generative Adversarial Network (GAN) and its variants exhibit
state-of-the-art performance in the class of generative models. To capture
higher-dimensional distributions, the common learning procedure requires high
computational complexity and a large number of parameters. The problem of
employing such massive framework arises when deploying it on a platform with
limited computational power such as mobile phones. In this paper, we present a
new generative adversarial framework by representing each layer as a tensor
structure connected by multilinear operations, aiming to reduce the number of
model parameters by a large factor while preserving the generative performance
and sample quality. To learn the model, we employ an efficient algorithm which
alternatively optimizes both discriminator and generator. Experimental outcomes
demonstrate that our model can achieve high compression rate for model
parameters up to $35$ times when compared to the original GAN for MNIST
dataset.
",Efficient GAN Architecture for Mobile Devices,efficient generative adversarial networks
"  Despite the success of sequence-to-sequence approaches in automatic speech
recognition (ASR) systems, the models still suffer from several problems,
mainly due to the mismatch between the training and inference conditions. In
the sequence-to-sequence architecture, the model is trained to predict the
grapheme of the current time-step given the input of speech signal and the
ground-truth grapheme history of the previous time-steps. However, it remains
unclear how well the model approximates real-world speech during inference.
Thus, generating the whole transcription from scratch based on previous
predictions is complicated and errors can propagate over time. Furthermore, the
model is optimized to maximize the likelihood of training data instead of error
rate evaluation metrics that actually quantify recognition quality. This paper
presents an alternative strategy for training sequence-to-sequence ASR models
by adopting the idea of reinforcement learning (RL). Unlike the standard
training scheme with maximum likelihood estimation, our proposed approach
utilizes the policy gradient algorithm. We can (1) sample the whole
transcription based on the model's prediction in the training process and (2)
directly optimize the model with negative Levenshtein distance as the reward.
Experimental results demonstrate that we significantly improved the performance
compared to a model trained only with maximum likelihood estimation.
",Reinforcement Learning in ASR,automatic speech recognition (asr)
"  Deep learning models require extensive architecture design exploration and
hyperparameter optimization to perform well on a given task. The exploration of
the model design space is often made by a human expert, and optimized using a
combination of grid search and search heuristics over a large space of possible
choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach
that has been proposed to automate architecture design. NAS has been
successfully applied to generate Neural Networks that rival the best
human-designed architectures. However, NAS requires sampling, constructing, and
training hundreds to thousands of models to achieve well-performing
architectures. This procedure needs to be executed from scratch for each new
task. The application of NAS to a wide set of tasks currently lacks a way to
transfer generalizable knowledge across tasks. In this paper, we present the
Multitask Neural Model Search (MNMS) controller. Our goal is to learn a
generalizable framework that can condition model construction on successful
model searches for previously seen tasks, thus significantly speeding up the
search for new tasks. We demonstrate that MNMS can conduct an automated
architecture search for multiple tasks simultaneously while still learning
well-performing, specialized models for each task. We then show that
pre-trained MNMS controllers can transfer learning to new tasks. By leveraging
knowledge from previous searches, we find that pre-trained MNMS models start
from a better location in the search space and reduce search time on unseen
tasks, while still discovering models that outperform published human-designed
models.
",Neural Architecture Search Optimization,neural architecture search
"  Generative source separation methods such as non-negative matrix
factorization (NMF) or auto-encoders, rely on the assumption of an output
probability density. Generative Adversarial Networks (GANs) can learn data
distributions without needing a parametric assumption on the output density. We
show on a speech source separation experiment that, a multi-layer perceptron
trained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders
trained with maximum likelihood, and variational auto-encoders in terms of
source to distortion ratio.
",Speech Source Separation with GANs,deep learning for audio processing
"  Nonnegative matrix factorization (NMF), a dimensionality reduction and factor
analysis method, is a special case in which factor matrices have low-rank
nonnegative constraints. Considering the stochastic learning in NMF, we
specifically address the multiplicative update (MU) rule, which is the most
popular, but which has slow convergence property. This present paper introduces
on the stochastic MU rule a variance-reduced technique of stochastic gradient.
Numerical comparisons suggest that our proposed algorithms robustly outperform
state-of-the-art algorithms across different synthetic and real-world datasets.
",Variance-Reduced Stochastic Multiplicative Update for NMF,nonnegative matrix factorization
"  Why and how that deep learning works well on different tasks remains a
mystery from a theoretical perspective. In this paper we draw a geometric
picture of the deep learning system by finding its analogies with two existing
geometric structures, the geometry of quantum computations and the geometry of
the diffeomorphic template matching. In this framework, we give the geometric
structures of different deep learning systems including convolutional neural
networks, residual networks, recursive neural networks, recurrent neural
networks and the equilibrium prapagation framework. We can also analysis the
relationship between the geometrical structures and their performance of
different networks in an algorithmic level so that the geometric framework may
guide the design of the structures and algorithms of deep learning systems.
",Geometric Framework of Deep Learning Systems,geometric framework for understanding deep learning
"  Generative Adversarial Networks (GANs) have become a popular method to learn
a probability model from data. In this paper, we aim to provide an
understanding of some of the basic issues surrounding GANs including their
formulation, generalization and stability on a simple benchmark where the data
has a high-dimensional Gaussian distribution. Even in this simple benchmark,
the GAN problem has not been well-understood as we observe that existing
state-of-the-art GAN architectures may fail to learn a proper generative
distribution owing to (1) stability issues (i.e., convergence to bad local
solutions or not converging at all), (2) approximation issues (i.e., having
improper global GAN optimizers caused by inappropriate GAN's loss functions),
and (3) generalizability issues (i.e., requiring large number of samples for
training). In this setup, we propose a GAN architecture which recovers the
maximum-likelihood solution and demonstrates fast generalization. Moreover, we
analyze global stability of different computational approaches for the proposed
GAN optimization and highlight their pros and cons. Finally, we outline an
extension of our model-based approach to design GANs in more complex setups
than the considered Gaussian benchmark.
",GANs for High-Dimensional Gaussian Data,generative adversarial networks (gans)
"  Extreme learning machine (ELM) is a new single hidden layer feedback neural
network. The weights of the input layer and the biases of neurons in hidden
layer are randomly generated, the weights of the output layer can be
analytically determined. ELM has been achieved good results for a large number
of classification tasks. In this paper, a new extreme learning machine called
rough extreme learning machine (RELM) was proposed. RELM uses rough set to
divide data into upper approximation set and lower approximation set, and the
two approximation sets are utilized to train upper approximation neurons and
lower approximation neurons. In addition, an attribute reduction is executed in
this algorithm to remove redundant attributes. The experimental results showed,
comparing with the comparison algorithms, RELM can get a better accuracy and
repeatability in most cases, RELM can not only maintain the advantages of fast
speed, but also effectively cope with the classification task for
high-dimensional data.
",Rough Extreme Learning Machine (RELM) for Classification,machine learning
"  Approximate dynamic programming algorithms, such as approximate value
iteration, have been successfully applied to many complex reinforcement
learning tasks, and a better approximate dynamic programming algorithm is
expected to further extend the applicability of reinforcement learning to
various tasks. In this paper we propose a new, robust dynamic programming
algorithm that unifies value iteration, advantage learning, and dynamic policy
programming. We call it generalized value iteration (GVI) and its approximated
version, approximate GVI (AGVI). We show AGVI's performance guarantee, which
includes performance guarantees for existing algorithms, as special cases. We
discuss theoretical weaknesses of existing algorithms, and explain the
advantages of AGVI. Numerical experiments in a simple environment support
theoretical arguments, and suggest that AGVI is a promising alternative to
previous algorithms.
",Reinforcement Learning Algorithms,reinforcement learning
"  This paper shows that a simple baseline based on a Bag-of-Words (BoW)
representation learns surprisingly good knowledge graph embeddings. By casting
knowledge base completion and question answering as supervised classification
problems, we observe that modeling co-occurences of entities and relations
leads to state-of-the-art performance with a training time of a few minutes
using the open sourced library fastText.
",Knowledge Graph Embeddings,knowledge graph embeddings
"  We present graph attention networks (GATs), novel neural network
architectures that operate on graph-structured data, leveraging masked
self-attentional layers to address the shortcomings of prior methods based on
graph convolutions or their approximations. By stacking layers in which nodes
are able to attend over their neighborhoods' features, we enable (implicitly)
specifying different weights to different nodes in a neighborhood, without
requiring any kind of costly matrix operation (such as inversion) or depending
on knowing the graph structure upfront. In this way, we address several key
challenges of spectral-based graph neural networks simultaneously, and make our
model readily applicable to inductive as well as transductive problems. Our GAT
models have achieved or matched state-of-the-art results across four
established transductive and inductive graph benchmarks: the Cora, Citeseer and
Pubmed citation network datasets, as well as a protein-protein interaction
dataset (wherein test graphs remain unseen during training).
",Graph Attention Networks,graph neural networks
"  We analyze the loss landscape and expressiveness of practical deep
convolutional neural networks (CNNs) with shared weights and max pooling
layers. We show that such CNNs produce linearly independent features at a
""wide"" layer which has more neurons than the number of training samples. This
condition holds e.g. for the VGG network. Furthermore, we provide for such wide
CNNs necessary and sufficient conditions for global minima with zero training
error. For the case where the wide layer is followed by a fully connected layer
we show that almost every critical point of the empirical loss is a global
minimum with zero training error. Our analysis suggests that both depth and
width are very important in deep learning. While depth brings more
representational power and allows the network to learn high level features,
width smoothes the optimization landscape of the loss function in the sense
that a sufficiently wide network has a well-behaved loss surface with almost no
bad local minima.
",Deep Learning Architecture Design,deep learning and optimization
"  Neural networks have shown great potential in many applications like speech
recognition, drug discovery, image classification, and object detection. Neural
network models are inspired by biological neural networks, but they are
optimized to perform machine learning tasks on digital computers. The proposed
work explores the possibilities of using living neural networks in vitro as
basic computational elements for machine learning applications. A new
supervised STDP-based learning algorithm is proposed in this work, which
considers neuron engineering constrains. A 74.7% accuracy is achieved on the
MNIST benchmark for handwritten digit recognition.
",Living Neural Networks for Machine Learning,biological neural networks for machine learning applications
"  Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, but understanding and explaining AI remain challenging. This
paper clarifies the connections between machine-learning algorithms to develop
AIs and the econometrics of dynamic structural models through the case studies
of three famous game AIs. Chess-playing Deep Blue is a calibrated value
function, whereas shogi-playing Bonanza is an estimated value function via
Rust's (1987) nested fixed-point method. AlphaGo's ""supervised-learning policy
network"" is a deep neural network implementation of Hotz and Miller's (1993)
conditional choice probability estimation; its ""reinforcement-learning value
network"" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional
choice simulation method. Relaxing these AIs' implicit econometric assumptions
would improve their structural interpretability.
",Interpreting AI Models through Econometrics,artificial intelligence and econometrics
"  This paper proposes a novel type of random forests called a denoising random
forests that are robust against noises contained in test samples. Such
noise-corrupted samples cause serious damage to the estimation performances of
random forests, since unexpected child nodes are often selected and the leaf
nodes that the input sample reaches are sometimes far from those for a clean
sample. Our main idea for tackling this problem originates from a binary
indicator vector that encodes a traversal path of a sample in the forest. Our
proposed method effectively employs this vector by introducing denoising
autoencoders into random forests. A denoising autoencoder can be trained with
indicator vectors produced from clean and noisy input samples, and non-leaf
nodes where incorrect decisions are made can be identified by comparing the
input and output of the trained denoising autoencoder. Multiple traversal paths
with respect to the nodes with incorrect decisions caused by the noises can
then be considered for the estimation.
",Robust Random Forests Against Noisy Data,machine learning
"  Stochastic gradient descent (SGD) is widely believed to perform implicit
regularization when used to train deep neural networks, but the precise manner
in which this occurs has thus far been elusive. We prove that SGD minimizes an
average potential over the posterior distribution of weights along with an
entropic regularization term. This potential is however not the original loss
function in general. So SGD does perform variational inference, but for a
different loss than the one used to compute the gradients. Even more
surprisingly, SGD does not even converge in the classical sense: we show that
the most likely trajectories of SGD for deep networks do not behave like
Brownian motion around critical points. Instead, they resemble closed loops
with deterministic components. We prove that such ""out-of-equilibrium"" behavior
is a consequence of highly non-isotropic gradient noise in SGD; the covariance
matrix of mini-batch gradients for deep networks has a rank as small as 1% of
its dimension. We provide extensive empirical validation of these claims,
proven in the appendix.
",SGD in Deep Learning,stochastic gradient descent (sgd) and variational inference
"  In spite of the recent success of neural machine translation (NMT) in
standard benchmarks, the lack of large parallel corpora poses a major practical
problem for many language pairs. There have been several proposals to alleviate
this issue with, for instance, triangulation and semi-supervised learning
techniques, but they still require a strong cross-lingual signal. In this work,
we completely remove the need of parallel data and propose a novel method to
train an NMT system in a completely unsupervised manner, relying on nothing but
monolingual corpora. Our model builds upon the recent work on unsupervised
embedding mappings, and consists of a slightly modified attentional
encoder-decoder model that can be trained on monolingual corpora alone using a
combination of denoising and backtranslation. Despite the simplicity of the
approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014
French-to-English and German-to-English translation. The model can also profit
from small parallel corpora, and attains 21.81 and 15.24 points when combined
with 100,000 parallel sentences, respectively. Our implementation is released
as an open source project.
",Unsupervised Neural Machine Translation,unsupervised neural machine translation
"  Two of the most popular modelling paradigms in computer vision are
feed-forward neural networks (FFNs) and probabilistic graphical models (GMs).
Various connections between the two have been studied in recent works, such as
e.g. expressing mean-field based inference in a GM as an FFN. This paper
establishes a new connection between FFNs and GMs. Our key observation is that
any FFN implements a certain approximation of a corresponding Bayesian network
(BN). We characterize various benefits of having this connection. In
particular, it results in a new learning algorithm for BNs. We validate the
proposed methods for a classification problem on CIFAR-10 dataset and for
binary image segmentation on Weizmann Horse dataset. We show that statistically
learned BNs improve performance, having at the same time essentially better
generalization capability, than their FFN counterparts.
",Connection between Feed-Forward Neural Networks and Bayesian Networks,connection between feed-forward neural networks and probabilistic graphical models
"  In this paper we study the frequentist convergence rate for the Latent
Dirichlet Allocation (Blei et al., 2003) topic models. We show that the maximum
likelihood estimator converges to one of the finitely many equivalent
parameters in Wasserstein's distance metric at a rate of $n^{-1/4}$ without
assuming separability or non-degeneracy of the underlying topics and/or the
existence of more than three words per document, thus generalizing the previous
works of Anandkumar et al. (2012, 2014) from an information-theoretical
perspective. We also show that the $n^{-1/4}$ convergence rate is optimal in
the worst case.
",Convergence Rate of Latent Dirichlet Allocation,latent dirichlet allocation (lda) convergence rate
"  Options in reinforcement learning allow agents to hierarchically decompose a
task into subtasks, having the potential to speed up learning and planning.
However, autonomously learning effective sets of options is still a major
challenge in the field. In this paper we focus on the recently introduced idea
of using representation learning methods to guide the option discovery process.
Specifically, we look at eigenoptions, options obtained from representations
that encode diffusive information flow in the environment. We extend the
existing algorithms for eigenoption discovery to settings with stochastic
transitions and in which handcrafted features are not available. We propose an
algorithm that discovers eigenoptions while learning non-linear state
representations from raw pixels. It exploits recent successes in the deep
reinforcement learning literature and the equivalence between proto-value
functions and the successor representation. We use traditional tabular domains
to provide intuition about our approach and Atari 2600 games to demonstrate its
potential.
",Eigenoption Discovery in Reinforcement Learning,reinforcement learning
"  In this paper, we discussed limitation of current
electronic-design-automoation (EDA) tool and proposed a machine learning
framework to overcome the limitations and achieve better design quality. We
explored how to efficiently extract relevant features and leverage gradient
boost regressor (GBR) model to predict underestimated risky net (URN).
Customized routing optimizations are applied to the URNs and results show clear
timing improvement and trend to converge toward timing closure.
",EDA Tool Limitations and Machine Learning Solutions,electronics design automation (eda) with machine learning
"  In cities with tall buildings, emergency responders need an accurate floor
level location to find 911 callers quickly. We introduce a system to estimate a
victim's floor level via their mobile device's sensor data in a two-step
process. First, we train a neural network to determine when a smartphone enters
or exits a building via GPS signal changes. Second, we use a barometer equipped
smartphone to measure the change in barometric pressure from the entrance of
the building to the victim's indoor location. Unlike impractical previous
approaches, our system is the first that does not require the use of beacons,
prior knowledge of the building infrastructure, or knowledge of user behavior.
We demonstrate real-world feasibility through 63 experiments across five
different tall buildings throughout New York City where our system predicted
the correct floor level with 100% accuracy.
",Emergency Floor Localization Systems,floor level estimation using mobile device sensors
"  We advance the state of the art in polyphonic piano music transcription by
using a deep convolutional and recurrent neural network which is trained to
jointly predict onsets and frames. Our model predicts pitch onset events and
then uses those predictions to condition framewise pitch predictions. During
inference, we restrict the predictions from the framewise detector by not
allowing a new note to start unless the onset detector also agrees that an
onset for that pitch is present in the frame. We focus on improving onsets and
offsets together instead of either in isolation as we believe this correlates
better with human musical perception. Our approach results in over a 100%
relative improvement in note F1 score (with offsets) on the MAPS dataset.
Furthermore, we extend the model to predict relative velocities of normalized
audio which results in more natural-sounding transcriptions.
",Polyphonic Piano Music Transcription using Deep Neural Networks,deep learning for music transcription
"  We introduce a new deep convolutional neural network, CrescendoNet, by
stacking simple building blocks without residual connections. Each Crescendo
block contains independent convolution paths with increased depths. The numbers
of convolution layers and parameters are only increased linearly in Crescendo
blocks. In experiments, CrescendoNet with only 15 layers outperforms almost all
networks without residual connections on benchmark datasets, CIFAR10, CIFAR100,
and SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with
15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250
layers and 15.3M parameters. CrescendoNet provides a new way to construct high
performance deep convolutional neural networks without residual connections.
Moreover, through investigating the behavior and performance of subnetworks in
CrescendoNet, we note that the high performance of CrescendoNet may come from
its implicit ensemble behavior, which differs from the FractalNet that is also
a deep convolutional neural network without residual connections. Furthermore,
the independence between paths in CrescendoNet allows us to introduce a new
path-wise training procedure, which can reduce the memory needed for training.
",Deep Convolutional Neural Networks without Residual Connections,deep convolutional neural networks without residual connections
"  Policy gradient methods have achieved remarkable successes in solving
challenging reinforcement learning problems. However, it still often suffers
from the large variance issue on policy gradient estimation, which leads to
poor sample efficiency during training. In this work, we propose a control
variate method to effectively reduce variance for policy gradient methods.
Motivated by the Stein's identity, our method extends the previous control
variate methods used in REINFORCE and advantage actor-critic by introducing
more general action-dependent baseline functions. Empirical studies show that
our method significantly improves the sample efficiency of the state-of-the-art
policy gradient approaches.
",Variance Reduction in Policy Gradient Methods,reinforcement learning
"  Due to the success of deep learning to solving a variety of challenging
machine learning tasks, there is a rising interest in understanding loss
functions for training neural networks from a theoretical aspect. Particularly,
the properties of critical points and the landscape around them are of
importance to determine the convergence performance of optimization algorithms.
In this paper, we provide full (necessary and sufficient) characterization of
the analytical forms for the critical points (as well as global minimizers) of
the square loss functions for various neural networks. We show that the
analytical forms of the critical points characterize the values of the
corresponding loss functions as well as the necessary and sufficient conditions
to achieve global minimum. Furthermore, we exploit the analytical forms of the
critical points to characterize the landscape properties for the loss functions
of these neural networks. One particular conclusion is that: The loss function
of linear networks has no spurious local minimum, while the loss function of
one-hidden-layer nonlinear networks with ReLU activation function does have
local minimum that is not global minimum.
",Critical Points of Neural Network Loss Functions,neural network loss functions
"  Recommendation systems are ubiquitous and impact many domains; they have the
potential to influence product consumption, individuals' perceptions of the
world, and life-altering decisions. These systems are often evaluated or
trained with data from users already exposed to algorithmic recommendations;
this creates a pernicious feedback loop. Using simulations, we demonstrate how
using data confounded in this way homogenizes user behavior without increasing
utility.
",Biased Recommendation Systems,bias in recommendation systems
"  We focus on the problem of estimating the change in the dependency structures
of two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for
sparse change estimation in GGMs involve expensive and difficult non-smooth
optimization. We propose a novel method, DIFFEE for estimating DIFFerential
networks via an Elementary Estimator under a high-dimensional situation. DIFFEE
is solved through a faster and closed form solution that enables it to work in
large-scale settings. We conduct a rigorous statistical analysis showing that
surprisingly DIFFEE achieves the same asymptotic convergence rates as the
state-of-the-art estimators that are much more difficult to compute. Our
experimental results on multiple synthetic datasets and one real-world data
about brain connectivity show strong performance improvements over baselines,
as well as significant computational benefits.
",Gaussian Graphical Models Change Estimation,gaussian graphical model change estimation
"  One of the fundamental tasks in understanding genomics is the problem of
predicting Transcription Factor Binding Sites (TFBSs). With more than hundreds
of Transcription Factors (TFs) as labels, genomic-sequence based TFBS
prediction is a challenging multi-label classification task. There are two
major biological mechanisms for TF binding: (1) sequence-specific binding
patterns on genomes known as ""motifs"" and (2) interactions among TFs known as
co-binding effects. In this paper, we propose a novel deep architecture, the
Prototype Matching Network (PMN) to mimic the TF binding mechanisms. Our PMN
model automatically extracts prototypes (""motif""-like features) for each TF
through a novel prototype-matching loss. Borrowing ideas from few-shot matching
models, we use the notion of support set of prototypes and an LSTM to learn how
TFs interact and bind to genomic sequences. On a reference TFBS dataset with
$2.1$ $million$ genomic sequences, PMN significantly outperforms baselines and
validates our design choices empirically. To our knowledge, this is the first
deep learning architecture that introduces prototype learning and considers
TF-TF interactions for large-scale TFBS prediction. Not only is the proposed
architecture accurate, but it also models the underlying biology.
",Transcription Factor Binding Site Prediction,transcription factor binding site prediction
"  Inspired by the success of deep learning techniques in the physical and
chemical sciences, we apply a modification of an autoencoder type deep neural
network to the task of dimension reduction of molecular dynamics data. We can
show that our time-lagged autoencoder reliably finds low-dimensional embeddings
for high-dimensional feature spaces which capture the slow dynamics of the
underlying stochastic processes - beyond the capabilities of linear dimension
reduction techniques.
",Molecular Dynamics Dimension Reduction,dimensionality reduction in molecular dynamics data
"  In this paper, we study the problem of optimizing a two-layer artificial
neural network that best fits a training dataset. We look at this problem in
the setting where the number of parameters is greater than the number of
sampled points. We show that for a wide class of differentiable activation
functions (this class involves ""almost"" all functions which are not piecewise
linear), we have that first-order optimal solutions satisfy global optimality
provided the hidden layer is non-singular. Our results are easily extended to
hidden layers given by a flat matrix from that of a square matrix. Results are
applicable even if network has more than one hidden layer provided all hidden
layers satisfy non-singularity, all activations are from the given ""good"" class
of differentiable functions and optimization is only with respect to the last
hidden layer. We also study the smoothness properties of the objective function
and show that it is actually Lipschitz smooth, i.e., its gradients do not
change sharply. We use smoothness properties to guarantee asymptotic
convergence of O(1/number of iterations) to a first-order optimal solution. We
also show that our algorithm will maintain non-singularity of hidden layer for
any finite number of iterations.
",Neural Network Optimization,optimization of two-layer artificial neural networks
"  Reinforcement learning provides a powerful and general framework for decision
making and control, but its application in practice is often hindered by the
need for extensive feature and reward engineering. Deep reinforcement learning
methods can remove the need for explicit engineering of policy or value
features, but still require a manually specified reward function. Inverse
reinforcement learning holds the promise of automatic reward acquisition, but
has proven exceptionally difficult to apply to large, high-dimensional problems
with unknown dynamics. In this work, we propose adverserial inverse
reinforcement learning (AIRL), a practical and scalable inverse reinforcement
learning algorithm based on an adversarial reward learning formulation. We
demonstrate that AIRL is able to recover reward functions that are robust to
changes in dynamics, enabling us to learn policies even under significant
variation in the environment seen during training. Our experiments show that
AIRL greatly outperforms prior methods in these transfer settings.
",Adversarial Inverse Reinforcement Learning,reinforcement learning
"  We study the $\ell_0$-Low Rank Approximation Problem, where the goal is,
given an $m \times n$ matrix $A$, to output a rank-$k$ matrix $A'$ for which
$\|A'-A\|_0$ is minimized. Here, for a matrix $B$, $\|B\|_0$ denotes the number
of its non-zero entries. This NP-hard variant of low rank approximation is
natural for problems with no underlying metric, and its goal is to minimize the
number of disagreeing data positions. We provide approximation algorithms which
significantly improve the running time and approximation factor of previous
work. For $k > 1$, we show how to find, in poly$(mn)$ time for every $k$, a
rank $O(k \log(n/k))$ matrix $A'$ for which $\|A'-A\|_0 \leq O(k^2 \log(n/k))
\mathrm{OPT}$. To the best of our knowledge, this is the first algorithm with
provable guarantees for the $\ell_0$-Low Rank Approximation Problem for $k >
1$, even for bicriteria algorithms. For the well-studied case when $k = 1$, we
give a $(2+\epsilon)$-approximation in {\it sublinear time}, which is
impossible for other variants of low rank approximation such as for the
Frobenius norm. We strengthen this for the well-studied case of binary matrices
to obtain a $(1+O(\psi))$-approximation in sublinear time, where $\psi =
\mathrm{OPT}/\lVert A\rVert_0$. For small $\psi$, our approximation factor is
$1+o(1)$.
",Low Rank Matrix Approximation Algorithms,low rank approximation algorithms
"  We provide an overview of several non-linear activation functions in a neural
network architecture that have proven successful in many machine learning
applications. We conduct an empirical analysis on the effectiveness of using
these function on the MNIST classification task, with the aim of clarifying
which functions produce the best results overall. Based on this first set of
results, we examine the effects of building deeper architectures with an
increasing number of hidden layers. We also survey the impact of using, on the
same task, different initialisation schemes for the weights of our neural
network. Using these sets of experiments as a base, we conclude by providing a
optimal neural network architecture that yields impressive results in accuracy
on the MNIST classification task.
",Evaluating Non-Linear Activation Functions in Neural Networks,neural network architecture and activation functions
"  This paper presents a new method --- adversarial advantage actor-critic
(Adversarial A2C), which significantly improves the efficiency of dialogue
policy learning in task-completion dialogue systems. Inspired by generative
adversarial networks (GAN), we train a discriminator to differentiate
responses/actions generated by dialogue agents from responses/actions by
experts. Then, we incorporate the discriminator as another critic into the
advantage actor-critic (A2C) framework, to encourage the dialogue agent to
explore state-action within the regions where the agent takes actions similar
to those of the experts. Experimental results in a movie-ticket booking domain
show that the proposed Adversarial A2C can accelerate policy exploration
efficiently.
",Dialogue Policy Learning,dialogue policy learning
"  This article concerns the expressive power of depth in deep feed-forward
neural nets with ReLU activations. Specifically, we answer the following
question: for a fixed $d_{in}\geq 1,$ what is the minimal width $w$ so that
neural nets with ReLU activations, input dimension $d_{in}$, hidden layer
widths at most $w,$ and arbitrary depth can approximate any continuous,
real-valued function of $d_{in}$ variables arbitrarily well? It turns out that
this minimal width is exactly equal to $d_{in}+1.$ That is, if all the hidden
layer widths are bounded by $d_{in}$, then even in the infinite depth limit,
ReLU nets can only express a very limited class of functions, and, on the other
hand, any continuous function on the $d_{in}$-dimensional unit cube can be
approximated to arbitrary precision by ReLU nets in which all hidden layers
have width exactly $d_{in}+1.$ Our construction in fact shows that any
continuous function $f:[0,1]^{d_{in}}\to\mathbb R^{d_{out}}$ can be
approximated by a net of width $d_{in}+d_{out}$. We obtain quantitative depth
estimates for such an approximation in terms of the modulus of continuity of
$f$.
",Minimal Width of Neural Networks,expressive power of deep neural networks
"  We study the problem of identifying a probability distribution for some given
randomly sampled data in the limit, in the context of algorithmic learning
theory as proposed recently by Vinanyi and Chater. We show that there exists a
computable partial learner for the computable probability measures, while by
Bienvenu, Monin and Shen it is known that there is no computable learner for
the computable probability measures. Our main result is the characterization of
the oracles that compute explanatory learners for the computable (continuous)
probability measures as the high oracles. This provides an analogue of a
well-known result of Adleman and Blum in the context of learning computable
probability distributions. We also discuss related learning notions such as
behaviorally correct learning and orther variations of explanatory learning, in
the context of learning probability distributions from data.
",Algorithmic Learning of Probability Distributions,computational learning theory
"  We study rank-1 {L1-norm-based TUCKER2} (L1-TUCKER2) decomposition of 3-way
tensors, treated as a collection of $N$ $D \times M$ matrices that are to be
jointly decomposed. Our contributions are as follows. i) We prove that the
problem is equivalent to combinatorial optimization over $N$ antipodal-binary
variables. ii) We derive the first two algorithms in the literature for its
exact solution. The first algorithm has cost exponential in $N$; the second one
has cost polynomial in $N$ (under a mild assumption). Our algorithms are
accompanied by formal complexity analysis. iii) We conduct numerical studies to
compare the performance of exact L1-TUCKER2 (proposed) with standard HOSVD,
HOOI, GLRAM, PCA, L1-PCA, and TPCA-L1. Our studies show that L1-TUCKER2
outperforms (in tensor approximation) all the above counterparts when the
processed data are outlier corrupted.
",Tensor Decomposition,rank-1 l1-norm-based tucker2 decomposition of 3-way tensors
"  We consider the problems of learning forward models that map state to
high-dimensional images and inverse models that map high-dimensional images to
state in robotics. Specifically, we present a perceptual model for generating
video frames from state with deep networks, and provide a framework for its use
in tracking and prediction tasks. We show that our proposed model greatly
outperforms standard deconvolutional methods and GANs for image generation,
producing clear, photo-realistic images. We also develop a convolutional neural
network model for state estimation and compare the result to an Extended Kalman
Filter to estimate robot trajectories. We validate all models on a real robotic
system.
",Robot State Estimation from Images,robotics: deep learning for state estimation and image generation
"  Deep learning has been successfully applied to various tasks, but its
underlying mechanism remains unclear. Neural networks associate similar inputs
in the visible layer to the same state of hidden variables in deep layers. The
fraction of inputs that are associated to the same state is a natural measure
of similarity and is simply related to the cost in bits required to represent
these inputs. The degeneracy of states with the same information cost provides
instead a natural measure of noise and is simply related the entropy of the
frequency of states, that we call relevance. Representations with minimal
noise, at a given level of similarity (resolution), are those that maximise the
relevance. A signature of such efficient representations is that frequency
distributions follow power laws. We show, in extensive numerical experiments,
that deep neural networks extract a hierarchy of efficient representations from
data, because they i) achieve low levels of noise (i.e. high relevance) and ii)
exhibit power law distributions. We also find that the layer that is most
efficient to reliably generate patterns of training data is the one for which
relevance and resolution are traded at the same price, which implies that
frequency distribution follows Zipf's law.
",Measuring Noise and Similarity in Deep Neural Networks,neural network representation and efficiency
"  Due to their complex nature, it is hard to characterize the ways in which
machine learning models can misbehave or be exploited when deployed. Recent
work on adversarial examples, i.e. inputs with minor perturbations that result
in substantially different model predictions, is helpful in evaluating the
robustness of these models by exposing the adversarial scenarios where they
fail. However, these malicious perturbations are often unnatural, not
semantically meaningful, and not applicable to complicated domains such as
language. In this paper, we propose a framework to generate natural and legible
adversarial examples that lie on the data manifold, by searching in semantic
space of dense and continuous data representation, utilizing the recent
advances in generative adversarial networks. We present generated adversaries
to demonstrate the potential of the proposed approach for black-box classifiers
for a wide range of applications such as image classification, textual
entailment, and machine translation. We include experiments to show that the
generated adversaries are natural, legible to humans, and useful in evaluating
and analyzing black-box classifiers.
",Generating Natural Adversarial Examples,machine learning model adversarial examples
"  Low-rank tensor regression, a new model class that learns high-order
correlation from data, has recently received considerable attention. At the
same time, Gaussian processes (GP) are well-studied machine learning models for
structure learning. In this paper, we demonstrate interesting connections
between the two, especially for multi-way data analysis. We show that low-rank
tensor regression is essentially learning a multi-linear kernel in Gaussian
processes, and the low-rank assumption translates to the constrained Bayesian
inference problem. We prove the oracle inequality and derive the average case
learning curve for the equivalent GP model. Our finding implies that low-rank
tensor regression, though empirically successful, is highly dependent on the
eigenvalues of covariance functions as well as variable correlations.
",Low-Rank Tensor Regression and Gaussian Processes,connections between low-rank tensor regression and gaussian processes
"  One of the keys for deep learning to have made a breakthrough in various
fields was to utilize high computing powers centering around GPUs. Enabling the
use of further computing abilities by distributed processing is essential not
only to make the deep learning bigger and faster but also to tackle unsolved
challenges. We present the design, implementation, and evaluation of ChainerMN,
the distributed deep learning framework we have developed. We demonstrate that
ChainerMN can scale the learning process of the ResNet-50 model to the ImageNet
dataset up to 128 GPUs with the parallel efficiency of 90%.
",Distributed Deep Learning Frameworks,distributed deep learning
"  In implicit models, one often interpolates between sampled points in latent
space. As we show in this paper, care needs to be taken to match-up the
distributional assumptions on code vectors with the geometry of the
interpolating paths. Otherwise, typical assumptions about the quality and
semantics of in-between points may not be justified. Based on our analysis we
propose to modify the prior code distribution to put significantly more
probability mass closer to the origin. As a result, linear interpolation paths
are not only shortest paths, but they are also guaranteed to pass through
high-density regions, irrespective of the dimensionality of the latent space.
Experiments on standard benchmark image datasets demonstrate clear visual
improvements in the quality of the generated samples and exhibit more
meaningful interpolation paths.
",Latent Space Interpolation,latent space interpolation
"  We consider the problem of training generative models with deep neural
networks as generators, i.e. to map latent codes to data points. Whereas the
dominant paradigm combines simple priors over codes with complex deterministic
models, we argue that it might be advantageous to use more flexible code
distributions. We demonstrate how these distributions can be induced directly
from the data. The benefits include: more powerful generative models, better
modeling of latent structure and explicit control of the degree of
generalization.
",Flexible Code Distributions in Generative Models,generative models
"  It is commonly agreed that the use of relevant invariances as a good
statistical bias is important in machine-learning. However, most approaches
that explicitly incorporate invariances into a model architecture only make use
of very simple transformations, such as translations and rotations. Hence,
there is a need for methods to model and extract richer transformations that
capture much higher-level invariances. To that end, we introduce a tool
allowing to parametrize the set of filters of a trained convolutional neural
network with the latent space of a generative adversarial network. We then show
that the method can capture highly non-linear invariances of the data by
visualizing their effect in the data space.
",Invariant Representation Learning in CNNs,machine learning: modeling and extracting high-level invariances
"  Combining deep model-free reinforcement learning with on-line planning is a
promising approach to building on the successes of deep RL. On-line planning
with look-ahead trees has proven successful in environments where transition
models are known a priori. However, in complex environments where transition
models need to be learned from data, the deficiencies of learned models have
limited their utility for planning. To address these challenges, we propose
TreeQN, a differentiable, recursive, tree-structured model that serves as a
drop-in replacement for any value function network in deep RL with discrete
actions. TreeQN dynamically constructs a tree by recursively applying a
transition model in a learned abstract state space and then aggregating
predicted rewards and state-values using a tree backup to estimate Q-values. We
also propose ATreeC, an actor-critic variant that augments TreeQN with a
softmax layer to form a stochastic policy network. Both approaches are trained
end-to-end, such that the learned model is optimised for its actual use in the
tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a
box-pushing task, as well as n-step DQN and value prediction networks (Oh et
al. 2017) on multiple Atari games. Furthermore, we present ablation studies
that demonstrate the effect of different auxiliary losses on learning
transition models.
",Tree-based Reinforcement Learning,reinforcement learning
"  Deep reinforcement learning algorithms that estimate state and state-action
value functions have been shown to be effective in a variety of challenging
domains, including learning control strategies from raw image pixels. However,
algorithms that estimate state and state-action value functions typically
assume a fully observed state and must compensate for partial observations by
using finite length observation histories or recurrent networks. In this work,
we propose a new deep reinforcement learning algorithm based on counterfactual
regret minimization that iteratively updates an approximation to an
advantage-like function and is robust to partially observed state. We
demonstrate that this new algorithm can substantially outperform strong
baseline methods on several partially observed reinforcement learning tasks:
learning first-person 3D navigation in Doom and Minecraft, and acting in the
presence of partially observed objects in Doom and Pong.
",Partially Observed Reinforcement Learning,deep reinforcement learning
"  Separating two sources from an audio mixture is an important task with many
applications. It is a challenging problem since only one signal channel is
available for analysis. In this paper, we propose a novel framework for singing
voice separation using the generative adversarial network (GAN) with a
time-frequency masking function. The mixture spectra is considered to be a
distribution and is mapped to the clean spectra which is also considered a
distribtution. The approximation of distributions between mixture spectra and
clean spectra is performed during the adversarial training process. In contrast
with current deep learning approaches for source separation, the parameters of
the proposed framework are first initialized in a supervised setting and then
optimized by the training procedure of GAN in an unsupervised setting.
Experimental results on three datasets (MIR-1K, iKala and DSD100) show that
performance can be improved by the proposed framework consisting of
conventional networks.
",Singing Voice Separation,singing voice separation
"  This paper introduces a framework for combining scientific knowledge of
physics-based models with neural networks to advance scientific discovery. This
framework, termed physics-guided neural networks (PGNN), leverages the output
of physics-based model simulations along with observational features in a
hybrid modeling setup to generate predictions using a neural network
architecture. Further, this framework uses physics-based loss functions in the
learning objective of neural networks to ensure that the model predictions not
only show lower errors on the training set but are also scientifically
consistent with the known physics on the unlabeled set. We illustrate the
effectiveness of PGNN for the problem of lake temperature modeling, where
physical relationships between the temperature, density, and depth of water are
used to design a physics-based loss function. By using scientific knowledge to
guide the construction and learning of neural networks, we are able to show
that the proposed framework ensures better generalizability as well as
scientific consistency of results. All the code and datasets used in this study
have been made available on this link \url{https://github.com/arkadaw9/PGNN}.
",Physics-Guided Neural Networks,physics-guided machine learning
"  Cognitive neuroscience is enjoying rapid increase in extensive public
brain-imaging datasets. It opens the door to large-scale statistical models.
Finding a unified perspective for all available data calls for scalable and
automated solutions to an old challenge: how to aggregate heterogeneous
information on brain function into a universal cognitive system that relates
mental operations/cognitive processes/psychological tasks to brain networks? We
cast this challenge in a machine-learning approach to predict conditions from
statistical brain maps across different studies. For this, we leverage
multi-task learning and multi-scale dimension reduction to learn
low-dimensional representations of brain images that carry cognitive
information and can be robustly associated with psychological stimuli. Our
multi-dataset classification model achieves the best prediction performance on
several large reference datasets, compared to models without cognitive-aware
low-dimension representations, it brings a substantial performance boost to the
analysis of small datasets, and can be introspected to identify universal
template cognitive concepts.
",Integrating Brain Function Data with Cognitive Processes,neuroimaging-based cognitive neuroscience
"  This paper presents a statistical method of single-channel speech enhancement
that uses a variational autoencoder (VAE) as a prior distribution on clean
speech. A standard approach to speech enhancement is to train a deep neural
network (DNN) to take noisy speech as input and output clean speech. Although
this supervised approach requires a very large amount of pair data for
training, it is not robust against unknown environments. Another approach is to
use non-negative matrix factorization (NMF) based on basis spectra trained on
clean speech in advance and those adapted to noise on the fly. This
semi-supervised approach, however, causes considerable signal distortion in
enhanced speech due to the unrealistic assumption that speech spectrograms are
linear combinations of the basis spectra. Replacing the poor linear generative
model of clean speech in NMF with a VAE---a powerful nonlinear deep generative
model---trained on clean speech, we formulate a unified probabilistic
generative model of noisy speech. Given noisy speech as observed data, we can
sample clean speech from its posterior distribution. The proposed method
outperformed the conventional DNN-based method in unseen noisy environments.
",Speech Enhancement with Variational Autoencoder,speech enhancement
"  When training a deep neural network for image classification, one can broadly
distinguish between two types of latent features of images that will drive the
classification. We can divide latent features into (i) ""core"" or ""conditionally
invariant"" features $X^\text{core}$ whose distribution $X^\text{core}\vert Y$,
conditional on the class $Y$, does not change substantially across domains and
(ii) ""style"" features $X^{\text{style}}$ whose distribution $X^{\text{style}}
\vert Y$ can change substantially across domains. Examples for style features
include position, rotation, image quality or brightness but also more complex
ones like hair color, image quality or posture for images of persons. Our goal
is to minimize a loss that is robust under changes in the distribution of these
style features. In contrast to previous work, we assume that the domain itself
is not observed and hence a latent variable.
  We do assume that we can sometimes observe a typically discrete identifier or
""$\mathrm{ID}$ variable"". In some applications we know, for example, that two
images show the same person, and $\mathrm{ID}$ then refers to the identity of
the person. The proposed method requires only a small fraction of images to
have $\mathrm{ID}$ information. We group observations if they share the same
class and identifier $(Y,\mathrm{ID})=(y,\mathrm{id})$ and penalize the
conditional variance of the prediction or the loss if we condition on
$(Y,\mathrm{ID})$. Using a causal framework, this conditional variance
regularization (CoRe) is shown to protect asymptotically against shifts in the
distribution of the style variables. Empirically, we show that the CoRe penalty
improves predictive accuracy substantially in settings where domain changes
occur in terms of image quality, brightness and color while we also look at
more complex changes such as changes in movement and posture.
",Domain Invariant Image Classification,domain adaptation in image classification
"  In deep neural networks with convolutional layers, each layer typically has
fixed-size/single-resolution receptive field (RF). Convolutional layers with a
large RF capture global information from the input features, while layers with
small RF size capture local details with high resolution from the input
features. In this work, we introduce novel deep multi-resolution fully
convolutional neural networks (MR-FCNN), where each layer has different RF
sizes to extract multi-resolution features that capture the global and local
details information from its input features. The proposed MR-FCNN is applied to
separate a target audio source from a mixture of many audio sources.
Experimental results show that using MR-FCNN improves the performance compared
to feedforward deep neural networks (DNNs) and single resolution deep fully
convolutional neural networks (FCNNs) on the audio source separation problem.
",Multi-Resolution Convolutional Neural Networks for Audio Source Separation,deep multi-resolution convolutional neural networks
"  A convergent algorithm for nonnegative matrix factorization with
orthogonality constraints imposed on both factors is proposed in this paper.
This factorization concept was first introduced by Ding et al. with intent to
further improve clustering capability of NMF. However, as the original
algorithm was developed based on multiplicative update rules, the convergence
of the algorithm cannot be guaranteed. In this paper, we utilize the technique
presented in our previous work to develop the algorithm and prove that it
converges to a stationary point inside the solution space.
",Convergent Algorithm for Nonnegative Matrix Factorization,machine learning
"  Gradient boosted decision trees are a popular machine learning technique, in
part because of their ability to give good accuracy with small models. We
describe two extensions to the standard tree boosting algorithm designed to
increase this advantage. The first improvement extends the boosting formalism
from scalar-valued trees to vector-valued trees. This allows individual trees
to be used as multiclass classifiers, rather than requiring one tree per class,
and drastically reduces the model size required for multiclass problems. We
also show that some other popular vector-valued gradient boosted trees
modifications fit into this formulation and can be easily obtained in our
implementation. The second extension, layer-by-layer boosting, takes smaller
steps in function space, which is empirically shown to lead to a faster
convergence and to a more compact ensemble. We have added both improvements to
the open-source TensorFlow Boosted trees (TFBT) package, and we demonstrate
their efficacy on a variety of multiclass datasets. We expect these extensions
will be of particular interest to boosted tree applications that require small
models, such as embedded devices, applications requiring fast inference, or
applications desiring more interpretable models.
",Tree Boosting Algorithm Extensions,machine learning
"  TF Boosted Trees (TFBT) is a new open-sourced frame-work for the distributed
training of gradient boosted trees. It is based on TensorFlow, and its
distinguishing features include a novel architecture, automatic loss
differentiation, layer-by-layer boosting that results in smaller ensembles and
faster prediction, principled multi-class handling, and a number of
regularization techniques to prevent overfitting.
",Distributed Gradient Boosting Frameworks,machine learning frameworks
"  As neural networks grow deeper and wider, learning networks with
hard-threshold activations is becoming increasingly important, both for network
quantization, which can drastically reduce time and energy requirements, and
for creating large integrated systems of deep networks, which may have
non-differentiable components and must avoid vanishing and exploding gradients
for effective learning. However, since gradient descent is not applicable to
hard-threshold functions, it is not clear how to learn networks of them in a
principled way. We address this problem by observing that setting targets for
hard-threshold hidden units in order to minimize loss is a discrete
optimization problem, and can be solved as such. The discrete optimization goal
is to find a set of targets such that each unit, including the output, has a
linearly separable problem to solve. Given these targets, the network
decomposes into individual perceptrons, which can then be learned with standard
convex approaches. Based on this, we develop a recursive mini-batch algorithm
for learning deep hard-threshold networks that includes the popular but poorly
justified straight-through estimator as a special case. Empirically, we show
that our algorithm improves classification accuracy in a number of settings,
including for AlexNet and ResNet-18 on ImageNet, when compared to the
straight-through estimator.
",Deep Learning with Hard-Threshold Activations,learning deep hard-threshold networks
"  Convolution Neural Network (CNN) has gained tremendous success in computer
vision tasks with its outstanding ability to capture the local latent features.
Recently, there has been an increasing interest in extending convolution
operations to the non-Euclidean geometry. Although various types of convolution
operations have been proposed for graphs or manifolds, their connections with
traditional convolution over grid-structured data are not well-understood. In
this paper, we show that depthwise separable convolution can be successfully
generalized for the unification of both graph-based and grid-based convolution
methods. Based on this insight we propose a novel Depthwise Separable Graph
Convolution (DSGC) approach which is compatible with the tradition convolution
network and subsumes existing convolution methods as special cases. It is
equipped with the combined strengths in model expressiveness, compatibility
(relatively small number of parameters), modularity and computational
efficiency in training. Extensive experiments show the outstanding performance
of DSGC in comparison with strong baselines on multi-domain benchmark datasets.
",Graph Convolutional Neural Networks,deep learning on non-euclidean data
"  We consider the problem of efficiently learning mixtures of a large number of
spherical Gaussians, when the components of the mixture are well separated. In
the most basic form of this problem, we are given samples from a uniform
mixture of $k$ standard spherical Gaussians, and the goal is to estimate the
means up to accuracy $\delta$ using $poly(k,d, 1/\delta)$ samples.
  In this work, we study the following question: what is the minimum separation
needed between the means for solving this task? The best known algorithm due to
Vempala and Wang [JCSS 2004] requires a separation of roughly
$\min\{k,d\}^{1/4}$. On the other hand, Moitra and Valiant [FOCS 2010] showed
that with separation $o(1)$, exponentially many samples are required. We
address the significant gap between these two bounds, by showing the following
results.
  1. We show that with separation $o(\sqrt{\log k})$, super-polynomially many
samples are required. In fact, this holds even when the $k$ means of the
Gaussians are picked at random in $d=O(\log k)$ dimensions.
  2. We show that with separation $\Omega(\sqrt{\log k})$, $poly(k,d,1/\delta)$
samples suffice. Note that the bound on the separation is independent of
$\delta$. This result is based on a new and efficient ""accuracy boosting""
algorithm that takes as input coarse estimates of the true means and in time
$poly(k,d, 1/\delta)$ outputs estimates of the means up to arbitrary accuracy
$\delta$ assuming the separation between the means is $\Omega(\min\{\sqrt{\log
k},\sqrt{d}\})$ (independently of $\delta$).
  We also present a computationally efficient algorithm in $d=O(1)$ dimensions
with only $\Omega(\sqrt{d})$ separation. These results together essentially
characterize the optimal order of separation between components that is needed
to learn a mixture of $k$ spherical Gaussians with polynomial samples.
",Learning Mixtures of Spherical Gaussians,efficient learning of mixture of spherical gaussians
"  Five simple soft sensor methodologies with two update conditions were
compared on two experimentally-obtained datasets and one simulated dataset. The
soft sensors investigated were moving window partial least squares regression
(and a recursive variant), moving window random forest regression, the mean
moving window of $y$, and a novel random forest partial least squares
regression ensemble (RF-PLS), all of which can be used with small sample sizes
so that they can be rapidly placed online. It was found that, on two of the
datasets studied, small window sizes led to the lowest prediction errors for
all of the moving window methods studied. On the majority of datasets studied,
the RF-PLS calibration method offered the lowest one-step-ahead prediction
errors compared to those of the other methods, and it demonstrated greater
predictive stability at larger time delays than moving window PLS alone. It was
found that both the random forest and RF-PLS methods most adequately modeled
the datasets that did not feature purely monotonic increases in property
values, but that both methods performed more poorly than moving window PLS
models on one dataset with purely monotonic property values. Other data
dependent findings are presented and discussed.
",Soft Sensor Methodologies Comparison,soft sensor calibration and prediction methods
"  Learning to learn is a powerful paradigm for enabling models to learn from
data more effectively and efficiently. A popular approach to meta-learning is
to train a recurrent model to read in a training dataset as input and output
the parameters of a learned model, or output predictions for new test inputs.
Alternatively, a more recent approach to meta-learning aims to acquire deep
representations that can be effectively fine-tuned, via standard gradient
descent, to new tasks. In this paper, we consider the meta-learning problem
from the perspective of universality, formalizing the notion of learning
algorithm approximation and comparing the expressive power of the
aforementioned recurrent models to the more recent approaches that embed
gradient descent into the meta-learner. In particular, we seek to answer the
following question: does deep representation combined with standard gradient
descent have sufficient capacity to approximate any learning algorithm? We find
that this is indeed true, and further find, in our experiments, that
gradient-based meta-learning consistently leads to learning strategies that
generalize more widely compared to those represented by recurrent models.
",Meta-Learning Universality,meta-learning and representation learning
"  We applied machine learning to predict whether a gene is involved in axon
regeneration. We extracted 31 features from different databases and trained
five machine learning models. Our optimal model, a Random Forest Classifier
with 50 submodels, yielded a test score of 85.71%, which is 4.1% higher than
the baseline score. We concluded that our models have some predictive
capability. Similar methodology and features could be applied to predict other
Gene Ontology (GO) terms.
",Gene Prediction Using Machine Learning,machine learning for gene annotation
"  Skip connections are increasingly utilized by deep neural networks to improve
accuracy and cost-efficiency. In particular, the recent DenseNet is efficient
in computation and parameters, and achieves state-of-the-art predictions by
directly connecting each feature layer to all previous ones. However,
DenseNet's extreme connectivity pattern may hinder its scalability to high
depths, and in applications like fully convolutional networks, full DenseNet
connections are prohibitively expensive. This work first experimentally shows
that one key advantage of skip connections is to have short distances among
feature layers during backpropagation. Specifically, using a fixed number of
skip connections, the connection patterns with shorter backpropagation distance
among layers have more accurate predictions. Following this insight, we propose
a connection template, Log-DenseNet, which, in comparison to DenseNet, only
slightly increases the backpropagation distances among layers from 1 to ($1 +
\log_2 L$), but uses only $L\log_2 L$ total connections instead of $O(L^2)$.
Hence, Log-DenseNets are easier than DenseNets to implement and to scale. We
demonstrate the effectiveness of our design principle by showing better
performance than DenseNets on tabula rasa semantic segmentation, and
competitive results on visual recognition.
",Neural Network Architectures,deep neural network architecture
"  Importance sampling (IS) as an elegant and efficient variance reduction (VR)
technique for the acceleration of stochastic optimization problems has
attracted many researches recently. Unlike commonly adopted stochastic uniform
sampling in stochastic optimizations, IS-integrated algorithms sample training
data at each iteration with respect to a weighted sampling probability
distribution $P$, which is constructed according to the precomputed importance
factors. Previous experimental results show that IS has achieved remarkable
progresses in the acceleration of training convergence. Unfortunately, the
calculation of the sampling probability distribution $P$ causes a major
limitation of IS: it requires the input data to be well-structured, i.e., the
feature vector is properly defined. Consequently, recurrent neural networks
(RNN) as a popular learning algorithm is not able to enjoy the benefits of IS
due to the fact that its raw input data, i.e., the training sequences, are
often unstructured which makes calculation of $P$ impossible. In considering of
the the popularity of RNN-based learning applications and their relative long
training time, we are interested in accelerating them through IS. This paper
propose a novel Fast-Importance-Mining algorithm to calculate the importance
factor for unstructured data which makes the application of IS in RNN-based
applications possible. Our experimental evaluation on popular open-source
RNN-based learning applications validate the effectiveness of IS in improving
the convergence rate of RNNs.
",Importance Sampling in Recurrent Neural Networks,importance sampling for accelerating recurrent neural networks
"  The state of the art in music source separation employs neural networks
trained in a supervised fashion on multi-track databases to estimate the
sources from a given mixture. With only few datasets available, often extensive
data augmentation is used to combat overfitting. Mixing random tracks, however,
can even reduce separation performance as instruments in real music are
strongly correlated. The key concept in our approach is that source estimates
of an optimal separator should be indistinguishable from real source signals.
Based on this idea, we drive the separator towards outputs deemed as realistic
by discriminator networks that are trained to tell apart real from separator
samples. This way, we can also use unpaired source and mixture recordings
without the drawbacks of creating unrealistic music mixtures. Our framework is
widely applicable as it does not assume a specific network architecture or
number of sources. To our knowledge, this is the first adoption of adversarial
training for music source separation. In a prototype experiment for singing
voice separation, separation performance increases with our approach compared
to purely supervised training.
",Music Source Separation Using Adversarial Training,music source separation
"  Image analysis using more than one modality (i.e. multi-modal) has been
increasingly applied in the field of biomedical imaging. One of the challenges
in performing the multimodal analysis is that there exist multiple schemes for
fusing the information from different modalities, where such schemes are
application-dependent and lack a unified framework to guide their designs. In
this work we firstly propose a conceptual architecture for the image fusion
schemes in supervised biomedical image analysis: fusing at the feature level,
fusing at the classifier level, and fusing at the decision-making level.
Further, motivated by the recent success in applying deep learning for natural
image analysis, we implement the three image fusion schemes above based on the
Convolutional Neural Network (CNN) with varied structures, and combined into a
single framework. The proposed image segmentation framework is capable of
analyzing the multi-modality images using different fusing schemes
simultaneously. The framework is applied to detect the presence of soft tissue
sarcoma from the combination of Magnetic Resonance Imaging (MRI), Computed
Tomography (CT) and Positron Emission Tomography (PET) images. It is found from
the results that while all the fusion schemes outperform the single-modality
schemes, fusing at the feature level can generally achieve the best performance
in terms of both accuracy and computational cost, but also suffers from the
decreased robustness in the presence of large errors in any image modalities.
",Multimodal Image Fusion in Biomedical Imaging,biomedical image fusion techniques
"  In classification problems, sampling bias between training data and testing
data is critical to the ranking performance of classification scores. Such bias
can be both unintentionally introduced by data collection and intentionally
introduced by the algorithm, such as under-sampling or weighting techniques
applied to imbalanced data. When such sampling bias exists, using the raw
classification score to rank observations in the testing data can lead to
suboptimal results. In this paper, I investigate the optimal calibration
strategy in general settings, and develop a practical solution for one specific
sampling bias case, where the sampling bias is introduced by stratified
sampling. The optimal solution is developed by analytically solving the problem
of optimizing the ROC curve. For practical data, I propose a ranking algorithm
for general classification models with stratified data. Numerical experiments
demonstrate that the proposed algorithm effectively addresses the stratified
sampling bias issue. Interestingly, the proposed method shows its potential
applicability in two other machine learning areas: unsupervised learning and
model ensembling, which can be future research topics.
",Calibration of Classification Scores under Sampling Bias,sampling bias mitigation in classification models
"  Recurrent neural networks (RNNs) are important class of architectures among
neural networks useful for language modeling and sequential prediction.
However, optimizing RNNs is known to be harder compared to feed-forward neural
networks. A number of techniques have been proposed in literature to address
this problem. In this paper we propose a simple technique called fraternal
dropout that takes advantage of dropout to achieve this goal. Specifically, we
propose to train two identical copies of an RNN (that share parameters) with
different dropout masks while minimizing the difference between their
(pre-softmax) predictions. In this way our regularization encourages the
representations of RNNs to be invariant to dropout mask, thus being robust. We
show that our regularization term is upper bounded by the expectation-linear
dropout objective which has been shown to address the gap due to the difference
between the train and inference phases of dropout. We evaluate our model and
achieve state-of-the-art results in sequence modeling tasks on two benchmark
datasets - Penn Treebank and Wikitext-2. We also show that our approach leads
to performance improvement by a significant margin in image captioning
(Microsoft COCO) and semi-supervised (CIFAR-10) tasks.
",Regularization Techniques for RNNs,recurrent neural networks
"  We present Higher-Order Tensor RNN (HOT-RNN), a novel family of neural
sequence architectures for multivariate forecasting in environments with
nonlinear dynamics. Long-term forecasting in such systems is highly
challenging, since there exist long-term temporal dependencies, higher-order
correlations and sensitivity to error propagation. Our proposed recurrent
architecture addresses these issues by learning the nonlinear dynamics directly
using higher-order moments and higher-order state transition functions.
Furthermore, we decompose the higher-order structure using the tensor-train
decomposition to reduce the number of parameters while preserving the model
performance. We theoretically establish the approximation guarantees and the
variance bound for HOT-RNN for general sequence inputs. We also demonstrate 5%
~ 12% improvements for long-term prediction over general RNN and LSTM
architectures on a range of simulated environments with nonlinear dynamics, as
well on real-world time series data.
",Higher-Order Tensor RNN for Multivariate Forecasting,neural network architectures for multivariate time series forecasting
"  Purpose: A new method for magnetic resonance (MR) imaging water-fat
separation using a convolutional neural network (ConvNet) and deep learning
(DL) is presented. Feasibility of the method with complex and magnitude images
is demonstrated with a series of patient studies and accuracy of predicted
quantitative values is analyzed.
  Methods: Water-fat separation of 1200 gradient-echo acquisitions from 90
imaging sessions (normal, acute and chronic myocardial infarction) was
performed using a conventional model based method with modeling of R2* and
off-resonance and a multi-peak fat spectrum. A U-Net convolutional neural
network for calculation of water-only, fat-only, R2* and off-resonance images
was trained with 900 gradient-echo Multiple and single-echo complex and
magnitude input data algorithms were studied and compared to conventional
extended echo modeling.
  Results: The U-Net ConvNet was easily trained and provided water-fat
separation results visually comparable to conventional methods. Myocardial fat
deposition in chronic myocardial infarction and intramyocardial hemorrhage in
acute myocardial infarction were well visualized in the DL results. Predicted
values for R2*, off-resonance, water and fat signal intensities were well
correlated with conventional model based water fat separation (R2>=0.97,
p<0.001). DL images had a 14% higher signal-to-noise ratio (p<0.001) when
compared to the conventional method.
  Conclusion: Deep learning utilizing ConvNets is a feasible method for MR
water-fat separationimaging with complex, magnitude and single echo image data.
A trained U-Net can be efficiently used for MR water-fat separation, providing
results comparable to conventional model based methods.
",MR Water-Fat Separation using Deep Learning,deep learning for magnetic resonance imaging water-fat separation
"  Existing deep multitask learning (MTL) approaches align layers shared between
tasks in a parallel ordering. Such an organization significantly constricts the
types of shared structure that can be learned. The necessity of parallel
ordering for deep MTL is first tested by comparing it with permuted ordering of
shared layers. The results indicate that a flexible ordering can enable more
effective sharing, thus motivating the development of a soft ordering approach,
which learns how shared layers are applied in different ways for different
tasks. Deep MTL with soft ordering outperforms parallel ordering methods across
a series of domains. These results suggest that the power of deep MTL comes
from learning highly general building blocks that can be assembled to meet the
demands of each task.
",Deep Multitask Learning with Flexible Layer Ordering,deep multitask learning with soft ordering
"  Gradient-based optimization is the foundation of deep learning and
reinforcement learning. Even when the mechanism being optimized is unknown or
not differentiable, optimization using high-variance or biased gradient
estimates is still often the best strategy. We introduce a general framework
for learning low-variance, unbiased gradient estimators for black-box functions
of random variables. Our method uses gradients of a neural network trained
jointly with model parameters or policies, and is applicable in both discrete
and continuous settings. We demonstrate this framework for training discrete
latent-variable models. We also give an unbiased, action-conditional extension
of the advantage actor-critic reinforcement learning algorithm.
",Unbiased Gradient Estimation in Deep Learning,reinforcement learning
"  State-of-the-art algorithms for sparse subspace clustering perform spectral
clustering on a similarity matrix typically obtained by representing each data
point as a sparse combination of other points using either basis pursuit (BP)
or orthogonal matching pursuit (OMP). BP-based methods are often prohibitive in
practice while the performance of OMP-based schemes are unsatisfactory,
especially in settings where data points are highly similar. In this paper, we
propose a novel algorithm that exploits an accelerated variant of orthogonal
least-squares to efficiently find the underlying subspaces. We show that under
certain conditions the proposed algorithm returns a subspace-preserving
solution. Simulation results illustrate that the proposed method compares
favorably with BP-based method in terms of running time while being
significantly more accurate than OMP-based schemes.
",Sparse Subspace Clustering Algorithms,sparse subspace clustering
"  We present pomegranate, an open source machine learning package for
probabilistic modeling in Python. Probabilistic modeling encompasses a wide
range of methods that explicitly describe uncertainty using probability
distributions. Three widely used probabilistic models implemented in
pomegranate are general mixture models, hidden Markov models, and Bayesian
networks. A primary focus of pomegranate is to abstract away the complexities
of training models from their definition. This allows users to focus on
specifying the correct model for their application instead of being limited by
their understanding of the underlying algorithms. An aspect of this focus
involves the collection of additive sufficient statistics from data sets as a
strategy for training models. This approach trivially enables many useful
learning strategies, such as out-of-core learning, minibatch learning, and
semi-supervised learning, without requiring the user to consider how to
partition data or modify the algorithms to handle these tasks themselves.
pomegranate is written in Cython to speed up calculations and releases the
global interpreter lock to allow for built-in multithreaded parallelism, making
it competitive with---or outperform---other implementations of similar
algorithms. This paper presents an overview of the design choices in
pomegranate, and how they have enabled complex features to be supported by
simple code.
",Probabilistic Modeling Libraries,machine learning
"  We address the issue of limit cycling behavior in training Generative
Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for
training Wasserstein GANs. Recent theoretical results have shown that
optimistic mirror decent (OMD) can enjoy faster regret rates in the context of
zero-sum games. WGANs is exactly a context of solving a zero-sum game with
simultaneous no-regret dynamics. Moreover, we show that optimistic mirror
decent addresses the limit cycling problem in training WGANs. We formally show
that in the case of bi-linear zero-sum games the last iterate of OMD dynamics
converges to an equilibrium, in contrast to GD dynamics which are bound to
cycle. We also portray the huge qualitative difference between GD and OMD
dynamics with toy examples, even when GD is modified with many adaptations
proposed in the recent literature, such as gradient penalty or momentum. We
apply OMD WGAN training to a bioinformatics problem of generating DNA
sequences. We observe that models trained with OMD achieve consistently smaller
KL divergence with respect to the true underlying distribution, than models
trained with GD variants. Finally, we introduce a new algorithm, Optimistic
Adam, which is an optimistic variant of Adam. We apply it to WGAN training on
CIFAR10 and observe improved performance in terms of inception score as
compared to Adam.
",Optimistic Mirror Descent for Training Wasserstein GANs,optimistic mirror descent for training generative adversarial networks
"  We study the problem of sampling a bandlimited graph signal in the presence
of noise, where the objective is to select a node subset of prescribed
cardinality that minimizes the signal reconstruction mean squared error (MSE).
To that end, we formulate the task at hand as the minimization of MSE subject
to binary constraints, and approximate the resulting NP-hard problem via
semidefinite programming (SDP) relaxation. Moreover, we provide an alternative
formulation based on maximizing a monotone weak submodular function and propose
a randomized-greedy algorithm to find a sub-optimal subset. We then derive a
worst-case performance guarantee on the MSE returned by the randomized greedy
algorithm for general non-stationary graph signals. The efficacy of the
proposed methods is illustrated through numerical simulations on synthetic and
real-world graphs. Notably, the randomized greedy algorithm yields an
order-of-magnitude speedup over state-of-the-art greedy sampling schemes, while
incurring only a marginal MSE performance loss.
",Graph Signal Sampling,sampling and optimization
"  We propose an approach to Multitask Learning (MTL) to make deep learning
models faster and lighter for applications in which multiple tasks need to be
solved simultaneously, which is particularly useful in embedded, real-time
systems. We develop a multitask model for both Object Detection and Semantic
Segmentation and analyze the challenges that appear during its training. Our
multitask network is 1.6x faster, lighter and uses less memory than deploying
the single-task models in parallel. We conclude that MTL has the potential to
give superior performance in exchange of a more complex training process that
introduces challenges not present in single-task models.
",Multitask Learning for Real-time Systems,multitask learning for efficient deep learning models
"  It has long been known that a single-layer fully-connected neural network
with an i.i.d. prior over its parameters is equivalent to a Gaussian process
(GP), in the limit of infinite network width. This correspondence enables exact
Bayesian inference for infinite width neural networks on regression tasks by
means of evaluating the corresponding GP. Recently, kernel functions which
mimic multi-layer random neural networks have been developed, but only outside
of a Bayesian framework. As such, previous work has not identified that these
kernels can be used as covariance functions for GPs and allow fully Bayesian
prediction with a deep neural network.
  In this work, we derive the exact equivalence between infinitely wide deep
networks and GPs. We further develop a computationally efficient pipeline to
compute the covariance function for these GPs. We then use the resulting GPs to
perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10.
We observe that trained neural network accuracy approaches that of the
corresponding GP with increasing layer width, and that the GP uncertainty is
strongly correlated with trained network prediction error. We further find that
test performance increases as finite-width trained networks are made wider and
more similar to a GP, and thus that GP predictions typically outperform those
of finite-width networks. Finally we connect the performance of these GPs to
the recent theory of signal propagation in random neural networks.
",Neural Network Bayesian Inference,neural network equivalence to gaussian processes
"  This work targets the automated minimum-energy optimization of Quantized
Neural Networks (QNNs) - networks using low precision weights and activations.
These networks are trained from scratch at an arbitrary fixed point precision.
At iso-accuracy, QNNs using fewer bits require deeper and wider network
architectures than networks using higher precision operators, while they
require less complex arithmetic and less bits per weights. This fundamental
trade-off is analyzed and quantified to find the minimum energy QNN for any
benchmark and hence optimize energy-efficiency. To this end, the energy
consumption of inference is modeled for a generic hardware platform. This
allows drawing several conclusions across different benchmarks. First, energy
consumption varies orders of magnitude at iso-accuracy depending on the number
of bits used in the QNN. Second, in a typical system, BinaryNets or int4
implementations lead to the minimum energy solution, outperforming int8
networks up to 2-10x at iso-accuracy. All code used for QNN training is
available from https://github.com/BertMoons.
",Energy-Efficient Quantized Neural Networks,energy-efficient neural network optimization
"  This paper presents a novel variational inference framework for deriving a
family of Bayesian sparse Gaussian process regression (SGPR) models whose
approximations are variationally optimal with respect to the full-rank GPR
model enriched with various corresponding correlation structures of the
observation noises. Our variational Bayesian SGPR (VBSGPR) models jointly treat
both the distributions of the inducing variables and hyperparameters as
variational parameters, which enables the decomposability of the variational
lower bound that in turn can be exploited for stochastic optimization. Such a
stochastic optimization involves iteratively following the stochastic gradient
of the variational lower bound to improve its estimates of the optimal
variational distributions of the inducing variables and hyperparameters (and
hence the predictive distribution) of our VBSGPR models and is guaranteed to
achieve asymptotic convergence to them. We show that the stochastic gradient is
an unbiased estimator of the exact gradient and can be computed in constant
time per iteration, hence achieving scalability to big data. We empirically
evaluate the performance of our proposed framework on two real-world, massive
datasets.
",Bayesian Gaussian Process Regression,variational bayesian sparse gaussian process regression
"  With the evolution of data collection ways, it is possible to produce
abundant data described by multiple feature sets. Previous studies show that
including more features does not necessarily bring positive effect. How to
prevent the augmented features worsening classification performance is crucial
but rarely studied. In this paper, we study this challenging problem by
proposing a secure classification approach, whose accuracy is never degenerated
when exploiting augmented features. We propose two ways to achieve the security
of our method named as SEcure Classification (SEC). Firstly, to leverage
augmented features, we learn various types of classifiers and adapt them by
employing a specially designed robust loss. It provides various candidate
classifiers to meet the following assumption of security operation. Secondly,
we integrate all candidate classifiers by approximately maximizing the
performance improvement. Under a mild assumption, the integrated classifier has
theoretical security guarantee. Several new optimization methods have been
developed to accommodate the problems with proved convergence. Besides
evaluating SEC on 16 data sets, we also apply SEC in the application of
diagnostic classification of schizophrenia since it has vast application
potentiality. Experimental results demonstrate the effectiveness of SEC in both
tackling security problem and discriminating schizophrenic patients from
healthy controls.
",Feature Selection for Secure Classification,secure classification
"  Large number of weights in deep neural networks makes the models difficult to
be deployed in low memory environments such as, mobile phones, IOT edge devices
as well as ""inferencing as a service"" environments on cloud. Prior work has
considered reduction in the size of the models, through compression techniques
like pruning, quantization, Huffman encoding etc. However, efficient
inferencing using the compressed models has received little attention,
specially with the Huffman encoding in place. In this paper, we propose
efficient parallel algorithms for inferencing of single image and batches,
under various memory constraints. Our experimental results show that our
approach of using variable batch size for inferencing achieves 15-25\%
performance improvement in the inference throughput for AlexNet, while
maintaining memory and latency constraints.
",Efficient Inference in Compressed Deep Neural Networks,efficient neural network inference on resource-constrained devices
"  The recently proposed self-ensembling methods have achieved promising results
in deep semi-supervised learning, which penalize inconsistent predictions of
unlabeled data under different perturbations. However, they only consider
adding perturbations to each single data point, while ignoring the connections
between data samples. In this paper, we propose a novel method, called Smooth
Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on
the predictions of the teacher model, i.e., the implicit self-ensemble of
models. Then the graph serves as a similarity measure with respect to which the
representations of ""similar"" neighboring points are learned to be smooth on the
low-dimensional manifold. We achieve state-of-the-art results on
semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for
CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,
the improvements are significant when the labels are fewer. For the
non-augmented MNIST with only 20 labels, the error rate is reduced from
previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.
",Semi-supervised Learning with Graph-based Methods,deep learning
"  Understanding physical phenomena is a key component of human intelligence and
enables physical interaction with previously unseen environments. In this
paper, we study how an artificial agent can autonomously acquire this intuition
through interaction with the environment. We created a synthetic block stacking
environment with physics simulation in which the agent can learn a policy
end-to-end through trial and error. Thereby, we bypass to explicitly model
physical knowledge within the policy. We are specifically interested in tasks
that require the agent to reach a given goal state that may be different for
every new trial. To this end, we propose a deep reinforcement learning
framework that learns policies which are parametrized by a goal. We validated
the model on a toy example navigating in a grid world with different target
positions and in a block stacking task with different target structures of the
final tower. In contrast to prior work, our policies show better generalization
across different goals.
",Autonomous Learning of Physical Intuition,artificial intelligence for acquiring physical intuition
"  Training deep neural networks requires massive amounts of training data, but
for many tasks only limited labeled data is available. This makes weak
supervision attractive, using weak or noisy signals like the output of
heuristic methods or user click-through data for training. In a semi-supervised
setting, we can use a large set of data with weak labels to pretrain a neural
network and then fine-tune the parameters with a small amount of data with true
labels. This feels intuitively sub-optimal as these two independent stages
leave the model unaware about the varying label quality. What if we could
somehow inform the model about the label quality? In this paper, we propose a
semi-supervised learning method where we train two neural networks in a
multi-task fashion: a ""target network"" and a ""confidence network"". The target
network is optimized to perform a given task and is trained using a large set
of unlabeled data that are weakly annotated. We propose to weight the gradient
updates to the target network using the scores provided by the second
confidence network, which is trained on a small amount of supervised data. Thus
we avoid that the weight updates computed from noisy labels harm the quality of
the target network model. We evaluate our learning strategy on two different
tasks: document ranking and sentiment classification. The results demonstrate
that our approach not only enhances the performance compared to the baselines
but also speeds up the learning process from weak labels.
",Semi-supervised Learning with Weak Labels,semi-supervised learning with confidence network
"  Double machine learning provides $\sqrt{n}$-consistent estimates of
parameters of interest even when high-dimensional or nonparametric nuisance
parameters are estimated at an $n^{-1/4}$ rate. The key is to employ
Neyman-orthogonal moment equations which are first-order insensitive to
perturbations in the nuisance parameters. We show that the $n^{-1/4}$
requirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order
notion of orthogonality that grants robustness to more complex or
higher-dimensional nuisance parameters. In the partially linear regression
setting popular in causal inference, we show that we can construct second-order
orthogonal moments if and only if the treatment residual is not normally
distributed. Our proof relies on Stein's lemma and may be of independent
interest. We conclude by demonstrating the robustness benefits of an explicit
doubly-orthogonal estimation procedure for treatment effect.
",Double Machine Learning in Causal Inference,machine learning
"  Humans can understand and produce new utterances effortlessly, thanks to
their compositional skills. Once a person learns the meaning of a new verb
""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing
and dax."" In this paper, we introduce the SCAN domain, consisting of a set of
simple compositional navigation commands paired with the corresponding action
sequences. We then test the zero-shot generalization capabilities of a variety
of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence
methods. We find that RNNs can make successful zero-shot generalizations when
the differences between training and test commands are small, so that they can
apply ""mix-and-match"" strategies to solve the task. However, when
generalization requires systematic compositional skills (as in the ""dax""
example above), RNNs fail spectacularly. We conclude with a proof-of-concept
experiment in neural machine translation, suggesting that lack of systematicity
might be partially responsible for neural networks' notorious training data
thirst.
",Compositional Generalization in AI Models,compositional generalization in neural networks
"  Non-orthogonal multiple access (NOMA) has emerged as a promising radio access
technique for enabling the performance enhancements promised by the
fifth-generation (5G) networks in terms of connectivity, low latency, and high
spectrum efficiency. In the NOMA uplink, successive interference cancellation
(SIC) based detection with device clustering has been suggested. In the case of
multiple receive antennas, SIC can be combined with the minimum mean-squared
error (MMSE) beamforming. However, there exists a tradeoff between the NOMA
cluster size and the incurred SIC error. Larger clusters lead to larger errors
but they are desirable from the spectrum efficiency and connectivity point of
view. We propose a novel online learning based detection for the NOMA uplink.
In particular, we design an online adaptive filter in the sum space of linear
and Gaussian reproducing kernel Hilbert spaces (RKHSs). Such a sum space design
is robust against variations of a dynamic wireless network that can deteriorate
the performance of a purely nonlinear adaptive filter. We demonstrate by
simulations that the proposed method outperforms the MMSE-SIC based detection
for large cluster sizes.
",NOMA Detection Methods for 5G Networks,non-orthogonal multiple access (noma) techniques
"  In recent studies, it has shown that speaker patterns can be learned from
very short speech segments (e.g., 0.3 seconds) by a carefully designed
convolutional & time-delay deep neural network (CT-DNN) model. By enforcing the
model to discriminate the speakers in the training data, frame-level speaker
features can be derived from the last hidden layer. In spite of its good
performance, a potential problem of the present model is that it involves a
parametric classifier, i.e., the last affine layer, which may consume some
discriminative knowledge, thus leading to `information leak' for the feature
learning. This paper presents a full-info training approach that discards the
parametric classifier and enforces all the discriminative knowledge learned by
the feature net. Our experiments on the Fisher database demonstrate that this
new training scheme can produce more coherent features, leading to consistent
and notable performance improvement on the speaker verification task.
",Speaker Identification using Deep Neural Networks,speaker recognition using deep neural networks
"  In this work, we give the first algorithms for tolerant testing of nontrivial
classes in the active model: estimating the distance of a target function to a
hypothesis class C with respect to some arbitrary distribution D, using only a
small number of label queries to a polynomial-sized pool of unlabeled examples
drawn from D. Specifically, we show that for the class D of unions of d
intervals on the line, we can estimate the error rate of the best hypothesis in
the class to an additive error epsilon from only $O(\frac{1}{\epsilon^6}\log
\frac{1}{\epsilon})$ label queries to an unlabeled pool of size
$O(\frac{d}{\epsilon^2}\log \frac{1}{\epsilon})$. The key point here is the
number of labels needed is independent of the VC-dimension of the class. This
extends the work of Balcan et al. [2012] who solved the non-tolerant testing
problem for this class (distinguishing the zero-error case from the case that
the best hypothesis in the class has error greater than epsilon).
  We also consider the related problem of estimating the performance of a given
learning algorithm A in this setting. That is, given a large pool of unlabeled
examples drawn from distribution D, can we, from only a few label queries,
estimate how well A would perform if the entire dataset were labeled? We focus
on k-Nearest Neighbor style algorithms, and also show how our results can be
applied to the problem of hyperparameter tuning (selecting the best value of k
for the given learning problem).
",Tolerant Testing of Hypothesis Classes,tolerant testing and performance estimation of learning algorithms
"  This paper introduces and addresses a wide class of stochastic bandit
problems where the function mapping the arm to the corresponding reward
exhibits some known structural properties. Most existing structures (e.g.
linear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our
framework. We derive an asymptotic instance-specific regret lower bound for
these problems, and develop OSSB, an algorithm whose regret matches this
fundamental limit. OSSB is not based on the classical principle of ""optimism in
the face of uncertainty"" or on Thompson sampling, and rather aims at matching
the minimal exploration rates of sub-optimal arms as characterized in the
derivation of the regret lower bound. We illustrate the efficiency of OSSB
using numerical experiments in the case of the linear bandit problem and show
that OSSB outperforms existing algorithms, including Thompson sampling.
",Stochastic Bandit Problems with Structural Properties,stochastic bandit problems
"  We explore efficient neural architecture search methods and show that a
simple yet powerful evolutionary algorithm can discover new architectures with
excellent performance. Our approach combines a novel hierarchical genetic
representation scheme that imitates the modularized design pattern commonly
adopted by human experts, and an expressive search space that supports complex
topologies. Our algorithm efficiently discovers architectures that outperform a
large number of manually designed models for image classification, obtaining
top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which
is competitive with the best existing neural architecture search approaches. We
also present results using random search, achieving 0.3% less top-1 accuracy on
CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36
hours down to 1 hour.
",Neural Architecture Search,neural architecture search
"  This paper addresses matrix approximation problems for matrices that are
large, sparse and/or that are representations of large graphs. To tackle these
problems, we consider algorithms that are based primarily on coarsening
techniques, possibly combined with random sampling. A multilevel coarsening
technique is proposed which utilizes a hypergraph associated with the data
matrix and a graph coarsening strategy based on column matching. Theoretical
results are established that characterize the quality of the dimension
reduction achieved by a coarsening step, when a proper column matching strategy
is employed. We consider a number of standard applications of this technique as
well as a few new ones. Among the standard applications we first consider the
problem of computing the partial SVD for which a combination of sampling and
coarsening yields significantly improved SVD results relative to sampling
alone. We also consider the Column subset selection problem, a popular low rank
approximation method used in data related applications, and show how multilevel
coarsening can be adapted for this problem. Similarly, we consider the problem
of graph sparsification and show how coarsening techniques can be employed to
solve it. Numerical experiments illustrate the performances of the methods in
various applications.
",Matrix Coarsening Techniques for Large-Scale Data Approximation,matrix approximation and dimension reduction
"  Neural networks with low-precision weights and activations offer compelling
efficiency advantages over their full-precision equivalents. The two most
frequently discussed benefits of quantization are reduced memory consumption,
and a faster forward pass when implemented with efficient bitwise operations.
We propose a third benefit of very low-precision neural networks: improved
robustness against some adversarial attacks, and in the worst case, performance
that is on par with full-precision models. We focus on the very low-precision
case where weights and activations are both quantized to $\pm$1, and note that
stochastically quantizing weights in just one layer can sharply reduce the
impact of iterative attacks. We observe that non-scaled binary neural networks
exhibit a similar effect to the original defensive distillation procedure that
led to gradient masking, and a false notion of security. We address this by
conducting both black-box and white-box experiments with binary models that do
not artificially mask gradients.
",Adversarial Robustness of Low-Precision Neural Networks,low-precision neural networks for improved robustness
"  The success of Deep Learning and its potential use in many safety-critical
applications has motivated research on formal verification of Neural Network
(NN) models. Despite the reputation of learned NN models to behave as black
boxes and the theoretical hardness of proving their properties, researchers
have been successful in verifying some classes of models by exploiting their
piecewise linear structure and taking insights from formal methods such as
Satisifiability Modulo Theory. These methods are however still far from scaling
to realistic neural networks. To facilitate progress on this crucial area, we
make two key contributions. First, we present a unified framework that
encompasses previous methods. This analysis results in the identification of
new methods that combine the strengths of multiple existing approaches,
accomplishing a speedup of two orders of magnitude compared to the previous
state of the art. Second, we propose a new data set of benchmarks which
includes a collection of previously released testcases. We use the benchmark to
provide the first experimental comparison of existing algorithms and identify
the factors impacting the hardness of verification problems.
",Formal Verification of Neural Networks,formal verification of neural networks
"  Recent work in unsupervised representation learning has focused on learning
deep directed latent-variable models. Fitting these models by maximizing the
marginal likelihood or evidence is typically intractable, thus a common
approximation is to maximize the evidence lower bound (ELBO) instead. However,
maximum likelihood training (whether exact or approximate) does not necessarily
result in a good latent representation, as we demonstrate both theoretically
and empirically. In particular, we derive variational lower and upper bounds on
the mutual information between the input and the latent variable, and use these
bounds to derive a rate-distortion curve that characterizes the tradeoff
between compression and reconstruction accuracy. Using this framework, we
demonstrate that there is a family of models with identical ELBO, but different
quantitative and qualitative characteristics. Our framework also suggests a
simple new method to ensure that latent variable models with powerful
stochastic decoders do not ignore their latent code.
",Latent Variable Models Evaluation,unsupervised representation learning
"  It is common practice to decay the learning rate. Here we show one can
usually obtain the same learning curve on both training and test sets by
instead increasing the batch size during training. This procedure is successful
for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum,
and Adam. It reaches equivalent test accuracies after the same number of
training epochs, but with fewer parameter updates, leading to greater
parallelism and shorter training times. We can further reduce the number of
parameter updates by increasing the learning rate $\epsilon$ and scaling the
batch size $B \propto \epsilon$. Finally, one can increase the momentum
coefficient $m$ and scale $B \propto 1/(1-m)$, although this tends to slightly
reduce the test accuracy. Crucially, our techniques allow us to repurpose
existing training schedules for large batch training with no hyper-parameter
tuning. We train ResNet-50 on ImageNet to $76.1\%$ validation accuracy in under
30 minutes.
",Batch Size Optimization in Deep Learning,large batch training techniques
"  We consider the problem of learning a one-hidden-layer neural network: we
assume the input $x\in \mathbb{R}^d$ is from Gaussian distribution and the
label $y = a^\top \sigma(Bx) + \xi$, where $a$ is a nonnegative vector in
$\mathbb{R}^m$ with $m\le d$, $B\in \mathbb{R}^{m\times d}$ is a full-rank
weight matrix, and $\xi$ is a noise vector. We first give an analytic formula
for the population risk of the standard squared loss and demonstrate that it
implicitly attempts to decompose a sequence of low-rank tensors simultaneously.
  Inspired by the formula, we design a non-convex objective function $G(\cdot)$
whose landscape is guaranteed to have the following properties: 1. All local
minima of $G$ are also global minima.
  2. All global minima of $G$ correspond to the ground truth parameters.
  3. The value and gradient of $G$ can be estimated using samples.
  With these properties, stochastic gradient descent on $G$ provably converges
to the global minimum and learn the ground-truth parameters. We also prove
finite sample complexity result and validate the results by simulations.
",Neural Network Optimization,learning neural networks
"  Robust speech processing in multi-talker environments requires effective
speech separation. Recent deep learning systems have made significant progress
toward solving this problem, yet it remains challenging particularly in
real-time, short latency applications. Most methods attempt to construct a mask
for each source in time-frequency representation of the mixture signal which is
not necessarily an optimal representation for speech separation. In addition,
time-frequency decomposition results in inherent problems such as
phase/magnitude decoupling and long time window which is required to achieve
sufficient frequency resolution. We propose Time-domain Audio Separation
Network (TasNet) to overcome these limitations. We directly model the signal in
the time-domain using an encoder-decoder framework and perform the source
separation on nonnegative encoder outputs. This method removes the frequency
decomposition step and reduces the separation problem to estimation of source
masks on encoder outputs which is then synthesized by the decoder. Our system
outperforms the current state-of-the-art causal and noncausal speech separation
algorithms, reduces the computational cost of speech separation, and
significantly reduces the minimum required latency of the output. This makes
TasNet suitable for applications where low-power, real-time implementation is
desirable such as in hearable and telecommunication devices.
",Speech Separation in Multi-Talker Environments,speech separation
"  The detection of anomalous executions is valuable for reducing potential
hazards in assistive manipulation. Multimodal sensory signals can be helpful
for detecting a wide range of anomalies. However, the fusion of
high-dimensional and heterogeneous modalities is a challenging problem. We
introduce a long short-term memory based variational autoencoder (LSTM-VAE)
that fuses signals and reconstructs their expected distribution. We also
introduce an LSTM-VAE-based detector using a reconstruction-based anomaly score
and a state-based threshold. For evaluations with 1,555 robot-assisted feeding
executions including 12 representative types of anomalies, our detector had a
higher area under the receiver operating characteristic curve (AUC) of 0.8710
than 5 other baseline detectors from the literature. We also show the
multimodal fusion through the LSTM-VAE is effective by comparing our detector
with 17 raw sensory signals versus 4 hand-engineered features.
",Anomaly Detection in Assistive Manipulation,anomaly detection in assistive robotics
"  This paper proposes a practical approach for automatic sleep stage
classification based on a multi-level feature learning framework and Recurrent
Neural Network (RNN) classifier using heart rate and wrist actigraphy derived
from a wearable device. The feature learning framework is designed to extract
low- and mid-level features. Low-level features capture temporal and frequency
domain properties and mid-level features learn compositions and structural
information of signals. Since sleep staging is a sequential problem with
long-term dependencies, we take advantage of RNNs with Bidirectional Long
Short-Term Memory (BLSTM) architectures for sequence data learning. To simulate
the actual situation of daily sleep, experiments are conducted with a resting
group in which sleep is recorded in resting state, and a comprehensive group in
which both resting sleep and non-resting sleep are included.We evaluate the
algorithm based on an eight-fold cross validation to classify five sleep stages
(W, N1, N2, N3, and REM). The proposed algorithm achieves weighted precision,
recall and F1 score of 58.0%, 60.3%, and 58.2% in the resting group and 58.5%,
61.1%, and 58.5% in the comprehensive group, respectively. Various comparison
experiments demonstrate the effectiveness of feature learning and BLSTM. We
further explore the influence of depth and width of RNNs on performance. Our
method is specially proposed for wearable devices and is expected to be
applicable for long-term sleep monitoring at home. Without using too much prior
domain knowledge, our method has the potential to generalize sleep disorder
detection.
",Sleep Stage Classification Using Wearable Devices,sleep stage classification using wearable devices
"  This paper proposes a method for multi-class classification problems, where
the number of classes K is large. The method, referred to as Candidates vs.
Noises Estimation (CANE), selects a small subset of candidate classes and
samples the remaining classes. We show that CANE is always consistent and
computationally efficient. Moreover, the resulting estimator has low
statistical variance approaching that of the maximum likelihood estimator, when
the observed label belongs to the selected candidates with high probability. In
practice, we use a tree structure with leaves as classes to promote fast beam
search for candidate selection. We further apply the CANE method to estimate
word probabilities in learning large neural language models. Extensive
experimental results show that CANE achieves better prediction accuracy over
the Noise-Contrastive Estimation (NCE), its variants and a number of the
state-of-the-art tree classifiers, while it gains significant speedup compared
to standard O(K) methods.
",Multi-Class Classification Methods,multi-class classification methods
"  Traditional dictionary learning methods are based on quadratic convex loss
function and thus are sensitive to outliers. In this paper, we propose a
generic framework for robust dictionary learning based on concave losses. We
provide results on composition of concave functions, notably regarding
super-gradient computations, that are key for developing generic dictionary
learning algorithms applicable to smooth and non-smooth losses. In order to
improve identification of outliers, we introduce an initialization heuristic
based on undercomplete dictionary learning. Experimental results using
synthetic and real data demonstrate that our method is able to better detect
outliers, is capable of generating better dictionaries, outperforming
state-of-the-art methods such as K-SVD and LC-KSVD.
",Robust Dictionary Learning,robust dictionary learning
"  We consider the problem of inference in a causal generative model where the
set of available observations differs between data instances. We show how
combining samples drawn from the graphical model with an appropriate masking
function makes it possible to train a single neural network to approximate all
the corresponding conditional marginal distributions and thus amortize the cost
of inference. We further demonstrate that the efficiency of importance sampling
may be improved by basing proposals on the output of the neural network. We
also outline how the same network can be used to generate samples from an
approximate joint posterior via a chain decomposition of the graph.
",Causal Inference with Missing Data,causal inference in generative models
"  Recent work have done a good job in modeling rumors and detecting them over
microblog streams. However, the performance of their automatic approaches are
not relatively high when looking early in the diffusion. A first intuition is
that, at early stage, most of the aggregated rumor features (e.g., propagation
features) are not mature and distinctive enough. The objective of rumor
debunking in microblogs, however, are to detect these misinformation as early
as possible. In this work, we leverage neural models in learning the hidden
representations of individual rumor-related tweets at the very beginning of a
rumor. Our extensive experiments show that the resulting signal improves our
classification performance over time, significantly within the first 10 hours.
To deepen the understanding of these low and high-level features in
contributing to the model performance over time, we conduct an extensive study
on a wide range of high impact rumor features for the 48 hours range. The end
model that engages these features are shown to be competitive, reaches over 90%
accuracy and out-performs strong baselines in our carefully cured dataset.
",Early Rumor Detection on Microblog Streams,rumor detection and debunking in microblog streams
"  With the demand of high data rate and low latency in fifth generation (5G),
deep neural network decoder (NND) has become a promising candidate due to its
capability of one-shot decoding and parallel computing. In this paper, three
types of NND, i.e., multi-layer perceptron (MLP), convolution neural network
(CNN) and recurrent neural network (RNN), are proposed with the same parameter
magnitude. The performance of these deep neural networks are evaluated through
extensive simulation. Numerical results show that RNN has the best decoding
performance, yet at the price of the highest computational overhead. Moreover,
we find there exists a saturation length for each type of neural network, which
is caused by their restricted learning abilities.
",Deep Neural Network Decoders for  and 5G,deep neural network decoding for 5g networks
"  Learning tasks on source code (i.e., formal languages) have been considered
recently, but most work has tried to transfer natural language methods and does
not capitalize on the unique opportunities offered by code's known syntax. For
example, long-range dependencies induced by using the same variable or function
in distant locations are often not considered. We propose to use graphs to
represent both the syntactic and semantic structure of code and use graph-based
deep learning methods to learn to reason over program structures.
  In this work, we present how to construct graphs from source code and how to
scale Gated Graph Neural Networks training to such large graphs. We evaluate
our method on two tasks: VarNaming, in which a network attempts to predict the
name of a variable given its usage, and VarMisuse, in which the network learns
to reason about selecting the correct variable that should be used at a given
program location. Our comparison to methods that use less structured program
representations shows the advantages of modeling known structure, and suggests
that our models learn to infer meaningful names and to solve the VarMisuse task
in many cases. Additionally, our testing showed that VarMisuse identifies a
number of bugs in mature open-source projects.
",Graph-Based Code Representation for Deep Learning,graph-based deep learning for source code analysis
"  We give a covering number bound for deep learning networks that is
independent of the size of the network. The key for the simple analysis is that
for linear classifiers, rotating the data doesn't affect the covering number.
Thus, we can ignore the rotation part of each layer's linear transformation,
and get the covering number bound by concentrating on the scaling part.
",Deep Learning Covering Number Bounds,machine learning
"  We present an algorithm for approximating a function defined over a
$d$-dimensional manifold utilizing only noisy function values at locations
sampled from the manifold with noise. To produce the approximation we do not
require any knowledge regarding the manifold other than its dimension $d$. We
use the Manifold Moving Least-Squares approach of (Sober and Levin 2016) to
reconstruct the atlas of charts and the approximation is built on-top of those
charts. The resulting approximant is shown to be a function defined over a
neighborhood of a manifold, approximating the originally sampled manifold. In
other words, given a new point, located near the manifold, the approximation
can be evaluated directly on that point. We prove that our construction yields
a smooth function, and in case of noiseless samples the approximation order is
$\mathcal{O}(h^{m+1})$, where $h$ is a local density of sample parameter (i.e.,
the fill distance) and $m$ is the degree of a local polynomial approximation,
used in our algorithm. In addition, the proposed algorithm has linear time
complexity with respect to the ambient-space's dimension. Thus, we are able to
avoid the computational complexity, commonly encountered in high dimensional
approximations, without having to perform non-linear dimension reduction, which
inevitably introduces distortions to the geometry of the data. Additionaly, we
show numerical experiments that the proposed approach compares favorably to
statistical approaches for regression over manifolds and show its potential.
",Manifold Approximation with Noisy Samples,manifold approximation using noisy function values
"  Deep neural networks are surprisingly efficient at solving practical tasks,
but the theory behind this phenomenon is only starting to catch up with the
practice. Numerous works show that depth is the key to this efficiency. A
certain class of deep convolutional networks -- namely those that correspond to
the Hierarchical Tucker (HT) tensor decomposition -- has been proven to have
exponentially higher expressive power than shallow networks. I.e. a shallow
network of exponential width is required to realize the same score function as
computed by the deep architecture. In this paper, we prove the expressive power
theorem (an exponential lower bound on the width of the equivalent shallow
network) for a class of recurrent neural networks -- ones that correspond to
the Tensor Train (TT) decomposition. This means that even processing an image
patch by patch with an RNN can be exponentially more efficient than a (shallow)
convolutional network with one hidden layer. Using theoretical results on the
relation between the tensor decompositions we compare expressive powers of the
HT- and TT-Networks. We also implement the recurrent TT-Networks and provide
numerical evidence of their expressivity.
",Expressive Power of Deep Neural Networks,neural network architecture
"  Time series shapelets are discriminative sub-sequences and their similarity
to time series can be used for time series classification. Initial shapelet
extraction algorithms searched shapelets by complete enumeration of all
possible data sub-sequences. Research on shapelets for univariate time series
proposed a mechanism called shapelet learning which parameterizes the shapelets
and learns them jointly with a prediction model in an optimization procedure.
Trivial extension of this method to multivariate time series does not yield
very good results due to the presence of noisy channels which lead to
overfitting. In this paper we propose a shapelet learning scheme for
multivariate time series in which we introduce channel masks to discount noisy
channels and serve as an implicit regularization.
",Multivariate Time Series Shapelet Learning,shapelet learning for multivariate time series classification
"  Computing the medoid of a large number of points in high-dimensional space is
an increasingly common operation in many data science problems. We present an
algorithm Med-dit which uses O(n log n) distance evaluations to compute the
medoid with high probability. Med-dit is based on a connection with the
multi-armed bandit problem. We evaluate the performance of Med-dit empirically
on the Netflix-prize and the single-cell RNA-Seq datasets, containing hundreds
of thousands of points living in tens of thousands of dimensions, and observe a
5-10x improvement in performance over the current state of the art. Med-dit is
available at https://github.com/bagavi/Meddit
",Efficient Medoid Computation,efficient medoid computation in high-dimensional space
"  To achieve general intelligence, agents must learn how to interact with
others in a shared environment: this is the challenge of multiagent
reinforcement learning (MARL). The simplest form is independent reinforcement
learning (InRL), where each agent treats its experience as part of its
(non-stationary) environment. In this paper, we first observe that policies
learned using InRL can overfit to the other agents' policies during training,
failing to sufficiently generalize during execution. We introduce a new metric,
joint-policy correlation, to quantify this effect. We describe an algorithm for
general MARL, based on approximate best responses to mixtures of policies
generated using deep reinforcement learning, and empirical game-theoretic
analysis to compute meta-strategies for policy selection. The algorithm
generalizes previous ones such as InRL, iterated best response, double oracle,
and fictitious play. Then, we present a scalable implementation which reduces
the memory requirement using decoupled meta-solvers. Finally, we demonstrate
the generality of the resulting policies in two partially observable settings:
gridworld coordination games and poker.
",Multiagent Reinforcement Learning,multiagent reinforcement learning
"  Learning from class-imbalanced data continues to be a common and challenging
problem in supervised learning as standard classification algorithms are
designed to handle balanced class distributions. While different strategies
exist to tackle this problem, methods which generate artificial data to achieve
a balanced class distribution are more versatile than modifications to the
classification algorithm. Such techniques, called oversamplers, modify the
training data, allowing any classifier to be used with class-imbalanced
datasets. Many algorithms have been proposed for this task, but most are
complex and tend to generate unnecessary noise. This work presents a simple and
effective oversampling method based on k-means clustering and SMOTE
oversampling, which avoids the generation of noise and effectively overcomes
imbalances between and within classes. Empirical results of extensive
experiments with 71 datasets show that training data oversampled with the
proposed method improves classification results. Moreover, k-means SMOTE
consistently outperforms other popular oversampling methods. An implementation
is made available in the python programming language.
",Oversampling Techniques for Class-Imbalanced Data,class imbalance problem
"  We consider numerical schemes for root finding of noisy responses through
generalizing the Probabilistic Bisection Algorithm (PBA) to the more practical
context where the sampling distribution is unknown and location-dependent. As
in standard PBA, we rely on a knowledge state for the approximate posterior of
the root location. To implement the corresponding Bayesian updating, we also
carry out inference of oracle accuracy, namely learning the probability of
correct response. To this end we utilize batched querying in combination with a
variety of frequentist and Bayesian estimators based on majority vote, as well
as the underlying functional responses, if available. For guiding sampling
selection we investigate both Information Directed sampling, as well as
Quantile sampling. Our numerical experiments show that these strategies perform
quite differently; in particular we demonstrate the efficiency of randomized
quantile sampling which is reminiscent of Thompson sampling. Our work is
motivated by the root-finding sub-routine in pricing of Bermudan financial
derivatives, illustrated in the last section of the paper.
",Root Finding in Noisy Responses,root finding in noisy response models
"  Disentangled representations, where the higher level data generative factors
are reflected in disjoint latent dimensions, offer several benefits such as
ease of deriving invariant representations, transferability to other tasks,
interpretability, etc. We consider the problem of unsupervised learning of
disentangled representations from large pool of unlabeled observations, and
propose a variational inference based approach to infer disentangled latent
factors. We introduce a regularizer on the expectation of the approximate
posterior over observed data that encourages the disentanglement. We also
propose a new disentanglement metric which is better aligned with the
qualitative disentanglement observed in the decoder's output. We empirically
observe significant improvement over existing methods in terms of both
disentanglement and data likelihood (reconstruction quality).
",Unsupervised Disentangled Representation Learning,unsupervised learning of disentangled representations
"  We propose a method to learn deep ReLU-based classifiers that are provably
robust against norm-bounded adversarial perturbations on the training data. For
previously unseen examples, the approach is guaranteed to detect all
adversarial examples, though it may flag some non-adversarial examples as well.
The basic idea is to consider a convex outer approximation of the set of
activations reachable through a norm-bounded perturbation, and we develop a
robust optimization procedure that minimizes the worst case loss over this
outer region (via a linear program). Crucially, we show that the dual problem
to this linear program can be represented itself as a deep network similar to
the backpropagation network, leading to very efficient optimization approaches
that produce guaranteed bounds on the robust loss. The end result is that by
executing a few more forward and backward passes through a slightly modified
version of the original network (though possibly with much larger batch sizes),
we can learn a classifier that is provably robust to any norm-bounded
adversarial attack. We illustrate the approach on a number of tasks to train
classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a
convolutional classifier that provably has less than 5.8% test error for any
adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$),
and code for all experiments in the paper is available at
https://github.com/locuslab/convex_adversarial.
",Adversarial Robustness in Deep Learning,robust deep learning for adversarial classification
"  Saliency methods aim to explain the predictions of deep neural networks.
These methods lack reliability when the explanation is sensitive to factors
that do not contribute to the model prediction. We use a simple and common
pre-processing step ---adding a constant shift to the input data--- to show
that a transformation with no effect on the model can cause numerous methods to
incorrectly attribute. In order to guarantee reliability, we posit that methods
should fulfill input invariance, the requirement that a saliency method mirror
the sensitivity of the model with respect to transformations of the input. We
show, through several examples, that saliency methods that do not satisfy input
invariance result in misleading attribution.
",Reliability of Saliency Methods in Deep Neural Networks,reliability of saliency methods in deep learning
"  Visual data, such as an image or a sequence of video frames, is often
naturally represented as a point set. In this paper, we consider the
fundamental problem of finding a nearest set from a collection of sets, to a
query set. This problem has obvious applications in large-scale visual
retrieval and recognition, and also in applied fields beyond computer vision.
One challenge stands out in solving the problem---set representation and
measure of similarity. Particularly, the query set and the sets in dataset
collection can have varying cardinalities. The training collection is large
enough such that linear scan is impractical. We propose a simple representation
scheme that encodes both statistical and structural information of the sets.
The derived representations are integrated in a kernel framework for flexible
similarity measurement. For the query set process, we adopt a learning-to-hash
pipeline that turns the kernel representations into hash bits based on simple
learners, using multiple kernel learning. Experiments on two visual retrieval
datasets show unambiguously that our set-to-set hashing framework outperforms
prior methods that do not take the set-to-set search setting.
",Image Retrieval,set-to-set hashing for visual data retrieval
"  We study the problem of conditional generative modeling based on designated
semantics or structures. Existing models that build conditional generators
either require massive labeled instances as supervision or are unable to
accurately control the semantics of generated samples. We propose structured
generative adversarial networks (SGANs) for semi-supervised conditional
generative modeling. SGAN assumes the data x is generated conditioned on two
independent latent variables: y that encodes the designated semantics, and z
that contains other factors of variation. To ensure disentangled semantics in y
and z, SGAN builds two collaborative games in the hidden space to minimize the
reconstruction error of y and z, respectively. Training SGAN also involves
solving two adversarial games that have their equilibrium concentrating at the
true joint data distributions p(x, z) and p(x, y), avoiding distributing the
probability mass diffusely over data space that MLE-based methods may suffer.
We assess SGAN by evaluating its trained networks, and its performance on
downstream tasks. We show that SGAN delivers a highly controllable generator,
and disentangled representations; it also establishes start-of-the-art results
across multiple datasets when applied for semi-supervised image classification
(1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000
and 4000 labels, respectively). Benefiting from the separate modeling of y and
z, SGAN can generate images with high visual quality and strictly following the
designated semantic, and can be extended to a wide spectrum of applications,
such as style transfer.
",Conditional Generative Modeling,conditional generative modeling
"  A major challenge in X-ray computed tomography (CT) is reducing radiation
dose while maintaining high quality of reconstructed images. To reduce the
radiation dose, one can reduce the number of projection views (sparse-view CT);
however, it becomes difficult to achieve high-quality image reconstruction as
the number of projection views decreases. Researchers have applied the concept
of learning sparse representations from (high-quality) CT image dataset to the
sparse-view CT reconstruction. We propose a new statistical CT reconstruction
model that combines penalized weighted-least squares (PWLS) and $\ell_1$ prior
with learned sparsifying transform (PWLS-ST-$\ell_1$), and a corresponding
efficient algorithm based on Alternating Direction Method of Multipliers
(ADMM). To moderate the difficulty of tuning ADMM parameters, we propose a new
ADMM parameter selection scheme based on approximated condition numbers. We
interpret the proposed model by analyzing the minimum mean square error of its
($\ell_2$-norm relaxed) image update estimator. Our results with the extended
cardiac-torso (XCAT) phantom data and clinical chest data show that, for
sparse-view 2D fan-beam CT and 3D axial cone-beam CT, PWLS-ST-$\ell_1$ improves
the quality of reconstructed images compared to the CT reconstruction methods
using edge-preserving regularizer and $\ell_2$ prior with learned ST. These
results also show that, for sparse-view 2D fan-beam CT, PWLS-ST-$\ell_1$
achieves comparable or better image quality and requires much shorter runtime
than PWLS-DL using a learned overcomplete dictionary. Our results with clinical
chest data show that, methods using the unsupervised learned prior generalize
better than a state-of-the-art deep ""denoising"" neural network that does not
use a physical imaging model.
",Sparse-View CT Reconstruction,sparse-view ct reconstruction
"  Learning useful representations without supervision remains a key challenge
in machine learning. In this paper, we propose a simple yet powerful generative
model that learns such discrete representations. Our model, the Vector
Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways:
the encoder network outputs discrete, rather than continuous, codes; and the
prior is learnt rather than static. In order to learn a discrete latent
representation, we incorporate ideas from vector quantisation (VQ). Using the
VQ method allows the model to circumvent issues of ""posterior collapse"" --
where the latents are ignored when they are paired with a powerful
autoregressive decoder -- typically observed in the VAE framework. Pairing
these representations with an autoregressive prior, the model can generate high
quality images, videos, and speech as well as doing high quality speaker
conversion and unsupervised learning of phonemes, providing further evidence of
the utility of the learnt representations.
",Unsupervised Representation Learning,discrete representation learning in machine learning
"  The interpolation, prediction, and feature analysis of fine-gained air
quality are three important topics in the area of urban air computing. The
solutions to these topics can provide extremely useful information to support
air pollution control, and consequently generate great societal and technical
impacts. Most of the existing work solves the three problems separately by
different models. In this paper, we propose a general and effective approach to
solve the three problems in one model called the Deep Air Learning (DAL). The
main idea of DAL lies in embedding feature selection and semi-supervised
learning in different layers of the deep learning network. The proposed
approach utilizes the information pertaining to the unlabeled spatio-temporal
data to improve the performance of the interpolation and the prediction, and
performs feature selection and association analysis to reveal the main relevant
features to the variation of the air quality. We evaluate our approach with
extensive experiments based on real data sources obtained in Beijing, China.
Experiments show that DAL is superior to the peer models from the recent
literature when solving the topics of interpolation, prediction, and feature
analysis of fine-gained air quality.
",Urban Air Quality Prediction,deep air learning for fine-gained air quality analysis
"  This paper is concerned with pool-based active learning for deep neural
networks. Motivated by coreset dataset compression ideas, we present a novel
active learning algorithm that queries consecutive points from the pool using
farthest-first traversals in the space of neural activation over a
representation layer. We show consistent and overwhelming improvement in sample
complexity over passive learning (random sampling) for three datasets: MNIST,
CIFAR-10, and CIFAR-100. In addition, our algorithm outperforms the traditional
uncertainty sampling technique (obtained using softmax activations), and we
identify cases where uncertainty sampling is only slightly better than random
sampling.
",Active Learning for Deep Neural Networks,pool-based active learning for deep neural networks
"  We present an efficient and practical algorithm for the online prediction of
discrete-time linear dynamical systems with a symmetric transition matrix. We
circumvent the non-convex optimization problem using improper learning:
carefully overparameterize the class of LDSs by a polylogarithmic factor, in
exchange for convexity of the loss functions. From this arises a
polynomial-time algorithm with a near-optimal regret guarantee, with an
analogous sample complexity bound for agnostic learning. Our algorithm is based
on a novel filtering technique, which may be of independent interest: we
convolve the time series with the eigenvectors of a certain Hankel matrix.
",Online Prediction of Linear Dynamical Systems,efficient online prediction of discrete-time linear dynamical systems
"  We present an algorithm to identify sparse dependence structure in continuous
and non-Gaussian probability distributions, given a corresponding set of data.
The conditional independence structure of an arbitrary distribution can be
represented as an undirected graph (or Markov random field), but most
algorithms for learning this structure are restricted to the discrete or
Gaussian cases. Our new approach allows for more realistic and accurate
descriptions of the distribution in question, and in turn better estimates of
its sparse Markov structure. Sparsity in the graph is of interest as it can
accelerate inference, improve sampling methods, and reveal important
dependencies between variables. The algorithm relies on exploiting the
connection between the sparsity of the graph and the sparsity of transport
maps, which deterministically couple one probability measure to another.
",Learning Sparse Markov Structure in Non-Gaussian Distributions,machine learning/statistics: graph theory
"  A basic, and still largely unanswered, question in the context of Generative
Adversarial Networks (GANs) is whether they are truly able to capture all the
fundamental characteristics of the distributions they are trained on. In
particular, evaluating the diversity of GAN distributions is challenging and
existing methods provide only a partial understanding of this issue. In this
paper, we develop quantitative and scalable tools for assessing the diversity
of GAN distributions. Specifically, we take a classification-based perspective
and view loss of diversity as a form of covariate shift introduced by GANs. We
examine two specific forms of such shift: mode collapse and boundary
distortion. In contrast to prior work, our methods need only minimal human
supervision and can be readily applied to state-of-the-art GANs on large,
canonical datasets. Examining popular GANs using our tools indicates that these
GANs have significant problems in reproducing the more distributional
properties of their training dataset.
",Evaluating Diversity of GAN Distributions,evaluation of generative adversarial networks (gans) diversity
"  Discovering statistical structure from links is a fundamental problem in the
analysis of social networks. Choosing a misspecified model, or equivalently, an
incorrect inference algorithm will result in an invalid analysis or even
falsely uncover patterns that are in fact artifacts of the model. This work
focuses on unifying two of the most widely used link-formation models: the
stochastic blockmodel (SBM) and the small world (or latent space) model (SWM).
Integrating techniques from kernel learning, spectral graph theory, and
nonlinear dimensionality reduction, we develop the first statistically sound
polynomial-time algorithm to discover latent patterns in sparse graphs for both
models. When the network comes from an SBM, the algorithm outputs a block
structure. When it is from an SWM, the algorithm outputs estimates of each
node's latent position.
",Social Network Modeling,link-based network analysis
"  Neighborhood regression has been a successful approach in graphical and
structural equation modeling, with applications to learning undirected and
directed graphical models. We extend these ideas by defining and studying an
algebraic structure called the neighborhood lattice based on a generalized
notion of neighborhood regression. We show that this algebraic structure has
the potential to provide an economic encoding of all conditional independence
statements in a Gaussian distribution (or conditional uncorrelatedness in
general), even in the cases where no graphical model exists that could
""perfectly"" encode all such statements. We study the computational complexity
of computing these structures and show that under a sparsity assumption, they
can be computed in polynomial time, even in the absence of the assumption of
perfectness to a graph. On the other hand, assuming perfectness, we show how
these neighborhood lattices may be ""graphically"" computed using the separation
properties of the so-called partial correlation graph. We also draw connections
with directed acyclic graphical models and Bayesian networks. We derive these
results using an abstract generalization of partial uncorrelatedness, called
partial orthogonality, which allows us to use algebraic properties of
projection operators on Hilbert spaces to significantly simplify and extend
existing ideas and arguments. Consequently, our results apply to a wide range
of random objects and data structures, such as random vectors, data matrices,
and functions.
",Neighborhood Lattice in Graphical Modeling,neighborhood lattice and conditional independence
"  Genetic algorithms have been widely used in many practical optimization
problems. Inspired by natural selection, operators, including mutation,
crossover and selection, provide effective heuristics for search and black-box
optimization. However, they have not been shown useful for deep reinforcement
learning, possibly due to the catastrophic consequence of parameter crossovers
of neural networks. Here, we present Genetic Policy Optimization (GPO), a new
genetic algorithm for sample-efficient deep policy optimization. GPO uses
imitation learning for policy crossover in the state space and applies policy
gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as
a genetic algorithm is able to provide superior performance over the
state-of-the-art policy gradient methods and achieves comparable or higher
sample efficiency.
",Genetic Policy Optimization in Deep Reinforcement Learning,deep reinforcement learning
"  In (online) learning theory the concepts of sparsity, variance and curvature
are well-understood and are routinely used to obtain refined regret and
generalization bounds. In this paper we further our understanding of these
concepts in the more challenging limited feedback scenario. We consider the
adversarial multi-armed bandit and linear bandit settings and solve several
open problems pertaining to the existence of algorithms with favorable regret
bounds under the following assumptions: (i) sparsity of the individual losses,
(ii) small variation of the loss sequence, and (iii) curvature of the action
set. Specifically we show that (i) for $s$-sparse losses one can obtain
$\tilde{O}(\sqrt{s T})$-regret (solving an open problem by Kwon and Perchet),
(ii) for loss sequences with variation bounded by $Q$ one can obtain
$\tilde{O}(\sqrt{Q})$-regret (solving an open problem by Kale and Hazan), and
(iii) for linear bandit on an $\ell_p^n$ ball one can obtain $\tilde{O}(\sqrt{n
T})$-regret for $p \in [1,2]$ and one has $\tilde{\Omega}(n \sqrt{T})$-regret
for $p>2$ (solving an open problem by Bubeck, Cesa-Bianchi and Kakade). A key
new insight to obtain these results is to use regularizers satisfying more
refined conditions than general self-concordance
",Regret Bounds in Limited Feedback Bandit Settings,multi-armed bandit problem
"  Sparse variational approximations allow for principled and scalable inference
in Gaussian Process (GP) models. In settings where several GPs are part of the
generative model, theses GPs are a posteriori coupled. For many applications
such as regression where predictive accuracy is the quantity of interest, this
coupling is not crucial. Howewer if one is interested in posterior uncertainty,
it cannot be ignored. A key element of variational inference schemes is the
choice of the approximate posterior parameterization. When the number of latent
variables is large, mean field (MF) methods provide fast and accurate posterior
means while more structured posterior lead to inference algorithm of greater
computational complexity. Here, we extend previous sparse GP approximations and
propose a novel parameterization of variational posteriors in the multi-GP
setting allowing for fast and scalable inference capturing posterior
dependencies.
",Variational Inference for Multi-GP Models,sparse variational approximations for multi-gaussian process models
"  The purpose of this paper is to study the algorithm FCM and some of its
famous innovations, analyse and discover the method of applying hedge algebra
theory that uses algebra to represent linguistic-valued variables, to FCM.
Then, this paper will propose a new FCM-based algorithm which uses hedge
algebra to model FCM's exponent parameter. Finally, the design, analysis and
implementation of the new algorithm as well some experimental results will be
presented to prove our algorithm's capacity of solving clustering problems in
practice.
",Hedge Algebra in Fuzzy Clustering,fuzzy clustering algorithm using hedge algebra
"  Neural samplers such as variational autoencoders (VAEs) or generative
adversarial networks (GANs) approximate distributions by transforming samples
from a simple random source---the latent space---to samples from a more complex
distribution represented by a dataset. While the manifold hypothesis implies
that the density induced by a dataset contains large regions of low density,
the training criterions of VAEs and GANs will make the latent space densely
covered. Consequently points that are separated by low-density regions in
observation space will be pushed together in latent space, making stationary
distances poor proxies for similarity. We transfer ideas from Riemannian
geometry to this setting, letting the distance between two points be the
shortest path on a Riemannian manifold induced by the transformation. The
method yields a principled distance measure, provides a tool for visual
inspection of deep generative models, and an alternative to linear
interpolation in latent space. In addition, it can be applied for robot
movement generalization using previously learned skills. The method is
evaluated on a synthetic dataset with known ground truth; on a simulated robot
arm dataset; on human motion capture data; and on a generative model of
handwritten digits.
",Riemannian Geometry in Generative Models,non-euclidean distance measures for deep generative models
"  Multi-task learning (MTL) with neural networks leverages commonalities in
tasks to improve performance, but often suffers from task interference which
reduces the benefits of transfer. To address this issue we introduce the
routing network paradigm, a novel neural network and training algorithm. A
routing network is a kind of self-organizing neural network consisting of two
components: a router and a set of one or more function blocks. A function block
may be any neural network - for example a fully-connected or a convolutional
layer. Given an input the router makes a routing decision, choosing a function
block to apply and passing the output back to the router recursively,
terminating when a fixed recursion depth is reached. In this way the routing
network dynamically composes different function blocks for each input. We
employ a collaborative multi-agent reinforcement learning (MARL) approach to
jointly train the router and function blocks. We evaluate our model against
cross-stitch networks and shared-layer baselines on multi-task settings of the
MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a
significant improvement in accuracy, with sharper convergence. In addition,
routing networks have nearly constant per-task training cost while cross-stitch
networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we
obtain cross-stitch performance levels with an 85% reduction in training time.
",Multi-Task Learning with Neural Networks,multi-task learning with neural networks
"  This paper proposes ReBNet, an end-to-end framework for training
reconfigurable binary neural networks on software and developing efficient
accelerators for execution on FPGA. Binary neural networks offer an intriguing
opportunity for deploying large-scale deep learning models on
resource-constrained devices. Binarization reduces the memory footprint and
replaces the power-hungry matrix-multiplication with light-weight XnorPopcount
operations. However, binary networks suffer from a degraded accuracy compared
to their fixed-point counterparts. We show that the state-of-the-art methods
for optimizing binary networks accuracy, significantly increase the
implementation cost and complexity. To compensate for the degraded accuracy
while adhering to the simplicity of binary networks, we devise the first
reconfigurable scheme that can adjust the classification accuracy based on the
application. Our proposition improves the classification accuracy by
representing features with multiple levels of residual binarization. Unlike
previous methods, our approach does not exacerbate the area cost of the
hardware accelerator. Instead, it provides a tradeoff between throughput and
accuracy while the area overhead of multi-level binarization is negligible.
",Reconfigurable Binary Neural Networks for Efficient FPGA Execution,reconfigurable binary neural networks
"  In meta-learning an agent extracts knowledge from observed tasks, aiming to
facilitate learning of novel future tasks. Under the assumption that future
tasks are 'related' to previous tasks, the accumulated knowledge should be
learned in a way which captures the common structure across learned tasks,
while allowing the learner sufficient flexibility to adapt to novel aspects of
new tasks. We present a framework for meta-learning that is based on
generalization error bounds, allowing us to extend various PAC-Bayes bounds to
meta-learning. Learning takes place through the construction of a distribution
over hypotheses based on the observed tasks, and its utilization for learning a
new task. Thus, prior knowledge is incorporated through setting an
experience-dependent prior for novel tasks. We develop a gradient-based
algorithm which minimizes an objective function derived from the bounds and
demonstrate its effectiveness numerically with deep neural networks. In
addition to establishing the improved performance available through
meta-learning, we demonstrate the intuitive way by which prior information is
manifested at different levels of the network.
",Meta-Learning with PAC-Bayes Bounds,meta-learning in artificial intelligence
"  Contemporary Deep Neural Network (DNN) contains millions of synaptic
connections with tens to hundreds of layers. The large computation and memory
requirements pose a challenge to the hardware design. In this work, we leverage
the intrinsic activation sparsity of DNN to substantially reduce the execution
cycles and the energy consumption. An end-to-end training algorithm is proposed
to develop a lightweight run-time predictor for the output activation sparsity
on the fly. From our experimental results, the computation overhead of the
prediction phase can be reduced to less than 5% of the original feedforward
phase with negligible accuracy loss. Furthermore, an energy-efficient hardware
architecture, SparseNN, is proposed to exploit both the input and output
sparsity. SparseNN is a scalable architecture with distributed memories and
processing elements connected through a dedicated on-chip network. Compared
with the state-of-the-art accelerators which only exploit the input sparsity,
SparseNN can achieve a 10%-70% improvement in throughput and a power reduction
of around 50%.
",Energy-Efficient Neural Network Architectures,energy-efficient deep neural network architectures
"  Process Discovery is concerned with the automatic generation of a process
model that describes a business process from execution data of that business
process. Real life event logs can contain chaotic activities. These activities
are independent of the state of the process and can, therefore, happen at
rather arbitrary points in time. We show that the presence of such chaotic
activities in an event log heavily impacts the quality of the process models
that can be discovered with process discovery techniques. The current modus
operandi for filtering activities from event logs is to simply filter out
infrequent activities. We show that frequency-based filtering of activities
does not solve the problems that are caused by chaotic activities. Moreover, we
propose a novel technique to filter out chaotic activities from event logs. We
evaluate this technique on a collection of seventeen real-life event logs that
originate from both the business process management domain and the smart home
environment domain. As demonstrated, the developed activity filtering methods
enable the discovery of process models that are more behaviorally specific
compared to process models that are discovered using standard frequency-based
filtering.
",Process Model Quality Improvement,process discovery and activity filtering
"  Modern neural networks tend to be overconfident on unseen, noisy or
incorrectly labelled data and do not produce meaningful uncertainty measures.
Bayesian deep learning aims to address this shortcoming with variational
approximations (such as Bayes by Backprop or Multiplicative Normalising Flows).
However, current approaches have limitations regarding flexibility and
scalability. We introduce Bayes by Hypernet (BbH), a new method of variational
approximation that interprets hypernetworks as implicit distributions. It
naturally uses neural networks to model arbitrarily complex distributions and
scales to modern deep learning architectures. In our experiments, we
demonstrate that our method achieves competitive accuracies and predictive
uncertainties on MNIST and a CIFAR5 task, while being the most robust against
adversarial attacks.
",Bayesian Deep Learning for Uncertainty Estimation,bayesian deep learning
"  We address online learning in complex auction settings, such as sponsored
search auctions, where the value of the bidder is unknown to her, evolving in
an arbitrary manner and observed only if the bidder wins an allocation. We
leverage the structure of the utility of the bidder and the partial feedback
that bidders typically receive in auctions, in order to provide algorithms with
regret rates against the best fixed bid in hindsight, that are exponentially
faster in convergence in terms of dependence on the action space, than what
would have been derived by applying a generic bandit algorithm and almost
equivalent to what would have been achieved in the full information setting.
Our results are enabled by analyzing a new online learning setting with
outcome-based feedback, which generalizes learning with feedback graphs. We
provide an online learning algorithm for this setting, of independent interest,
with regret that grows only logarithmically with the number of actions and
linearly only in the number of potential outcomes (the latter being very small
in most auction settings). Last but not least, we show that our algorithm
outperforms the bandit approach experimentally and that this performance is
robust to dropping some of our theoretical assumptions or introducing noise in
the feedback that the bidder receives.
",Online Auction Learning with Partial Feedback,online learning in complex auction settings
"  We propose a reconfigurable hardware architecture for deep neural networks
(DNNs) capable of online training and inference, which uses algorithmically
pre-determined, structured sparsity to significantly lower memory and
computational requirements. This novel architecture introduces the notion of
edge-processing to provide flexibility and combines junction pipelining and
operational parallelization to speed up training. The overall effect is to
reduce network complexity by factors up to 30x and training time by up to 35x
relative to GPUs, while maintaining high fidelity of inference results. This
has the potential to enable extensive parameter searches and development of the
largely unexplored theoretical foundation of DNNs. The architecture
automatically adapts itself to different network sizes given available hardware
resources. As proof of concept, we show results obtained for different bit
widths.
",Reconfigurable Hardware for DNNs,reconfigurable neural network architectures
"  In robotics, it is essential to be able to plan efficiently in
high-dimensional continuous state-action spaces for long horizons. For such
complex planning problems, unguided uniform sampling of actions until a path to
a goal is found is hopelessly inefficient, and gradient-based approaches often
fall short when the optimization manifold of a given problem is not smooth. In
this paper we present an approach that guides the search of a state-space
planner, such as A*, by learning an action-sampling distribution that can
generalize across different instances of a planning problem. The motivation is
that, unlike typical learning approaches for planning for continuous action
space that estimate a policy, an estimated action sampler is more robust to
error since it has a planner to fall back on. We use a Generative Adversarial
Network (GAN), and address an important issue: search experience consists of a
relatively large number of actions that are not on a solution path and a
relatively small number of actions that actually are on a solution path. We
introduce a new technique, based on an importance-ratio estimation method, for
using samples from a non-target distribution to make GAN learning more
data-efficient. We provide theoretical guarantees and empirical evaluation in
three challenging continuous robot planning problems to illustrate the
effectiveness of our algorithm.
",Robot Planning with Generative Adversarial Networks,guided planning in high-dimensional continuous state-action spaces
"  We consider the problem of recovering the superposition of $R$ distinct
complex exponential functions from compressed non-uniform time-domain samples.
Total Variation (TV) minimization or atomic norm minimization was proposed in
the literature to recover the $R$ frequencies or the missing data. However, it
is known that in order for TV minimization and atomic norm minimization to
recover the missing data or the frequencies, the underlying $R$ frequencies are
required to be well-separated, even when the measurements are noiseless. This
paper shows that the Hankel matrix recovery approach can super-resolve the $R$
complex exponentials and their frequencies from compressed non-uniform
measurements, regardless of how close their frequencies are to each other. We
propose a new concept of orthonormal atomic norm minimization (OANM), and
demonstrate that the success of Hankel matrix recovery in separation-free
super-resolution comes from the fact that the nuclear norm of a Hankel matrix
is an orthonormal atomic norm. More specifically, we show that, in traditional
atomic norm minimization, the underlying parameter values $\textbf{must}$ be
well separated to achieve successful signal recovery, if the atoms are changing
continuously with respect to the continuously-valued parameter. In contrast,
for the OANM, it is possible the OANM is successful even though the original
atoms can be arbitrarily close.
  As a byproduct of this research, we provide one matrix-theoretic inequality
of nuclear norm, and give its proof from the theory of compressed sensing.
",Super-Resolution of Complex Exponentials from Compressed Measurements,super-resolution of complex exponential signals
"  We propose a statistical model for natural language that begins by
considering language as a monoid, then representing it in complex matrices with
a compatible translation invariant probability measure. We interpret the
probability measure as arising via the Born rule from a translation invariant
matrix product state.
",Quantum Natural Language Processing,quantum physics
"  Machine learning is usually defined in behaviourist terms, where external
validation is the primary mechanism of learning. In this paper, I argue for a
more holistic interpretation in which finding more probable, efficient and
abstract representations is as central to learning as performance. In other
words, machine learning should be extended with strategies to reason over its
own learning process, leading to so-called meta-cognitive machine learning. As
such, the de facto definition of machine learning should be reformulated in
these intrinsically multi-objective terms, taking into account not only the
task performance but also internal learning objectives. To this end, we suggest
a ""model entropy function"" to be defined that quantifies the efficiency of the
internal learning processes. It is conjured that the minimization of this model
entropy leads to concept formation. Besides philosophical aspects, some initial
illustrations are included to support the claims.
",Meta-Cognitive Machine Learning,meta-cognitive machine learning
"  Rapid growth of modern technologies such as internet and mobile computing are
bringing dramatically increased e-commerce payments, as well as the explosion
in transaction fraud. Meanwhile, fraudsters are continually refining their
tricks, making rule-based fraud detection systems difficult to handle the
ever-changing fraud patterns. Many data mining and artificial intelligence
methods have been proposed for identifying small anomalies in large transaction
data sets, increasing detecting efficiency to some extent. Nevertheless, there
is always a contradiction that most methods are irrelevant to transaction
sequence, yet sequence-related methods usually cannot learn information at
single-transaction level well. In this paper, a new ""within->between->within""
sandwich-structured sequence learning architecture has been proposed by
stacking an ensemble method, a deep sequential learning method and another
top-layer ensemble classifier in proper order. Moreover, attention mechanism
has also been introduced in to further improve performance. Models in this
structure have been manifested to be very efficient in scenarios like fraud
detection, where the information sequence is made up of vectors with complex
interconnected features.
",Sequence-Based Fraud Detection in E-commerce,fraud detection using machine learning
"  The Gaussian kernel is a very popular kernel function used in many machine
learning algorithms, especially in support vector machines (SVMs). It is more
often used than polynomial kernels when learning from nonlinear datasets, and
is usually employed in formulating the classical SVM for nonlinear problems. In
[3], Rebentrost et al. discussed an elegant quantum version of a least square
support vector machine using quantum polynomial kernels, which is exponentially
faster than the classical counterpart. This paper demonstrates a quantum
version of the Gaussian kernel and analyzes its runtime complexity using the
quantum random access memory (QRAM) in the context of quantum SVM. Our analysis
shows that the runtime computational complexity of the quantum Gaussian kernel
seems to be significantly faster as compared to its classical version.
",Quantum Gaussian Kernel for SVMs,machine learning
"  Deep learning approaches such as convolutional neural nets have consistently
outperformed previous methods on challenging tasks such as dense, semantic
segmentation. However, the various proposed networks perform differently, with
behaviour largely influenced by architectural choices and training settings.
This paper explores Ensembles of Multiple Models and Architectures (EMMA) for
robust performance through aggregation of predictions from a wide range of
methods. The approach reduces the influence of the meta-parameters of
individual models and the risk of overfitting the configuration to a particular
database. EMMA can be seen as an unbiased, generic deep learning model which is
shown to yield excellent performance, winning the first position in the BRATS
2017 competition among 50+ participating teams.
",Deep Learning Ensembles for Semantic Segmentation,deep learning ensemble methods for robust performance
"  This work provides performance guarantees for the greedy solution of
experimental design problems. In particular, it focuses on A- and E-optimal
designs, for which typical guarantees do not apply since the mean-square error
and the maximum eigenvalue of the estimation error covariance matrix are not
supermodular. To do so, it leverages the concept of approximate supermodularity
to derive non-asymptotic worst-case suboptimality bounds for these greedy
solutions. These bounds reveal that as the SNR of the experiments decreases,
these cost functions behave increasingly as supermodular functions. As such,
greedy A- and E-optimal designs approach (1-1/e)-optimality. These results
reconcile the empirical success of greedy experimental design with the
non-supermodularity of the A- and E-optimality criteria.
",Performance Guarantees for Greedy Experimental Design,non-asymptotic performance guarantees for greedy experimental design
"  Preserving the privacy of individuals by protecting their sensitive
attributes is an important consideration during microdata release. However, it
is equally important to preserve the quality or utility of the data for at
least some targeted workloads. We propose a novel framework for privacy
preservation based on the k-anonymity model that is ideally suited for
workloads that require preserving the probability distribution of the
quasi-identifier variables in the data. Our framework combines the principles
of distribution-preserving quantization and k-member clustering, and we
specialize it to two variants that respectively use intra-cluster and Gaussian
dithering of cluster centers to achieve distribution preservation. We perform
theoretical analysis of the proposed schemes in terms of distribution
preservation, and describe their utility in workloads such as covariate shift
and transfer learning where such a property is necessary. Using extensive
experiments on real-world Medical Expenditure Panel Survey data, we demonstrate
the merits of our algorithms over standard k-anonymization for a hallmark
health care application where an insurance company wishes to understand the
risk in entering a new market. Furthermore, by empirically quantifying the
reidentification risk, we also show that the proposed approaches indeed
maintain k-anonymity.
",Privacy-Preserving Data Release,privacy-preserving data release
"  The performance of many parallel applications depends on loop-level
parallelism. However, manually parallelizing all loops may result in degrading
parallel performance, as some of them cannot scale desirably to a large number
of threads. In addition, the overheads of manually tuning loop parameters might
prevent an application from reaching its maximum parallel performance. We
illustrate how machine learning techniques can be applied to address these
challenges. In this research, we develop a framework that is able to
automatically capture the static and dynamic information of a loop. Moreover,
we advocate a novel method by introducing HPX smart executors for determining
the execution policy, chunk size, and prefetching distance of an HPX loop to
achieve higher possible performance by feeding static information captured
during compilation and runtime-based dynamic information to our learning model.
Our evaluated execution results show that using these smart executors can speed
up the HPX execution process by around 12%-35% for the Matrix Multiplication,
Stream and $2D$ Stencil benchmarks compared to setting their HPX loop's
execution policy/parameters manually or using HPX auto-parallelization
techniques.
",Loop Parallelization Optimization,automatic loop parallelization using machine learning
"  Large-scale integration of distributed energy resources into residential
distribution feeders necessitates careful control of their operation through
power flow analysis. While the knowledge of the distribution system model is
crucial for this type of analysis, it is often unavailable or outdated. The
recent introduction of synchrophasor technology in low-voltage distribution
grids has created an unprecedented opportunity to learn this model from
high-precision, time-synchronized measurements of voltage and current phasors
at various locations. This paper focuses on joint estimation of model
parameters (admittance values) and operational structure of a poly-phase
distribution network from the available telemetry data via the lasso, a method
for regression shrinkage and selection. We propose tractable convex programs
capable of tackling the low rank structure of the distribution system and
develop an online algorithm for early detection and localization of critical
events that induce a change in the admittance matrix. The efficacy of these
techniques is corroborated through power flow studies on four three-phase
radial distribution systems serving real household demands.
",Distribution System Modeling from Telemetry Data,power grid control and optimization
"  We study the relationship between geometry and capacity measures for deep
neural networks from an invariance viewpoint. We introduce a new notion of
capacity --- the Fisher-Rao norm --- that possesses desirable invariance
properties and is motivated by Information Geometry. We discover an analytical
characterization of the new capacity measure, through which we establish
norm-comparison inequalities and further show that the new measure serves as an
umbrella for several existing norm-based complexity measures. We discuss upper
bounds on the generalization error induced by the proposed measure. Extensive
numerical experiments on CIFAR-10 support our theoretical findings. Our
theoretical analysis rests on a key structural lemma about partial derivatives
of multi-layer rectifier networks.
",Invariance in Deep Neural Networks,machine learning
"  We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building
a generative model of the data distribution. WAE minimizes a penalized form of
the Wasserstein distance between the model distribution and the target
distribution, which leads to a different regularizer than the one used by the
Variational Auto-Encoder (VAE). This regularizer encourages the encoded
training distribution to match the prior. We compare our algorithm with several
other techniques and show that it is a generalization of adversarial
auto-encoders (AAE). Our experiments show that WAE shares many of the
properties of VAEs (stable training, encoder-decoder architecture, nice latent
manifold structure) while generating samples of better quality, as measured by
the FID score.
",Generative Models,generative modeling
"  With the development and widespread use of wireless devices in recent years
(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has
become extremely crowded. In order to counter security threats posed by rogue
or unknown transmitters, it is important to identify RF transmitters not by the
data content of the transmissions but based on the intrinsic physical
characteristics of the transmitters. RF waveforms represent a particular
challenge because of the extremely high data rates involved and the potentially
large number of transmitters present in a given location. These factors outline
the need for rapid fingerprinting and identification methods that go beyond the
traditional hand-engineered approaches. In this study, we investigate the use
of machine learning (ML) strategies to the classification and identification
problems, and the use of wavelets to reduce the amount of data required. Four
different ML strategies are evaluated: deep neural nets (DNN), convolutional
neural nets (CNN), support vector machines (SVM), and multi-stage training
(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method
preconditioned by wavelets was by far the most accurate, achieving 100%
classification accuracy of transmitters, as tested using data originating from
12 different transmitters. We discuss strategies for extension of MST to a much
larger number of transmitters.
",RF Transmitter Identification using Machine Learning,radio frequency identification
"  Stochastic optimization of continuous objectives is at the heart of modern
machine learning. However, many important problems are of discrete nature and
often involve submodular objectives. We seek to unleash the power of stochastic
continuous optimization, namely stochastic gradient descent and its variants,
to such discrete problems. We first introduce the problem of stochastic
submodular optimization, where one needs to optimize a submodular objective
which is given as an expectation. Our model captures situations where the
discrete objective arises as an empirical risk (e.g., in the case of
exemplar-based clustering), or is given as an explicit stochastic model (e.g.,
in the case of influence maximization in social networks). By exploiting that
common extensions act linearly on the class of submodular functions, we employ
projected stochastic gradient ascent and its variants in the continuous domain,
and perform rounding to obtain discrete solutions. We focus on the rich and
widely used family of weighted coverage functions. We show that our approach
yields solutions that are guaranteed to match the optimal approximation
guarantees, while reducing the computational cost by several orders of
magnitude, as we demonstrate empirically.
",Stochastic Submodular Optimization,stochastic optimization of submodular functions
"  This paper describes a general, scalable, end-to-end framework that uses the
generative adversarial network (GAN) objective to enable robust speech
recognition. Encoders trained with the proposed approach enjoy improved
invariance by learning to map noisy audio to the same embedding space as that
of clean audio. Unlike previous methods, the new framework does not rely on
domain expertise or simplifying assumptions as are often needed in signal
processing, and directly encourages robustness in a data-driven way. We show
the new approach improves simulated far-field speech recognition of vanilla
sequence-to-sequence models without specialized front-ends or preprocessing.
",Speech Recognition using GAN,robust speech recognition using generative adversarial networks (gans)
"  Temporal-difference (TD) learning is an important field in reinforcement
learning. Sarsa and Q-Learning are among the most used TD algorithms. The
Q($\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper
extends the Q($\sigma$) algorithm to an online multi-step algorithm Q($\sigma,
\lambda$) using eligibility traces and introduces Double Q($\sigma$) as the
extension of Q($\sigma$) to double learning. Experiments suggest that the new
Q($\sigma, \lambda$) algorithm can outperform the classical TD control methods
Sarsa($\lambda$), Q($\lambda$) and Q($\sigma$).
",Extensions of Temporal-Difference Learning Algorithms,reinforcement learning
"  Long Short-Term Memory (LSTM) is a popular approach to boosting the ability
of Recurrent Neural Networks to store longer term temporal information. The
capacity of an LSTM network can be increased by widening and adding layers.
However, usually the former introduces additional parameters, while the latter
increases the runtime. As an alternative we propose the Tensorized LSTM in
which the hidden states are represented by tensors and updated via a
cross-layer convolution. By increasing the tensor size, the network can be
widened efficiently without additional parameters since the parameters are
shared across different locations in the tensor; by delaying the output, the
network can be deepened implicitly with little additional runtime since deep
computations for each timestep are merged into temporal computations of the
sequence. Experiments conducted on five challenging sequence learning tasks
show the potential of the proposed model.
",Tensorized LSTM Architecture,tensorized lstm for efficient long-term memory modeling
"  Low-rank approximation is a common tool used to accelerate kernel methods:
the $n \times n$ kernel matrix $K$ is approximated via a rank-$k$ matrix
$\tilde K$ which can be stored in much less space and processed more quickly.
In this work we study the limits of computationally efficient low-rank kernel
approximation. We show that for a broad class of kernels, including the popular
Gaussian and polynomial kernels, computing a relative error $k$-rank
approximation to $K$ is at least as difficult as multiplying the input data
matrix $A \in \mathbb{R}^{n \times d}$ by an arbitrary matrix $C \in
\mathbb{R}^{d \times k}$. Barring a breakthrough in fast matrix multiplication,
when $k$ is not too large, this requires $\Omega(nnz(A)k)$ time where $nnz(A)$
is the number of non-zeros in $A$. This lower bound matches, in many parameter
regimes, recent work on subquadratic time algorithms for low-rank approximation
of general kernels [MM16,MW17], demonstrating that these algorithms are
unlikely to be significantly improved, in particular to $O(nnz(A))$ input
sparsity runtimes. At the same time there is hope: we show for the first time
that $O(nnz(A))$ time approximation is possible for general radial basis
function kernels (e.g., the Gaussian kernel) for the closely related problem of
low-rank approximation of the kernelized dataset.
",Low-Rank Kernel Approximation Complexity,computational complexity of low-rank kernel approximation
"  We consider decentralized stochastic multi-armed bandit problem with multiple
players in the case of different communication probabilities between players.
Each player makes a decision of pulling an arm without cooperation while aiming
to maximize his or her reward but informs his or her neighbors in the end of
every turn about the arm he or she pulled and the reward he or she got.
Neighbors of players are determined according to an Erdos-Renyi graph with
which is reproduced in the beginning of every turn. We consider i.i.d. rewards
generated by a Bernoulli distribution and assume that players are unaware about
the arms' probability distributions and their mean values. In case of a
collision, we assume that only one of the players who is randomly chosen gets
the reward where the others get zero reward. We study the effects of
connectivity, the degree of communication between players, on the cumulative
regret using well-known algorithms UCB1, epsilon-Greedy and Thompson Sampling.
",Decentralized Multi-Armed Bandit Problem with Limited Communication,decentralized stochastic multi-armed bandit problem
"  A remarkable feature of human beings is their capacity for creative
behaviour, referring to their ability to react to problems in ways that are
novel, surprising, and useful. Transformational creativity is a form of
creativity where the creative behaviour is induced by a transformation of the
actor's conceptual space, that is, the representational system with which the
actor interprets its environment. In this report, we focus on ways of adapting
systems of learned representations as they switch from performing one task to
performing another. We describe an experimental comparison of multiple
strategies for adaptation of learned features, and evaluate how effectively
each of these strategies realizes the adaptation, in terms of the amount of
training, and in terms of their ability to cope with restricted availability of
training data. We show, among other things, that across handwritten digits,
natural images, and classical music, adaptive strategies are systematically
more effective than a baseline method that starts learning from scratch.
",Adaptive Feature Learning Strategies,adaptive representation learning
"  In this paper, our goal is to compare performances of three different
algorithms to predict the ratings that will be given to movies by potential
users where we are given a user-movie rating matrix based on the past
observations. To this end, we evaluate User-Based Collaborative Filtering,
Iterative Matrix Factorization and Yehuda Koren's Integrated model using
neighborhood and factorization where we use root mean square error (RMSE) as
the performance evaluation metric. In short, we do not observe significant
differences between performances, especially when the complexity increase is
considered. We can conclude that Iterative Matrix Factorization performs fairly
well despite its simplicity.
",Movie Rating Prediction Algorithms,movie rating prediction algorithms
"  We study approximations of the partition function of dense graphical models.
Partition functions of graphical models play a fundamental role is statistical
physics, in statistics and in machine learning. Two of the main methods for
approximating the partition function are Markov Chain Monte Carlo and
Variational Methods. An impressive body of work in mathematics, physics and
theoretical computer science provides conditions under which Markov Chain Monte
Carlo methods converge in polynomial time. These methods often lead to
polynomial time approximation algorithms for the partition function in cases
where the underlying model exhibits correlation decay. There are very few
theoretical guarantees for the performance of variational methods. One
exception is recent results by Risteski (2016) who considered dense graphical
models and showed that using variational methods, it is possible to find an
$O(\epsilon n)$ additive approximation to the log partition function in time
$n^{O(1/\epsilon^2)}$ even in a regime where correlation decay does not hold.
  We show that under essentially the same conditions, an $O(\epsilon n)$
additive approximation of the log partition function can be found in constant
time, independent of $n$. In particular, our results cover dense Ising and
Potts models as well as dense graphical models with $k$-wise interaction. They
also apply for low threshold rank models.
",Approximating Partition Functions in Graphical Models,approximation algorithms for partition functions of graphical models
"  In this paper, we study the problem of \textit{constrained} and
\textit{stochastic} continuous submodular maximization. Even though the
objective function is not concave (nor convex) and is defined in terms of an
expectation, we develop a variant of the conditional gradient method, called
\alg, which achieves a \textit{tight} approximation guarantee. More precisely,
for a monotone and continuous DR-submodular function and subject to a
\textit{general} convex body constraint, we prove that \alg achieves a
$[(1-1/e)\text{OPT} -\eps]$ guarantee (in expectation) with
$\mathcal{O}{(1/\eps^3)}$ stochastic gradient computations. This guarantee
matches the known hardness results and closes the gap between deterministic and
stochastic continuous submodular maximization. By using stochastic continuous
optimization as an interface, we also provide the first $(1-1/e)$ tight
approximation guarantee for maximizing a \textit{monotone but stochastic}
submodular \textit{set} function subject to a general matroid constraint.
",Constrained Stochastic Submodular Maximization,stochastic continuous submodular maximization
"  Spatially aligning medical images from different modalities remains a
challenging task, especially for intraoperative applications that require fast
and robust algorithms. We propose a weakly-supervised, label-driven formulation
for learning 3D voxel correspondence from higher-level label correspondence,
thereby bypassing classical intensity-based image similarity measures. During
training, a convolutional neural network is optimised by outputting a dense
displacement field (DDF) that warps a set of available anatomical labels from
the moving image to match their corresponding counterparts in the fixed image.
These label pairs, including solid organs, ducts, vessels, point landmarks and
other ad hoc structures, are only required at training time and can be
spatially aligned by minimising a cross-entropy function of the warped moving
label and the fixed label. During inference, the trained network takes a new
image pair to predict an optimal DDF, resulting in a fully-automatic,
label-free, real-time and deformable registration. For interventional
applications where large global transformation prevails, we also propose a
neural network architecture to jointly optimise the global- and local
displacements. Experiment results are presented based on cross-validating
registrations of 111 pairs of T2-weighted magnetic resonance images and 3D
transrectal ultrasound images from prostate cancer patients with a total of
over 4000 anatomical labels, yielding a median target registration error of 4.2
mm on landmark centroids and a median Dice of 0.88 on prostate glands.
",Medical Image Registration,medical image registration
"  RoboCup is an international scientific robot competition in which teams of
multiple robots compete against each other. Its different leagues provide many
sources of robotics data, that can be used for further analysis and application
of machine learning. This paper describes a large dataset from games of some of
the top teams (from 2016 and 2017) in RoboCup Soccer Simulation League (2D),
where teams of 11 robots (agents) compete against each other. Overall, we used
10 different teams to play each other, resulting in 45 unique pairings. For
each pairing, we ran 25 matches (of 10mins), leading to 1125 matches or more
than 180 hours of game play. The generated CSV files are 17GB of data (zipped),
or 229GB (unzipped). The dataset is unique in the sense that it contains both
the ground truth data (global, complete, noise-free information of all objects
on the field), as well as the noisy, local and incomplete percepts of each
robot. These data are made available as CSV files, as well as in the original
soccer simulator formats.
",RoboCup Soccer Simulation Data,robocup soccer simulation league dataset
"  This work studies low-rank approximation of a positive semidefinite matrix
from partial entries via nonconvex optimization. We characterized how well
local-minimum based low-rank factorization approximates a fixed positive
semidefinite matrix without any assumptions on the rank-matching, the condition
number or eigenspace incoherence parameter. Furthermore, under certain
assumptions on rank-matching and well-boundedness of condition numbers and
eigenspace incoherence parameters, a corollary of our main theorem improves the
state-of-the-art sampling rate results for nonconvex matrix completion with no
spurious local minima in Ge et al. [2016, 2017]. In addition, we investigated
when the proposed nonconvex optimization results in accurate low-rank
approximations even in presence of large condition numbers, large incoherence
parameters, or rank mismatching. We also propose to apply the nonconvex
optimization to memory-efficient Kernel PCA. Compared to the well-known
Nystr\""{o}m methods, numerical experiments indicate that the proposed nonconvex
optimization approach yields more stable results in both low-rank approximation
and clustering.
",Low-Rank Matrix Approximation via Nonconvex Optimization,low-rank matrix approximation and optimization
"  Generative Adversarial Networks (GANs) were intuitively and attractively
explained under the perspective of game theory, wherein two involving parties
are a discriminator and a generator. In this game, the task of the
discriminator is to discriminate the real and generated (i.e., fake) data,
whilst the task of the generator is to generate the fake data that maximally
confuses the discriminator. In this paper, we propose a new viewpoint for GANs,
which is termed as the minimizing general loss viewpoint. This viewpoint shows
a connection between the general loss of a classification problem regarding a
convex loss function and a f-divergence between the true and fake data
distributions. Mathematically, we proposed a setting for the classification
problem of the true and fake data, wherein we can prove that the general loss
of this classification problem is exactly the negative f-divergence for a
certain convex function f. This allows us to interpret the problem of learning
the generator for dismissing the f-divergence between the true and fake data
distributions as that of maximizing the general loss which is equivalent to the
min-max problem in GAN if the Logistic loss is used in the classification
problem. However, this viewpoint strengthens GANs in two ways. First, it allows
us to employ any convex loss function for the discriminator. Second, it
suggests that rather than limiting ourselves in NN-based discriminators, we can
alternatively utilize other powerful families. Bearing this viewpoint, we then
propose using the kernel-based family for discriminators. This family has two
appealing features: i) a powerful capacity in classifying non-linear nature
data and ii) being convex in the feature space. Using the convexity of this
family, we can further develop Fenchel duality to equivalently transform the
max-min problem to the max-max dual problem.
",Minimizing General Loss Viewpoint for Generative Adversarial Networks,generative adversarial networks
"  It is expected that progress toward true artificial intelligence will be
achieved through the emergence of a system that integrates representation
learning and complex reasoning (LeCun et al. 2015). In response to this
prediction, research has been conducted on implementing the symbolic reasoning
of a von Neumann computer in an artificial neural network (Graves et al. 2016;
Graves et al. 2014; Reed et al. 2015). However, these studies have many
limitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we
present a new learning paradigm: a learning solving procedure (LSP) that learns
the procedure for solving complex problems. This is not accomplished merely by
learning input-output data, but by learning algorithms through a solving
procedure that obtains the output as a sequence of tasks for a given input
problem. The LSP neural network system not only learns simple problems of
addition and multiplication, but also the algorithms of complicated problems,
such as complex arithmetic expression, sorting, and Hanoi Tower. To realize
this, the LSP neural network structure consists of a deep neural network and
long short-term memory, which are recursively combined. Through
experimentation, we demonstrate the efficiency and scalability of LSP and its
validity as a mechanism of complex reasoning.
",Neural-Symbolic Integration in AI,neural-symbolic integration in artificial intelligence
"  We study a new aggregation operator for gradients coming from a mini-batch
for stochastic gradient (SG) methods that allows a significant speed-up in the
case of sparse optimization problems. We call this method AdaBatch and it only
requires a few lines of code change compared to regular mini-batch SGD
algorithms. We provide a theoretical insight to understand how this new class
of algorithms is performing and show that it is equivalent to an implicit
per-coordinate rescaling of the gradients, similarly to what Adagrad methods
can do. In theory and in practice, this new aggregation allows to keep the same
sample efficiency of SG methods while increasing the batch size.
Experimentally, we also show that in the case of smooth convex optimization,
our procedure can even obtain a better loss when increasing the batch size for
a fixed number of samples. We then apply this new algorithm to obtain a
parallelizable stochastic gradient method that is synchronous but allows
speed-up on par with Hogwild! methods as convergence does not deteriorate with
the increase of the batch size. The same approach can be used to make
mini-batch provably efficient for variance-reduced SG methods such as SVRG.
",AdaBatch for Stochastic Gradient Optimization,improving stochastic gradient methods for optimization
"  Many deployed learned models are black boxes: given input, returns output.
Internal information about the model, such as the architecture, optimisation
procedure, or training data, is not disclosed explicitly as it might contain
proprietary information or make the system more vulnerable. This work shows
that such attributes of neural networks can be exposed from a sequence of
queries. This has multiple implications. On the one hand, our work exposes the
vulnerability of black-box neural networks to different types of attacks -- we
show that the revealed internal information helps generate more effective
adversarial examples against the black box model. On the other hand, this
technique can be used for better protection of private content from automatic
recognition models using adversarial examples. Our paper suggests that it is
actually hard to draw a line between white box and black box models.
","""Neural Network Transparency and Security""",exposing internal information of neural networks
"  In this paper, we consider the block-sparse signals recovery problem in the
context of multiple measurement vectors (MMV) with common row sparsity
patterns. We develop a new method for recovery of common row sparsity MMV
signals, where a pattern-coupled hierarchical Gaussian prior model is
introduced to characterize both the block-sparsity of the coefficients and the
statistical dependency between neighboring coefficients of the common row
sparsity MMV signals. Unlike many other methods, the proposed method is able to
automatically capture the block sparse structure of the unknown signal. Our
method is developed using an expectation-maximization (EM) framework.
Simulation results show that our proposed method offers competitive performance
in recovering block-sparse common row sparsity pattern MMV signals.
",Block-Sparse Signal Recovery,block-sparse signals recovery in multiple measurement vectors
"  Accurate diagnosis of tool wear in metal turning process remains an open
challenge for both scientists and industrial practitioners because of
inhomogeneities in workpiece material, nonstationary machining settings to suit
production requirements, and nonlinear relations between measured variables and
tool wear. Common methodologies for tool condition monitoring still rely on
batch approaches which cannot cope with a fast sampling rate of metal cutting
process. Furthermore they require a retraining process to be completed from
scratch when dealing with a new set of machining parameters. This paper
presents an online tool condition monitoring approach based on Parsimonious
Ensemble+, pENsemble+. The unique feature of pENsemble+ lies in its highly
flexible principle where both ensemble structure and base-classifier structure
can automatically grow and shrink on the fly based on the characteristics of
data streams. Moreover, the online feature selection scenario is integrated to
actively sample relevant input attributes. The paper presents advancement of a
newly developed ensemble learning algorithm, pENsemble+, where online active
learning scenario is incorporated to reduce operator labelling effort. The
ensemble merging scenario is proposed which allows reduction of ensemble
complexity while retaining its diversity. Experimental studies utilising
real-world manufacturing data streams and comparisons with well known
algorithms were carried out. Furthermore, the efficacy of pENsemble was
examined using benchmark concept drift data streams. It has been found that
pENsemble+ incurs low structural complexity and results in a significant
reduction of operator labelling effort.
",Online Tool Condition Monitoring in Metal Turning Process,tool condition monitoring in metal turning process
"  Calcium imaging permits optical measurement of neural activity. Since
intracellular calcium concentration is an indirect measurement of neural
activity, computational tools are necessary to infer the true underlying
spiking activity from fluorescence measurements. Bayesian model inversion can
be used to solve this problem, but typically requires either computationally
expensive MCMC sampling, or faster but approximate maximum-a-posteriori
optimization. Here, we introduce a flexible algorithmic framework for fast,
efficient and accurate extraction of neural spikes from imaging data. Using the
framework of variational autoencoders, we propose to amortize inference by
training a deep neural network to perform model inversion efficiently. The
recognition network is trained to produce samples from the posterior
distribution over spike trains. Once trained, performing inference amounts to a
fast single forward pass through the network, without the need for iterative
optimization or sampling. We show that amortization can be applied flexibly to
a wide range of nonlinear generative models and significantly improves upon the
state of the art in computation time, while achieving competitive accuracy. Our
framework is also able to represent posterior distributions over spike-trains.
We demonstrate the generality of our method by proposing the first
probabilistic approach for separating backpropagating action potentials from
putative synaptic inputs in calcium imaging of dendritic spines.
",Neural Spike Inference from Calcium Imaging Data,neural spike extraction from calcium imaging data
"  This paper presents an automated approach for interpretable feature
recommendation for solving signal data analytics problems. The method has been
tested by performing experiments on datasets in the domain of prognostics where
interpretation of features is considered very important. The proposed approach
is based on Wide Learning architecture and provides means for interpretation of
the recommended features. It is to be noted that such an interpretation is not
available with feature learning approaches like Deep Learning (such as
Convolutional Neural Network) or feature transformation approaches like
Principal Component Analysis. Results show that the feature recommendation and
interpretation techniques are quite effective for the problems at hand in terms
of performance and drastic reduction in time to develop a solution. It is
further shown by an example, how this human-in-loop interpretation system can
be used as a prescriptive system.
",Interpretable Feature Recommendation in Signal Data Analytics,automated feature recommendation for signal data analytics
"  Traditional vision-based hand gesture recognition systems is limited under
dark circumstances. In this paper, we build a hand gesture recognition system
based on microwave transceiver and deep learning algorithm. A Doppler radar
sensor with dual receiving channels at 5.8GHz is used to acquire a big database
of hand gestures signals. The received hand gesture signals are then processed
with time-frequency analysis. Based on these big databases of hand gesture, we
propose a new machine learning architecture called deformable deep
convolutional generative adversarial network. Experimental results show the new
architecture can upgrade the recognition rate by 10% and the deformable kernel
can reduce the testing time cost by 30%.
",Microwave-based Hand Gesture Recognition,hand gesture recognition using microwave transceiver and deep learning
"  Generative models such as Variational Auto Encoders (VAEs) and Generative
Adversarial Networks (GANs) are typically trained for a fixed prior
distribution in the latent space, such as uniform or Gaussian. After a trained
model is obtained, one can sample the Generator in various forms for
exploration and understanding, such as interpolating between two samples,
sampling in the vicinity of a sample or exploring differences between a pair of
samples applied to a third sample. In this paper, we show that the latent space
operations used in the literature so far induce a distribution mismatch between
the resulting outputs and the prior distribution the model was trained on. To
address this, we propose to use distribution matching transport maps to ensure
that such latent space operations preserve the prior distribution, while
minimally modifying the original operation. Our experimental results validate
that the proposed operations give higher quality samples compared to the
original operations.
",Latent Space Operations in Generative Models,generative models and latent space operations
"  A grand challenge of the 21st century cosmology is to accurately estimate the
cosmological parameters of our Universe. A major approach to estimating the
cosmological parameters is to use the large-scale matter distribution of the
Universe. Galaxy surveys provide the means to map out cosmic large-scale
structure in three dimensions. Information about galaxy locations is typically
summarized in a ""single"" function of scale, such as the galaxy correlation
function or power-spectrum. We show that it is possible to estimate these
cosmological parameters directly from the distribution of matter. This paper
presents the application of deep 3D convolutional networks to volumetric
representation of dark-matter simulations as well as the results obtained using
a recently proposed distribution regression framework, showing that machine
learning techniques are comparable to, and can sometimes outperform,
maximum-likelihood point estimates using ""cosmological models"". This opens the
way to estimating the parameters of our Universe with higher accuracy.
",Cosmological Parameter Estimation with Machine Learning,cosmological parameter estimation
"  A central task in the field of quantum computing is to find applications
where quantum computer could provide exponential speedup over any classical
computer. Machine learning represents an important field with broad
applications where quantum computer may offer significant speedup. Several
quantum algorithms for discriminative machine learning have been found based on
efficient solving of linear algebraic problems, with potential exponential
speedup in runtime under the assumption of effective input from a quantum
random access memory. In machine learning, generative models represent another
large class which is widely used for both supervised and unsupervised learning.
Here, we propose an efficient quantum algorithm for machine learning based on a
quantum generative model. We prove that our proposed model is exponentially
more powerful to represent probability distributions compared with classical
generative models and has exponential speedup in training and inference at
least for some instances under a reasonable assumption in computational
complexity theory. Our result opens a new direction for quantum machine
learning and offers a remarkable example in which a quantum algorithm shows
exponential improvement over any classical algorithm in an important
application field.
",Quantum Generative Models for Machine Learning,quantum machine learning
